<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2018-03-16
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop Amazon Web Services support &#x2013; Troubleshooting</title>
    <style type="text/css" media="all">
      @import url("../../css/maven-base.css");
      @import url("../../css/maven-theme.css");
      @import url("../../css/site.css");
    </style>
    <link rel="stylesheet" href="../../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20180316" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="https://hadoop.apache.org/" id="bannerLeft">
                                        <img src="https://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="https://www.apache.org/" id="bannerRight">
                                        <img src="https://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                   <div class="xleft">
                          <a href="https://www.apache.org/" class="externalLink">Apache</a>
        &gt;
                  <a href="https://hadoop.apache.org/" class="externalLink">Hadoop</a>
        &gt;
                  <a href="../../index.html">Apache Hadoop Amazon Web Services support</a>
        &gt;
        Troubleshooting
        </div>
            <div class="xright">            <a href="https://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://git-wip-us.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
              
                                   &nbsp;| Last Published: 2018-03-16
              &nbsp;| Version: 3.0.1
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility Specification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DownstreamDev.html">Downstream Developer's Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html">Disk Balancer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html">Upgrade Domain</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html">DataNode Admin</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSRouterFederation.html">Router Federation</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html">Support for YARN Shared Cache</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceModel.html">Resource Model</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Running Applications in Docker Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/registry/index.html">Registry</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html">Graceful Decommission</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html">Opportunistic Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/Federation.html">YARN Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SharedCache.html">Shared Cache</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_v.2_REST_API">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-aliyun/tools/hadoop-aliyun/index.html">Aliyun OSS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-openstack/index.html">OpenStack Swift</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-resourceestimator/ResourceEstimator.html">Resource Estimator Service</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Benchmarking.html">Hadoop Benchmarking</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/release/index.html">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="https://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../../images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   https://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<h1>Troubleshooting</h1>
<ul>
<li><a href="#Introduction">  Introduction</a></li>
<li><a href="#Classpath_Setup"> Classpath Setup</a>
<ul>
<li><a href="#ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem">ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</a></li>
<li><a href="#ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client">ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</a></li>
<li><a href="#Missing_method_in_com.amazonaws_class">Missing method in com.amazonaws class</a></li></ul></li>
<li><a href="#Authentication_Failure"> Authentication Failure</a>
<ul>
<li><a href="#Authentication_failure_due_to_clock_skew">Authentication failure due to clock skew</a></li>
<li><a href="#Authentication_failure_when_using_URLs_with_embedded_secrets">Authentication failure when using URLs with embedded secrets</a></li>
<li><a href="#a.E2.80.9CBad_Request.E2.80.9D_exception_when_working_with_AWS_S3_Frankfurt.2C_Seoul.2C_or_other_.E2.80.9CV4.E2.80.9D_endpoint"> &#x201c;Bad Request&#x201d; exception when working with AWS S3 Frankfurt, Seoul, or other &#x201c;V4&#x201d; endpoint</a></li></ul></li>
<li><a href="#Connectivity_Problems"> Connectivity Problems</a>
<ul>
<li><a href="#Error_message_.E2.80.9CThe_bucket_you_are_attempting_to_access_must_be_addressed_using_the_specified_endpoint.E2.80.9D"> Error message &#x201c;The bucket you are attempting to access must be addressed using the specified endpoint&#x201d;</a></li>
<li><a href="#a.E2.80.9CTimeout_waiting_for_connection_from_pool.E2.80.9D_when_writing_data"> &#x201c;Timeout waiting for connection from pool&#x201d; when writing data</a></li>
<li><a href="#a.E2.80.9CTimeout_waiting_for_connection_from_pool.E2.80.9D_when_reading_data">&#x201c;Timeout waiting for connection from pool&#x201d; when reading data</a></li>
<li><a href="#Out_of_heap_memory_when_writing_with_via_Fast_Upload">Out of heap memory when writing with via Fast Upload</a></li>
<li><a href="#MultiObjectDeleteException_during_delete_or_rename_of_files">MultiObjectDeleteException during delete or rename of files</a></li>
<li><a href="#a.E2.80.9CFailed_to_Sanitize_XML_document.E2.80.9D">&#x201c;Failed to Sanitize XML document&#x201d;</a></li>
<li><a href="#JSON_Parse_Error_from_AWS_SDK">JSON Parse Error from AWS SDK</a></li></ul></li>
<li><a href="#Miscellaneous_Errors">Miscellaneous Errors</a>
<ul>
<li><a href="#When_writing_data:_.E2.80.9Cjava.io.FileNotFoundException:_Completing_multi-part_upload.E2.80.9D">When writing data: &#x201c;java.io.FileNotFoundException: Completing multi-part upload&#x201d;</a></li>
<li><a href="#Issue:_when_writing_data.2C_HTTP_Exceptions_logged_at_info_from_AmazonHttpClient">Issue: when writing data, HTTP Exceptions logged at info from AmazonHttpClient</a></li></ul></li>
<li><a href="#File_System_Semantics">File System Semantics</a>
<ul>
<li><a href="#Visible_S3_Inconsistency">Visible S3 Inconsistency</a></li>
<li><a href="#FileNotFoundException_even_though_the_file_was_just_written.">FileNotFoundException even though the file was just written.</a></li>
<li><a href="#File_not_found_in_a_directory_listing.2C_even_though_getFileStatus.28.29_finds_it">File not found in a directory listing, even though getFileStatus() finds it</a></li>
<li><a href="#File_not_visible.2Fsaved">File not visible/saved</a></li>
<li><a href="#File_flush.28.29.2C_hsync_and_hflush.28.29_calls_do_not_save_data_to_S3">File flush(), hsync and hflush() calls do not save data to S3</a></li></ul></li>
<li><a href="#S3_Server_Side_Encryption"> S3 Server Side Encryption</a>
<ul>
<li><a href="#Using_SSE-KMS_.E2.80.9CInvalid_arn.E2.80.9D">Using SSE-KMS &#x201c;Invalid arn&#x201d;</a></li>
<li><a href="#Using_SSE-C_.E2.80.9CBad_Request.E2.80.9D">Using SSE-C &#x201c;Bad Request&#x201d;</a></li></ul></li>
<li><a href="#Performance"> Performance</a></li>
<li><a href="#Troubleshooting_network_performance">Troubleshooting network performance</a></li>
<li><a href="#Other_Issues">Other Issues</a>
<ul>
<li><a href="#Enabling_low-level_logging"> Enabling low-level logging</a></li></ul></li></ul>

<div class="section">
<h2><a name="Introduction"></a><a name="introduction"></a>  Introduction</h2>
<p>Common problems working with S3 are</p>
<ol style="list-style-type: decimal">

<li>Classpath setup</li>
<li>Authentication</li>
<li>S3 Inconsistency side-effects</li>
</ol>
<p>Classpath is usually the first problem. For the S3x filesystem clients, you need the Hadoop-specific filesystem clients, third party S3 client libraries compatible with the Hadoop code, and any dependent libraries compatible with Hadoop and the specific JVM.</p>
<p>The classpath must be set up for the process talking to S3: if this is code running in the Hadoop cluster, the JARs must be on that classpath. That includes <tt>distcp</tt> and the <tt>hadoop fs</tt> command.</p>
<ul>
<li><a href="#Introduction">  Introduction</a></li>
<li><a href="#Classpath_Setup"> Classpath Setup</a>
<ul>
<li><a href="#ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem">ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</a></li>
<li><a href="#ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client">ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</a></li>
<li><a href="#Missing_method_in_com.amazonaws_class">Missing method in com.amazonaws class</a></li></ul></li>
<li><a href="#Authentication_Failure"> Authentication Failure</a>
<ul>
<li><a href="#Authentication_failure_due_to_clock_skew">Authentication failure due to clock skew</a></li>
<li><a href="#Authentication_failure_when_using_URLs_with_embedded_secrets">Authentication failure when using URLs with embedded secrets</a></li>
<li><a href="#a.E2.80.9CBad_Request.E2.80.9D_exception_when_working_with_AWS_S3_Frankfurt.2C_Seoul.2C_or_other_.E2.80.9CV4.E2.80.9D_endpoint"> &#x201c;Bad Request&#x201d; exception when working with AWS S3 Frankfurt, Seoul, or other &#x201c;V4&#x201d; endpoint</a></li></ul></li>
<li><a href="#Connectivity_Problems"> Connectivity Problems</a>
<ul>
<li><a href="#Error_message_.E2.80.9CThe_bucket_you_are_attempting_to_access_must_be_addressed_using_the_specified_endpoint.E2.80.9D"> Error message &#x201c;The bucket you are attempting to access must be addressed using the specified endpoint&#x201d;</a></li>
<li><a href="#a.E2.80.9CTimeout_waiting_for_connection_from_pool.E2.80.9D_when_writing_data"> &#x201c;Timeout waiting for connection from pool&#x201d; when writing data</a></li>
<li><a href="#a.E2.80.9CTimeout_waiting_for_connection_from_pool.E2.80.9D_when_reading_data">&#x201c;Timeout waiting for connection from pool&#x201d; when reading data</a></li>
<li><a href="#Out_of_heap_memory_when_writing_with_via_Fast_Upload">Out of heap memory when writing with via Fast Upload</a></li>
<li><a href="#MultiObjectDeleteException_during_delete_or_rename_of_files">MultiObjectDeleteException during delete or rename of files</a></li>
<li><a href="#a.E2.80.9CFailed_to_Sanitize_XML_document.E2.80.9D">&#x201c;Failed to Sanitize XML document&#x201d;</a></li>
<li><a href="#JSON_Parse_Error_from_AWS_SDK">JSON Parse Error from AWS SDK</a></li></ul></li>
<li><a href="#Miscellaneous_Errors">Miscellaneous Errors</a>
<ul>
<li><a href="#When_writing_data:_.E2.80.9Cjava.io.FileNotFoundException:_Completing_multi-part_upload.E2.80.9D">When writing data: &#x201c;java.io.FileNotFoundException: Completing multi-part upload&#x201d;</a></li>
<li><a href="#Issue:_when_writing_data.2C_HTTP_Exceptions_logged_at_info_from_AmazonHttpClient">Issue: when writing data, HTTP Exceptions logged at info from AmazonHttpClient</a></li></ul></li>
<li><a href="#File_System_Semantics">File System Semantics</a>
<ul>
<li><a href="#Visible_S3_Inconsistency">Visible S3 Inconsistency</a></li>
<li><a href="#FileNotFoundException_even_though_the_file_was_just_written.">FileNotFoundException even though the file was just written.</a></li>
<li><a href="#File_not_found_in_a_directory_listing.2C_even_though_getFileStatus.28.29_finds_it">File not found in a directory listing, even though getFileStatus() finds it</a></li>
<li><a href="#File_not_visible.2Fsaved">File not visible/saved</a></li>
<li><a href="#File_flush.28.29.2C_hsync_and_hflush.28.29_calls_do_not_save_data_to_S3">File flush(), hsync and hflush() calls do not save data to S3</a></li></ul></li>
<li><a href="#S3_Server_Side_Encryption"> S3 Server Side Encryption</a>
<ul>
<li><a href="#Using_SSE-KMS_.E2.80.9CInvalid_arn.E2.80.9D">Using SSE-KMS &#x201c;Invalid arn&#x201d;</a></li>
<li><a href="#Using_SSE-C_.E2.80.9CBad_Request.E2.80.9D">Using SSE-C &#x201c;Bad Request&#x201d;</a></li></ul></li>
<li><a href="#Performance"> Performance</a></li>
<li><a href="#Troubleshooting_network_performance">Troubleshooting network performance</a></li>
<li><a href="#Other_Issues">Other Issues</a>
<ul>
<li><a href="#Enabling_low-level_logging"> Enabling low-level logging</a></li></ul></li></ul>
</div>
<div class="section">
<h2><a name="Classpath_Setup"></a><a name="classpath"></a> Classpath Setup</h2>
<p>Note that for security reasons, the S3A client does not provide much detail on the authentication process (i.e. the secrets used to authenticate).</p>
<div class="section">
<h3><a name="ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem"></a><tt>ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</tt></h3>
<p>These is Hadoop filesytem client classes, found in the <tt>hadoop-aws</tt> JAR. An exception reporting this class as missing means that this JAR is not on the classpath.</p></div>
<div class="section">
<h3><a name="ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client"></a><tt>ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</tt></h3>
<p>(or other <tt>com.amazonaws</tt> class.)</p>
<p>This means that the <tt>aws-java-sdk-bundle.jar</tt> JAR is not on the classpath: add it.</p></div>
<div class="section">
<h3><a name="Missing_method_in_com.amazonaws_class"></a>Missing method in <tt>com.amazonaws</tt> class</h3>
<p>This can be triggered by incompatibilities between the AWS SDK on the classpath and the version which Hadoop was compiled with.</p>
<p>The AWS SDK JARs change their signature enough between releases that the only way to safely update the AWS SDK version is to recompile Hadoop against the later version.</p>
<p>The sole fix is to use the same version of the AWS SDK with which Hadoop was built.</p></div></div>
<div class="section">
<h2><a name="Authentication_Failure"></a><a name="authentication"></a> Authentication Failure</h2>
<p>If Hadoop cannot authenticate with the S3 service endpoint, the client retries a number of times before eventually failing. When it finally gives up, it will report a message about signature mismatch:</p>

<div>
<div>
<pre class="source">com.amazonaws.services.s3.model.AmazonS3Exception:
 The request signature we calculated does not match the signature you provided.
 Check your key and signing method.
  (Service: Amazon S3; Status Code: 403; Error Code: SignatureDoesNotMatch,
</pre></div></div>

<p>The likely cause is that you either have the wrong credentials or somehow the credentials were not readable on the host attempting to read or write the S3 Bucket.</p>
<p>Enabling debug logging for the package <tt>org.apache.hadoop.fs.s3a</tt> can help provide more information.</p>
<p>The most common cause is that you have the wrong credentials for any of the current authentication mechanism(s) &#x2014;or somehow the credentials were not readable on the host attempting to read or write the S3 Bucket. However, there are a couple of system configuration problems (JVM version, system clock) which also need to be checked.</p>
<p>Most common: there&#x2019;s an error in the configuration properties.</p>
<ol style="list-style-type: decimal">

<li>

<p>Make sure that the name of the bucket is the correct one. That is: check the URL.</p>
</li>
<li>

<p>If using a private S3 server, make sure endpoint in <tt>fs.s3a.endpoint</tt> has been set to this server -and that the client is not accidentally trying to authenticate with the public Amazon S3 service.</p>
</li>
<li>

<p>Make sure the property names are correct. For S3A, they are <tt>fs.s3a.access.key</tt> and <tt>fs.s3a.secret.key</tt> &#x2014;you cannot just copy the S3N properties and replace <tt>s3n</tt> with <tt>s3a</tt>.</p>
</li>
<li>

<p>Make sure the properties are visible to the process attempting to talk to the object store. Placing them in <tt>core-site.xml</tt> is the standard mechanism.</p>
</li>
<li>

<p>If using session authentication, the session may have expired. Generate a new session token and secret.</p>
</li>
<li>

<p>If using environement variable-based authentication, make sure that the relevant variables are set in the environment in which the process is running.</p>
</li>
</ol>
<p>The standard first step is: try to use the AWS command line tools with the same credentials, through a command such as:</p>

<div>
<div>
<pre class="source">hadoop fs -ls s3a://my-bucket/
</pre></div></div>

<p>Note the trailing &#x201c;/&#x201d; here; without that the shell thinks you are trying to list your home directory under the bucket, which will only exist if explicitly created.</p>
<p>Attempting to list a bucket using inline credentials is a means of verifying that the key and secret can access a bucket;</p>

<div>
<div>
<pre class="source">hadoop fs -ls s3a://key:secret@my-bucket/
</pre></div></div>

<p>Do escape any <tt>+</tt> or <tt>/</tt> symbols in the secret, as discussed below, and never share the URL, logs generated using it, or use such an inline authentication mechanism in production.</p>
<p>Finally, if you set the environment variables, you can take advantage of S3A&#x2019;s support of environment-variable authentication by attempting the same ls operation. That is: unset the <tt>fs.s3a</tt> secrets and rely on the environment variables.</p>
<div class="section">
<h3><a name="Authentication_failure_due_to_clock_skew"></a>Authentication failure due to clock skew</h3>
<p>The timestamp is used in signing to S3, so as to defend against replay attacks. If the system clock is too far behind <i>or ahead</i> of Amazon&#x2019;s, requests will be rejected.</p>
<p>This can surface as the situation where read requests are allowed, but operations which write to the bucket are denied.</p>
<p>Check the system clock.</p></div>
<div class="section">
<h3><a name="Authentication_failure_when_using_URLs_with_embedded_secrets"></a>Authentication failure when using URLs with embedded secrets</h3>
<p>If using the (strongly discouraged) mechanism of including the AWS Key and secret in a URL, then both &#x201c;+&#x201d; and &#x201c;/&#x201d; symbols need to encoded in the URL. As many AWS secrets include these characters, encoding problems are not uncommon.</p>
<table border="0" class="bodyTable">
<thead>

<tr class="a">
<th> symbol </th>
<th> encoded  value</th></tr>
</thead><tbody>

<tr class="b">
<td> <tt>+</tt> </td>
<td> <tt>%2B</tt> </td></tr>
<tr class="a">
<td> <tt>/</tt> </td>
<td> <tt>%2F</tt> </td></tr>
</tbody>
</table>
<p>As an example, a URL for <tt>bucket</tt> with AWS ID <tt>user1</tt> and secret <tt>a+b/c</tt> would be represented as</p>

<div>
<div>
<pre class="source">s3a://user1:a%2Bb%2Fc@bucket/
</pre></div></div>

<p>This technique is only needed when placing secrets in the URL. Again, this is something users are strongly advised against using.</p></div>
<div class="section">
<h3><a name="a.E2.80.9CBad_Request.E2.80.9D_exception_when_working_with_AWS_S3_Frankfurt.2C_Seoul.2C_or_other_.E2.80.9CV4.E2.80.9D_endpoint"></a><a name="bad_request"></a> &#x201c;Bad Request&#x201d; exception when working with AWS S3 Frankfurt, Seoul, or other &#x201c;V4&#x201d; endpoint</h3>
<p>S3 Frankfurt and Seoul <i>only</i> support <a class="externalLink" href="http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html">the V4 authentication API</a>.</p>
<p>Requests using the V2 API will be rejected with 400 <tt>Bad Request</tt></p>

<div>
<div>
<pre class="source">$ bin/hadoop fs -ls s3a://frankfurt/
WARN s3a.S3AFileSystem: Client: Amazon S3 error 400: 400 Bad Request; Bad Request (retryable)

com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 923C5D9E75E44C06), S3 Extended Request ID: HDwje6k+ANEeDsM6aJ8+D5gUmNAMguOk2BvZ8PH3g9z0gpH+IuwT7N19oQOnIr5CIx7Vqb/uThE=
    at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)
    at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1107)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1070)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:307)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:284)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2793)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:101)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2830)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2812)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
    at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
    at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:235)
    at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:218)
    at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)
    at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
    at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
    at org.apache.hadoop.fs.FsShell.main(FsShell.java:373)
ls: doesBucketExist on frankfurt-new: com.amazonaws.services.s3.model.AmazonS3Exception:
  Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request;
</pre></div></div>

<p>This happens when trying to work with any S3 service which only supports the &#x201c;V4&#x201d; signing API &#x2014;but the client is configured to use the default S3 service endpoint.</p>
<p>The S3A client needs to be given the endpoint to use via the <tt>fs.s3a.endpoint</tt> property.</p>
<p>As an example, the endpoint for S3 Frankfurt is <tt>s3.eu-central-1.amazonaws.com</tt>:</p>

<div>
<div>
<pre class="source">&lt;property&gt;
  &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
  &lt;value&gt;s3.eu-central-1.amazonaws.com&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
</div></div>
<div class="section">
<h2><a name="Connectivity_Problems"></a><a name="connectivity"></a> Connectivity Problems</h2>
<div class="section">
<h3><a name="Error_message_.E2.80.9CThe_bucket_you_are_attempting_to_access_must_be_addressed_using_the_specified_endpoint.E2.80.9D"></a><a name="bad_endpoint"></a> Error message &#x201c;The bucket you are attempting to access must be addressed using the specified endpoint&#x201d;</h3>
<p>This surfaces when <tt>fs.s3a.endpoint</tt> is configured to use an S3 service endpoint which is neither the original AWS one, <tt>s3.amazonaws.com</tt> , nor the one where the bucket is hosted.  The error message contains the redirect target returned by S3, which can be used to determine the correct value for <tt>fs.s3a.endpoint</tt>.</p>

<div>
<div>
<pre class="source">org.apache.hadoop.fs.s3a.AWSS3IOException: Received permanent redirect response
  to bucket.s3-us-west-2.amazonaws.com.  This likely indicates that the S3
  endpoint configured in fs.s3a.endpoint does not match the AWS region
  containing the bucket.: The bucket you are attempting to access must be
  addressed using the specified endpoint. Please send all future requests to
  this endpoint. (Service: Amazon S3; Status Code: 301;
  Error Code: PermanentRedirect; Request ID: 7D39EC1021C61B11)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:132)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initMultipartUploads(S3AFileSystem.java:287)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:203)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2895)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:102)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2932)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2914)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:390)
</pre></div></div>

<ol style="list-style-type: decimal">

<li>Use the <a class="externalLink" href="http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region">Specific endpoint of the bucket&#x2019;s S3 service</a></li>
<li>If not using &#x201c;V4&#x201d; authentication (see above), the original S3 endpoint can be used:</li>
</ol>

<div>
<div>
<pre class="source">&lt;property&gt;
  &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
  &lt;value&gt;s3.amazonaws.com&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>

<p>Using the explicit endpoint for the region is recommended for speed and to use the V4 signing API.</p></div>
<div class="section">
<h3><a name="a.E2.80.9CTimeout_waiting_for_connection_from_pool.E2.80.9D_when_writing_data"></a><a name="timeout"></a> &#x201c;Timeout waiting for connection from pool&#x201d; when writing data</h3>
<p>This happens when using the output stream thread pool runs out of capacity.</p>

<div>
<div>
<pre class="source">[s3a-transfer-shared-pool1-t20] INFO  http.AmazonHttpClient (AmazonHttpClient.java:executeHelper(496)) - Unable to execute HTTP request: Timeout waiting for connection from poolorg.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool
  at org.apache.http.impl.conn.PoolingClientConnectionManager.leaseConnection(PoolingClientConnectionManager.java:230)
  at org.apache.http.impl.conn.PoolingClientConnectionManager$1.getConnection(PoolingClientConnectionManager.java:199)
  at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)
  at com.amazonaws.http.conn.$Proxy10.getConnection(Unknown Source)
  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:424)
  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)
  at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:728)
  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
  at com.amazonaws.services.s3.AmazonS3Client.doUploadPart(AmazonS3Client.java:2921)
  at com.amazonaws.services.s3.AmazonS3Client.uploadPart(AmazonS3Client.java:2906)
  at org.apache.hadoop.fs.s3a.S3AFileSystem.uploadPart(S3AFileSystem.java:1025)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload$1.call(S3ABlockOutputStream.java:360)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload$1.call(S3ABlockOutputStream.java:355)
  at org.apache.hadoop.fs.s3a.BlockingThreadPoolExecutorService$CallableWithPermitRelease.call(BlockingThreadPoolExecutorService.java:239)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
</pre></div></div>

<p>Make sure that <tt>fs.s3a.connection.maximum</tt> is at least larger than <tt>fs.s3a.threads.max</tt>.</p>

<div>
<div>
<pre class="source">&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.max&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.maximum&lt;/name&gt;
  &lt;value&gt;30&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>
</div>
<div class="section">
<h3><a name="a.E2.80.9CTimeout_waiting_for_connection_from_pool.E2.80.9D_when_reading_data"></a>&#x201c;Timeout waiting for connection from pool&#x201d; when reading data</h3>
<p>This happens when more threads are trying to read from an S3A system than the maximum number of allocated HTTP connections.</p>
<p>Set <tt>fs.s3a.connection.maximum</tt> to a larger value (and at least as large as <tt>fs.s3a.threads.max</tt>)</p></div>
<div class="section">
<h3><a name="Out_of_heap_memory_when_writing_with_via_Fast_Upload"></a>Out of heap memory when writing with via Fast Upload</h3>
<p>This can happen when using the upload buffering mechanism uses memory (either <tt>fs.s3a.fast.upload.buffer=array</tt> or <tt>fs.s3a.fast.upload.buffer=bytebuffer</tt>).</p>
<p>More data is being generated than in the JVM than it can upload to S3 &#x2014;and so much data has been buffered that the JVM has run out of memory.</p>
<ol style="list-style-type: decimal">

<li>

<p>Consult <a href="./index.html#fast_upload_thread_tuning">S3A Fast Upload Thread Tuning</a> for detail on this issue and options to address it.</p>
</li>
<li>

<p>Switch to buffering to disk, rather than memory.</p>
</li>
</ol>
<p>This surfaces if, while a multipart upload was taking place, all outstanding multipart uploads were garbage collected. The upload operation cannot complete because the data uploaded has been deleted.</p>
<p>Consult <a href="./index.html#multipart_purge">Cleaning up After Incremental Upload Failures</a> for details on how the multipart purge timeout can be set. If multipart uploads are failing with the message above, it may be a sign that this value is too low.</p></div>
<div class="section">
<h3><a name="MultiObjectDeleteException_during_delete_or_rename_of_files"></a><tt>MultiObjectDeleteException</tt> during delete or rename of files</h3>

<div>
<div>
<pre class="source">Exception in thread &quot;main&quot; com.amazonaws.services.s3.model.MultiObjectDeleteException:
    Status Code: 0, AWS Service: null, AWS Request ID: null, AWS Error Code: null,
    AWS Error Message: One or more objects could not be deleted, S3 Extended Request ID: null
  at com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:1745)
</pre></div></div>

<p>This happens when trying to delete multiple objects, and one of the objects could not be deleted. It <i>should not occur</i> just because the object is missing. More specifically: at the time this document was written, we could not create such a failure.</p>
<p>It will occur if the caller lacks the permission to delete any of the objects.</p>
<p>Consult the log to see the specifics of which objects could not be deleted. Do you have permission to do so?</p>
<p>If this operation is failing for reasons other than the caller lacking permissions:</p>
<ol style="list-style-type: decimal">

<li>Try setting <tt>fs.s3a.multiobjectdelete.enable</tt> to <tt>false</tt>.</li>
<li>Consult <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-11572">HADOOP-11572</a> for up to date advice.</li>
</ol></div>
<div class="section">
<h3><a name="a.E2.80.9CFailed_to_Sanitize_XML_document.E2.80.9D"></a>&#x201c;Failed to Sanitize XML document&#x201d;</h3>

<div>
<div>
<pre class="source">org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on test/testname/streaming/:
  com.amazonaws.AmazonClientException: Failed to sanitize XML document
  destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler:
  Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:105)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1462)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1227)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1203)
    at org.apache.hadoop.fs.s3a.S3AGlobber.listStatus(S3AGlobber.java:69)
    at org.apache.hadoop.fs.s3a.S3AGlobber.doGlob(S3AGlobber.java:210)
    at org.apache.hadoop.fs.s3a.S3AGlobber.glob(S3AGlobber.java:125)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:1853)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.globStatus(S3AFileSystem.java:1841)
</pre></div></div>

<p>We believe this is caused by the connection to S3 being broken. See <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-13811">HADOOP-13811</a>.</p>
<p>It may go away if the operation is retried.</p></div>
<div class="section">
<h3><a name="JSON_Parse_Error_from_AWS_SDK"></a>JSON Parse Error from AWS SDK</h3>
<p>Sometimes a JSON Parse error is reported with the stack trace in the <tt>com.amazonaws</tt>,</p>
<p>Again, we believe this is caused by the connection to S3 being broken.</p>
<p>It may go away if the operation is retried.</p></div></div>
<div class="section">
<h2><a name="Miscellaneous_Errors"></a>Miscellaneous Errors</h2>
<div class="section">
<h3><a name="When_writing_data:_.E2.80.9Cjava.io.FileNotFoundException:_Completing_multi-part_upload.E2.80.9D"></a>When writing data: &#x201c;java.io.FileNotFoundException: Completing multi-part upload&#x201d;</h3>

<div>
<div>
<pre class="source">java.io.FileNotFoundException: Completing multi-part upload on fork-5/test/multipart/1c397ca6-9dfb-4ac1-9cf7-db666673246b: com.amazonaws.services.s3.model.AmazonS3Exception: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchUpload; Request ID: 84FF8057174D9369), S3 Extended Request ID: Ij5Yn6Eq/qIERH4Z6Io3YL2t9/qNZ7z9gjPb1FrTtTovZ8k1MXqh+zCYYjqmfJ/fCY6E1+JR9jA=
  at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)
  at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)
  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
  at com.amazonaws.services.s3.AmazonS3Client.completeMultipartUpload(AmazonS3Client.java:2705)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:473)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:382)
  at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:272)
  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
  at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
</pre></div></div>
</div>
<div class="section">
<h3><a name="Issue:_when_writing_data.2C_HTTP_Exceptions_logged_at_info_from_AmazonHttpClient"></a>Issue: when writing data, HTTP Exceptions logged at info from <tt>AmazonHttpClient</tt></h3>

<div>
<div>
<pre class="source">[s3a-transfer-shared-pool4-t6] INFO  http.AmazonHttpClient (AmazonHttpClient.java:executeHelper(496)) - Unable to execute HTTP request: hwdev-steve-ireland-new.s3.amazonaws.com:443 failed to respond
org.apache.http.NoHttpResponseException: bucket.s3.amazonaws.com:443 failed to respond
  at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:143)
  at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57)
  at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261)
  at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:283)
  at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:259)
  at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:209)
  at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272)
  at com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:66)
  at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124)
  at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:686)
  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:488)
  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)
  at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:728)
  at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
  at com.amazonaws.services.s3.AmazonS3Client.copyPart(AmazonS3Client.java:1731)
  at com.amazonaws.services.s3.transfer.internal.CopyPartCallable.call(CopyPartCallable.java:41)
  at com.amazonaws.services.s3.transfer.internal.CopyPartCallable.call(CopyPartCallable.java:28)
  at org.apache.hadoop.fs.s3a.BlockingThreadPoolExecutorService$CallableWithPermitRelease.call(BlockingThreadPoolExecutorService.java:239)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
</pre></div></div>

<p>These are HTTP I/O exceptions caught and logged inside the AWS SDK. The client will attempt to retry the operation; it may just be a transient event. If there are many such exceptions in logs, it may be a symptom of connectivity or network problems.</p></div></div>
<div class="section">
<h2><a name="File_System_Semantics"></a>File System Semantics</h2>
<p>These are the issues where S3 does not appear to behave the way a filesystem &#x201c;should&#x201d;.</p>
<div class="section">
<h3><a name="Visible_S3_Inconsistency"></a>Visible S3 Inconsistency</h3>
<p>Amazon S3 is <i>an eventually consistent object store</i>. That is: not a filesystem.</p>
<p>To reduce visible inconsistencies, use the <a href="./s3guard.html">S3Guard</a> consistency cache.</p>
<p>By default, Amazon S3 offers read-after-create consistency: a newly created file is immediately visible. There is a small quirk: a negative GET may be cached, such that even if an object is immediately created, the fact that there &#x201c;wasn&#x2019;t&#x201d; an object is still remembered.</p>
<p>That means the following sequence on its own will be consistent</p>

<div>
<div>
<pre class="source">touch(path) -&gt; getFileStatus(path)
</pre></div></div>

<p>But this sequence <i>may</i> be inconsistent.</p>

<div>
<div>
<pre class="source">getFileStatus(path) -&gt; touch(path) -&gt; getFileStatus(path)
</pre></div></div>

<p>A common source of visible inconsistencies is that the S3 metadata database &#x2014;the part of S3 which serves list requests&#x2014; is updated asynchronously. Newly added or deleted files may not be visible in the index, even though direct operations on the object (<tt>HEAD</tt> and <tt>GET</tt>) succeed.</p>
<p>That means the <tt>getFileStatus()</tt> and <tt>open()</tt> operations are more likely to be consistent with the state of the object store, but without S3Guard enabled, directory list operations such as <tt>listStatus()</tt>, <tt>listFiles()</tt>, <tt>listLocatedStatus()</tt>, and <tt>listStatusIterator()</tt> may not see newly created files, and still list old files.</p></div>
<div class="section">
<h3><a name="FileNotFoundException_even_though_the_file_was_just_written."></a><tt>FileNotFoundException</tt> even though the file was just written.</h3>
<p>This can be a sign of consistency problems. It may also surface if there is some asynchronous file write operation still in progress in the client: the operation has returned, but the write has not yet completed. While the S3A client code does block during the <tt>close()</tt> operation, we suspect that asynchronous writes may be taking place somewhere in the stack &#x2014;this could explain why parallel tests fail more often than serialized tests.</p></div>
<div class="section">
<h3><a name="File_not_found_in_a_directory_listing.2C_even_though_getFileStatus.28.29_finds_it"></a>File not found in a directory listing, even though <tt>getFileStatus()</tt> finds it</h3>
<p>(Similarly: deleted file found in listing, though <tt>getFileStatus()</tt> reports that it is not there)</p>
<p>This is a visible sign of updates to the metadata server lagging behind the state of the underlying filesystem.</p>
<p>Fix: Use S3Guard</p></div>
<div class="section">
<h3><a name="File_not_visible.2Fsaved"></a>File not visible/saved</h3>
<p>The files in an object store are not visible until the write has been completed. In-progress writes are simply saved to a local file/cached in RAM and only uploaded. at the end of a write operation. If a process terminated unexpectedly, or failed to call the <tt>close()</tt> method on an output stream, the pending data will have been lost.</p></div>
<div class="section">
<h3><a name="File_flush.28.29.2C_hsync_and_hflush.28.29_calls_do_not_save_data_to_S3"></a>File <tt>flush()</tt>, <tt>hsync</tt> and <tt>hflush()</tt> calls do not save data to S3</h3>
<p>Again, this is due to the fact that the data is cached locally until the <tt>close()</tt> operation. The S3A filesystem cannot be used as a store of data if it is required that the data is persisted durably after every <tt>Syncable.hflush()</tt> or <tt>Syncable.hsync()</tt> call. This includes resilient logging, HBase-style journalling and the like. The standard strategy here is to save to HDFS and then copy to S3.</p></div></div>
<div class="section">
<h2><a name="S3_Server_Side_Encryption"></a><a name="encryption"></a> S3 Server Side Encryption</h2>
<div class="section">
<h3><a name="Using_SSE-KMS_.E2.80.9CInvalid_arn.E2.80.9D"></a>Using SSE-KMS &#x201c;Invalid arn&#x201d;</h3>
<p>When performing file operations, the user may run into an issue where the KMS key arn is invalid.</p>

<div>
<div>
<pre class="source">com.amazonaws.services.s3.model.AmazonS3Exception:
Invalid arn (Service: Amazon S3; Status Code: 400; Error Code: KMS.NotFoundException; Request ID: 708284CF60EE233F),
S3 Extended Request ID: iHUUtXUSiNz4kv3Bdk/hf9F+wjPt8GIVvBHx/HEfCBYkn7W6zmpvbA3XT7Y5nTzcZtfuhcqDunw=:
Invalid arn (Service: Amazon S3; Status Code: 400; Error Code: KMS.NotFoundException; Request ID: 708284CF60EE233F)
</pre></div></div>

<p>This is due to either, the KMS key id is entered incorrectly, or the KMS key id is in a different region than the S3 bucket being used.</p></div>
<div class="section">
<h3><a name="Using_SSE-C_.E2.80.9CBad_Request.E2.80.9D"></a>Using SSE-C &#x201c;Bad Request&#x201d;</h3>
<p>When performing file operations the user may run into an unexpected 400/403 error such as</p>

<div>
<div>
<pre class="source">org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on fork-4/: com.amazonaws.services.s3.model.AmazonS3Exception:
Bad Request (Service: Amazon S3; Status Code: 400;
Error Code: 400 Bad Request; Request ID: 42F9A1987CB49A99),
S3 Extended Request ID: jU2kcwaXnWj5APB14Cgb1IKkc449gu2+dhIsW/+7x9J4D+VUkKvu78mBo03oh9jnOT2eoTLdECU=:
Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 42F9A1987CB49A99)
</pre></div></div>

<p>This can happen in the cases of not specifying the correct SSE-C encryption key. Such cases can be as follows: 1. An object is encrypted using SSE-C on S3 and either the wrong encryption type is used, no encryption is specified, or the SSE-C specified is incorrect. 2. A directory is encrypted with a SSE-C keyA and the user is trying to move a file using configured SSE-C keyB into that structure.</p></div></div>
<div class="section">
<h2><a name="Performance"></a><a name="performance"></a> Performance</h2>
<p>S3 is slower to read data than HDFS, even on virtual clusters running on Amazon EC2.</p>
<ul>

<li>HDFS replicates data for faster query performance.</li>
<li>HDFS stores the data on the local hard disks, avoiding network traffic if the code can be executed on that host. As EC2 hosts often have their network bandwidth throttled, this can make a tangible difference.</li>
<li>HDFS is significantly faster for many &#x201c;metadata&#x201d; operations: listing the contents of a directory, calling <tt>getFileStatus()</tt> on path, creating or deleting directories. (S3Guard reduces but does not eliminate the speed gap).</li>
<li>On HDFS, Directory renames and deletes are <tt>O(1)</tt> operations. On S3 renaming is a very expensive <tt>O(data)</tt> operation which may fail partway through in which case the final state depends on where the copy+ delete sequence was when it failed. All the objects are copied, then the original set of objects are deleted, so a failure should not lose data &#x2014;it may result in duplicate datasets.</li>
<li>Unless fast upload enabled, the write only begins on a <tt>close()</tt> operation. This can take so long that some applications can actually time out.</li>
<li>File IO involving many seek calls/positioned read calls will encounter performance problems due to the size of the HTTP requests made. Enable the &#x201c;random&#x201d; fadvise policy to alleviate this at the expense of sequential read performance and bandwidth.</li>
</ul>
<p>The slow performance of <tt>rename()</tt> surfaces during the commit phase of work, including</p>
<ul>

<li>The MapReduce <tt>FileOutputCommitter</tt>. This also used by Apache Spark.</li>
<li>DistCp&#x2019;s rename-after-copy operation.</li>
<li>The <tt>hdfs fs -rm</tt> command renaming the file under <tt>.Trash</tt> rather than deleting it. Use <tt>-skipTrash</tt> to eliminate that step.</li>
</ul>
<p>These operations can be significantly slower when S3 is the destination compared to HDFS or other &#x201c;real&#x201d; filesystem.</p>
<p><i>Improving S3 load-balancing behavior</i></p>
<p>Amazon S3 uses a set of front-end servers to provide access to the underlying data. The choice of which front-end server to use is handled via load-balancing DNS service: when the IP address of an S3 bucket is looked up, the choice of which IP address to return to the client is made based on the the current load of the front-end servers.</p>
<p>Over time, the load across the front-end changes, so those servers considered &#x201c;lightly loaded&#x201d; will change. If the DNS value is cached for any length of time, your application may end up talking to an overloaded server. Or, in the case of failures, trying to talk to a server that is no longer there.</p>
<p>And by default, for historical security reasons in the era of applets, the DNS TTL of a JVM is &#x201c;infinity&#x201d;.</p>
<p>To work with AWS better, set the DNS time-to-live of an application which works with S3 to something lower. See <a class="externalLink" href="http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-jvm-ttl.html">AWS documentation</a>.</p></div>
<div class="section">
<h2><a name="Troubleshooting_network_performance"></a><a name="network_performance"></a>Troubleshooting network performance</h2>
<p>An example of this is covered in <a class="externalLink" href="https://issues.apache.org/jira/browse/HADOOP-13871">HADOOP-13871</a>.</p>
<ol style="list-style-type: decimal">

<li>For public data, use <tt>curl</tt>:

<div>
<div>
<pre class="source">curl -O https://landsat-pds.s3.amazonaws.com/scene_list.gz
</pre></div></div>
</li>
<li>Use <tt>nettop</tt> to monitor a processes connections.</li>
</ol>
<p>Consider reducing the connection timeout of the s3a connection.</p>

<div>
<div>
<pre class="source">&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.timeout&lt;/name&gt;
  &lt;value&gt;15000&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>

<p>This <i>may</i> cause the client to react faster to network pauses, so display stack traces fast. At the same time, it may be less resilient to connectivity problems.</p></div>
<div class="section">
<h2><a name="Other_Issues"></a>Other Issues</h2>
<div class="section">
<h3><a name="Enabling_low-level_logging"></a><a name="logging"></a> Enabling low-level logging</h3>
<p>The AWS SDK and the Apache S3 components can be configured to log at more detail, as can S3A itself.</p>

<div>
<div>
<pre class="source">log4j.logger.org.apache.hadoop.fs.s3a=DEBUG
log4j.logger.com.amazonaws.request=DEBUG
log4j.logger.com.amazonaws.thirdparty.apache.http=DEBUG
</pre></div></div>

<p>If using the &#x201c;unshaded&#x201d; JAR, then the Apache HttpClient can be directly configured:</p>

<div>
<div>
<pre class="source">log4j.logger.org.apache.http=DEBUG
</pre></div></div>

<p>This produces a log such as this, wich is for a V4-authenticated PUT of a 0-byte file used as an empty directory marker</p>

<div>
<div>
<pre class="source">execchain.MainClientExec (MainClientExec.java:execute(255)) - Executing request PUT /test/ HTTP/1.1
execchain.MainClientExec (MainClientExec.java:execute(266)) - Proxy auth state: UNCHALLENGED
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(135)) - http-outgoing-0 &gt;&gt; PUT /test/ HTTP/1.1
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; Host: ireland-new.s3-eu-west-1.amazonaws.com
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; x-amz-content-sha256: UNSIGNED-PAYLOAD
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; Authorization: AWS4-HMAC-SHA256 Credential=AKIAIYZ5JEEEER/20170904/eu-west-1/s3/aws4_request,  ...
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; X-Amz-Date: 20170904T172929Z
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; User-Agent: Hadoop 3.0.0-beta-1, aws-sdk-java/1.11.134 ...
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; amz-sdk-invocation-id: 75b530f8-ad31-1ad3-13db-9bd53666b30d
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; amz-sdk-retry: 0/0/500
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; Content-Type: application/octet-stream
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; Content-Length: 0
http.headers (LoggingManagedHttpClientConnection.java:onRequestSubmitted(138)) - http-outgoing-0 &gt;&gt; Connection: Keep-Alive
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;PUT /test/ HTTP/1.1[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;Host: ireland-new.s3-eu-west-1.amazonaws.com[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;x-amz-content-sha256: UNSIGNED-PAYLOAD[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;Authorization: AWS4-HMAC-SHA256 Credential=AKIAIYZ5JEEEER/20170904/eu-west-1/s3/aws4_request, ,,,
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;X-Amz-Date: 20170904T172929Z[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;User-Agent: 3.0.0-beta-1, aws-sdk-java/1.11.134  ...
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;amz-sdk-invocation-id: 75b530f8-ad31-1ad3-13db-9bd53666b30d[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;amz-sdk-retry: 0/0/500[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;Content-Type: application/octet-stream[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;Content-Length: 0[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;Connection: Keep-Alive[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &gt;&gt; &quot;[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;HTTP/1.1 200 OK[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;x-amz-id-2: mad9GqKztzlL0cdnCKAj9GJOAs+DUjbSC5jRkO7W1E7Nk2BUmFvt81bhSNPGdZmyyKqQI9i/B/A=[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;x-amz-request-id: C953D2FE4ABF5C51[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;Date: Mon, 04 Sep 2017 17:29:30 GMT[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;ETag: &quot;d41d8cd98f00b204e9800998ecf8427e&quot;[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;Content-Length: 0[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;Server: AmazonS3[\r][\n]&quot;
http.wire (Wire.java:wire(72)) - http-outgoing-0 &lt;&lt; &quot;[\r][\n]&quot;
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(124)) - http-outgoing-0 &lt;&lt; HTTP/1.1 200 OK
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(127)) - http-outgoing-0 &lt;&lt; x-amz-id-2: mad9GqKztzlL0cdnCKAj9GJOAs+DUjbSC5jRkO7W1E7Nk2BUmFvt81bhSNPGdZmyyKqQI9i/B/A=
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(127)) - http-outgoing-0 &lt;&lt; x-amz-request-id: C953D2FE4ABF5C51
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(127)) - http-outgoing-0 &lt;&lt; Date: Mon, 04 Sep 2017 17:29:30 GMT
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(127)) - http-outgoing-0 &lt;&lt; ETag: &quot;d41d8cd98f00b204e9800998ecf8427e&quot;
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(127)) - http-outgoing-0 &lt;&lt; Content-Length: 0
http.headers (LoggingManagedHttpClientConnection.java:onResponseReceived(127)) - http-outgoing-0 &lt;&lt; Server: AmazonS3
execchain.MainClientExec (MainClientExec.java:execute(284)) - Connection can be kept alive for 60000 MILLISECONDS
</pre></div></div></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2008-2018
              Apache Software Foundation
            
                          - <a href="https://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
