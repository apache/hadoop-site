<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Generated by Apache Maven Doxia at 2014-06-21 -->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 2.4.1 - Hadoop Distributed File System-2.4.1 - HDFS NFS Gateway</title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20140621" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      </head>
  <body class="composite">
    <div id="banner">
                  <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                        <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
                &gt;
                      <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
                &gt;
                      <a href="../">Apache Hadoop Project Dist POM</a>
                &gt;
                Apache Hadoop 2.4.1
                </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://svn.apache.org/repos/asf/hadoop/" class="externalLink">SVN</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                &nbsp;| Last Published: 2014-06-21
              &nbsp;| Version: 2.4.1
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CommandsManual.html">Hadoop Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/FileSystemShell.html">File System Shell</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Compatibility.html">Hadoop Compatibility</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Superusers.html">Superusers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">HDFS User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">High Availability With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">High Availability With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">HDFS Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/Hftp.html">HFTP</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">C API libhdfs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS REST API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-hdfs-httpfs/index.html">HttpFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">HDFS NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">HDFS Rolling Upgrade</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibilty between Hadoop 1.x and Hadoop 2.x</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YARN.html">YARN Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">YARN Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">YARN Commands</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">History Server</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/releasenotes.html">Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../api/index.html">API docs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CHANGES.txt">Common CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/CHANGES.txt">HDFS CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-mapreduce/CHANGES.txt">MapReduce CHANGES.txt</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="./images/logos/maven-feather.png"/>
        </a>
                       
                            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!-- Licensed under the Apache License, Version 2.0 (the "License"); --><!-- you may not use this file except in compliance with the License. --><!-- You may obtain a copy of the License at --><!--  --><!-- http://www.apache.org/licenses/LICENSE-2.0 --><!--  --><!-- Unless required by applicable law or agreed to in writing, software --><!-- distributed under the License is distributed on an "AS IS" BASIS, --><!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. --><!-- See the License for the specific language governing permissions and --><!-- limitations under the License. See accompanying LICENSE file. --><div class="section">
<h2>HDFS NFS Gateway<a name="HDFS_NFS_Gateway"></a></h2>
<p>[ <a href="./index.html">Go Back</a> ]</p>
<ul>
<li><a href="#HDFS_NFS_Gateway">HDFS NFS Gateway</a>
<ul>
<li><a href="#Overview">Overview</a></li>
<li><a href="#Configuration">Configuration</a></li>
<li><a href="#Start_and_stop_NFS_gateway_service">Start and stop NFS gateway service</a></li>
<li><a href="#Verify_validity_of_NFS_related_services">Verify validity of NFS related services</a></li>
<li><a href="#Mount_the_export_">Mount the export &#x201c;/&#x201d;</a></li>
<li><a href="#User_authentication_and_mapping">User authentication and mapping</a></li></ul></li></ul>
<div class="section">
<h3><a name="Overview">Overview</a></h3>
<p>The NFS Gateway supports NFSv3 and allows HDFS to be mounted as part of the client's local file system. Currently NFS Gateway supports and enables the following usage patterns:</p>
<ul>
<li>Users can browse the HDFS file system through their local file system on NFSv3 client compatible operating systems.</li>
<li>Users can download files from the the HDFS file system on to their local file system.</li>
<li>Users can upload files from their local file system directly to the HDFS file system.</li>
<li>Users can stream data directly to HDFS through the mount point. File append is supported but random write is not supported. </li></ul>
<p>The NFS gateway machine needs the same thing to run an HDFS client like Hadoop JAR files, HADOOP_CONF directory. The NFS gateway can be on the same host as DataNode, NameNode, or any HDFS client. </p></div>
<div class="section">
<h3><a name="Configuration">Configuration</a></h3>
<p>The user running the NFS-gateway must be able to proxy all the users using the NFS mounts. For instance, if user 'nfsserver' is running the gateway, and users belonging to the groups 'nfs-users1' and 'nfs-users2' use the NFS mounts, then in core-site.xml of the namenode, the following must be set (NOTE: replace 'nfsserver' with the user name starting the gateway in your cluster):</p>
<div>
<pre>&lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.nfsserver.groups&lt;/name&gt;
  &lt;value&gt;nfs-users1,nfs-users2&lt;/value&gt;
  &lt;description&gt;
         The 'nfsserver' user is allowed to proxy all members of the 'nfs-users1' and 
         'nfs-users2' groups. Set this to '*' to allow nfsserver user to proxy any group.
  &lt;/description&gt;
&lt;/property&gt;</pre></div>
<div>
<pre>&lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.nfsserver.hosts&lt;/name&gt;
  &lt;value&gt;nfs-client-host1.com&lt;/value&gt;
  &lt;description&gt;
         This is the host where the nfs gateway is running. Set this to '*' to allow
         requests from any hosts to be proxied.
  &lt;/description&gt;
&lt;/property&gt;</pre></div>
<p>The above are the only required configuration for the NFS gateway in non-secure mode. For Kerberized hadoop clusters, the following configurations need to be added to hdfs-site.xml:</p>
<div>
<pre>  &lt;property&gt;
    &lt;name&gt;dfs.nfsgateway.keytab.file&lt;/name&gt;
    &lt;value&gt;/etc/hadoop/conf/nfsserver.keytab&lt;/value&gt; &lt;!-- path to the nfs gateway keytab --&gt;
  &lt;/property&gt;</pre></div>
<div>
<pre>  &lt;property&gt;
    &lt;name&gt;dfs.nfsgateway.kerberos.principal&lt;/name&gt;
    &lt;value&gt;nfsserver/_HOST@YOUR-REALM.COM&lt;/value&gt;
  &lt;/property&gt;</pre></div>
<p>It's strongly recommended for the users to update a few configuration properties based on their use cases. All the related configuration properties can be added or updated in hdfs-site.xml.</p>
<ul>
<li>If the client mounts the export with access time update allowed, make sure the following property is not disabled in the configuration file. Only NameNode needs to restart after this property is changed. On some Unix systems, the user can disable access time update by mounting the export with &quot;noatime&quot;. If the export is mounted with &quot;noatime&quot;, the user doesn't need to change the following property and thus no need to restart namenode.
<div>
<pre>&lt;property&gt;
  &lt;name&gt;dfs.namenode.accesstime.precision&lt;/name&gt;
  &lt;value&gt;3600000&lt;/value&gt;
  &lt;description&gt;The access time for HDFS file is precise upto this value.
    The default value is 1 hour. Setting a value of 0 disables
    access times for HDFS.
  &lt;/description&gt;
&lt;/property&gt;</pre></div></li>
<li>Users are expected to update the file dump directory. NFS client often reorders writes. Sequential writes can arrive at the NFS gateway at random order. This directory is used to temporarily save out-of-order writes before writing to HDFS. For each file, the out-of-order writes are dumped after they are accumulated to exceed certain threshold (e.g., 1MB) in memory. One needs to make sure the directory has enough space. For example, if the application uploads 10 files with each having 100MB, it is recommended for this directory to have roughly 1GB space in case if a worst-case write reorder happens to every file. Only NFS gateway needs to restart after this property is updated.
<div>
<pre>  &lt;property&gt;    
    &lt;name&gt;dfs.nfs3.dump.dir&lt;/name&gt;
    &lt;value&gt;/tmp/.hdfs-nfs&lt;/value&gt;
  &lt;/property&gt;</pre></div></li>
<li>For optimal performance, it is recommended that rtmax be updated to 1MB. However, note that this 1MB is a per client allocation, and not from a shared memory pool, and therefore a larger value may adversely affect small reads, consuming a lot of memory. The maximum value of this property is 1MB.
<div>
<pre>&lt;property&gt;
  &lt;name&gt;dfs.nfs.rtmax&lt;/name&gt;
  &lt;value&gt;1048576&lt;/value&gt;
  &lt;description&gt;This is the maximum size in bytes of a READ request
    supported by the NFS gateway. If you change this, make sure you
    also update the nfs mount's rsize(add rsize= # of bytes to the 
    mount directive).
  &lt;/description&gt;
&lt;/property&gt;</pre></div>
<div>
<pre>&lt;property&gt;
  &lt;name&gt;dfs.nfs.wtmax&lt;/name&gt;
  &lt;value&gt;65536&lt;/value&gt;
  &lt;description&gt;This is the maximum size in bytes of a WRITE request
    supported by the NFS gateway. If you change this, make sure you
    also update the nfs mount's wsize(add wsize= # of bytes to the 
    mount directive).
  &lt;/description&gt;
&lt;/property&gt;</pre></div></li></ul>
<ul>
<li>By default, the export can be mounted by any client. To better control the access, users can update the following property. The value string contains machine name and access privilege, separated by whitespace characters. Machine name format can be single host, wildcards, and IPv4 networks.The access privilege uses rw or ro to specify readwrite or readonly access of the machines to exports. If the access privilege is not provided, the default is read-only. Entries are separated by &quot;;&quot;. For example: &quot;192.168.0.0/22 rw ; host*.example.com ; host1.test.org ro;&quot;. Only NFS gateway needs to restart after this property is updated.
<div>
<pre>&lt;property&gt;
  &lt;name&gt;dfs.nfs.exports.allowed.hosts&lt;/name&gt;
  &lt;value&gt;* rw&lt;/value&gt;
&lt;/property&gt;</pre></div></li>
<li>Customize log settings. To get NFS debug trace, users can edit the log4j.property file to add the following. Note, debug trace, especially for ONCRPC, can be very verbose.
<p>To change logging level:</p>
<div>
<pre>    log4j.logger.org.apache.hadoop.hdfs.nfs=DEBUG</pre></div>
<p>To get more details of ONCRPC requests:</p>
<div>
<pre>    log4j.logger.org.apache.hadoop.oncrpc=DEBUG</pre></div></li></ul></div>
<div class="section">
<h3><a name="Start_and_stop_NFS_gateway_service">Start and stop NFS gateway service</a></h3>
<p>Three daemons are required to provide NFS service: rpcbind (or portmap), mountd and nfsd. The NFS gateway process has both nfsd and mountd. It shares the HDFS root &quot;/&quot; as the only export. It is recommended to use the portmap included in NFS gateway package. Even though NFS gateway works with portmap/rpcbind provide by most Linux distributions, the package included portmap is needed on some Linux systems such as REHL6.2 due to an <a class="externalLink" href="https://bugzilla.redhat.com/show_bug.cgi?id=731542">rpcbind bug</a>. More detailed discussions can be found in <a class="externalLink" href="https://issues.apache.org/jira/browse/HDFS-4763">HDFS-4763</a>.</p>
<ol style="list-style-type: decimal">
<li>Stop nfs/rpcbind/portmap services provided by the platform (commands can be different on various Unix platforms):
<div>
<pre>     service nfs stop
      
     service rpcbind stop</pre></div></li>
<li>Start package included portmap (needs root privileges):
<div>
<pre>     hadoop portmap
  
     OR

     hadoop-daemon.sh start portmap</pre></div></li>
<li>Start mountd and nfsd.
<p>No root privileges are required for this command. However, ensure that the user starting the Hadoop cluster and the user starting the NFS gateway are same.</p>
<div>
<pre>     hadoop nfs3

     OR

     hadoop-daemon.sh start nfs3</pre></div>
<p>Note, if the hadoop-daemon.sh script starts the NFS gateway, its log can be found in the hadoop log folder.</p></li>
<li>Stop NFS gateway services.
<div>
<pre>      hadoop-daemon.sh stop nfs3

      hadoop-daemon.sh stop portmap</pre></div></li></ol></div>
<div class="section">
<h3><a name="Verify_validity_of_NFS_related_services">Verify validity of NFS related services</a></h3>
<ol style="list-style-type: decimal">
<li>Execute the following command to verify if all the services are up and running:
<div>
<pre>       rpcinfo -p $nfs_server_ip</pre></div>
<p>You should see output similar to the following:</p>
<div>
<pre>       program vers proto   port

       100005    1   tcp   4242  mountd

       100005    2   udp   4242  mountd

       100005    2   tcp   4242  mountd

       100000    2   tcp    111  portmapper

       100000    2   udp    111  portmapper

       100005    3   udp   4242  mountd

       100005    1   udp   4242  mountd

       100003    3   tcp   2049  nfs

       100005    3   tcp   4242  mountd</pre></div></li>
<li>Verify if the HDFS namespace is exported and can be mounted.
<div>
<pre>        showmount -e $nfs_server_ip                         </pre></div>
<p>You should see output similar to the following:</p>
<div>
<pre>        Exports list on $nfs_server_ip :

        / (everyone)</pre></div></li></ol></div>
<div class="section">
<h3><a name="Mount_the_export_">Mount the export &#x201c;/&#x201d;</a></h3>
<p>Currently NFS v3 only uses TCP as the transportation protocol. NLM is not supported so mount option &quot;nolock&quot; is needed. It's recommended to use hard mount. This is because, even after the client sends all data to NFS gateway, it may take NFS gateway some extra time to transfer data to HDFS when writes were reorderd by NFS client Kernel.</p>
<p>If soft mount has to be used, the user should give it a relatively long timeout (at least no less than the default timeout on the host) .</p>
<p>The users can mount the HDFS namespace as shown below:</p>
<div>
<pre>       mount -t nfs -o vers=3,proto=tcp,nolock $server:/  $mount_point</pre></div>
<p>Then the users can access HDFS as part of the local file system except that, hard link and random write are not supported yet.</p></div>
<div class="section">
<h3><a name="User_authentication_and_mapping">User authentication and mapping</a></h3>
<p>NFS gateway in this release uses AUTH_UNIX style authentication. When the user on NFS client accesses the mount point, NFS client passes the UID to NFS gateway. NFS gateway does a lookup to find user name from the UID, and then passes the username to the HDFS along with the HDFS requests. For example, if the NFS client has current user as &quot;admin&quot;, when the user accesses the mounted directory, NFS gateway will access HDFS as user &quot;admin&quot;. To access HDFS as the user &quot;hdfs&quot;, one needs to switch the current user to &quot;hdfs&quot; on the client system when accessing the mounted directory.</p>
<p>The system administrator must ensure that the user on NFS client host has the same name and UID as that on the NFS gateway host. This is usually not a problem if the same user management system (e.g., LDAP/NIS) is used to create and deploy users on HDFS nodes and NFS client node. In case the user account is created manually in different hosts, one might need to modify UID (e.g., do &quot;usermod -u 123 myusername&quot;) on either NFS client or NFS gateway host in order to make it the same on both sides. More technical details of RPC AUTH_UNIX can be found in <a class="externalLink" href="http://tools.ietf.org/html/rfc1057">RPC specification</a>.</p></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">&#169;            2014
              Apache Software Foundation
            
                       - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a></div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
