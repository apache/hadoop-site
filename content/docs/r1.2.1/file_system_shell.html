<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta content="Apache Forrest" name="Generator">
<meta name="Forrest-version" content="0.9">
<meta name="Forrest-skin-name" content="pelt">
<title>File System Shell Guide</title>
<link type="text/css" href="skin/basic.css" rel="stylesheet">
<link media="screen" type="text/css" href="skin/screen.css" rel="stylesheet">
<link media="print" type="text/css" href="skin/print.css" rel="stylesheet">
<link type="text/css" href="skin/profile.css" rel="stylesheet">
<script src="skin/getBlank.js" language="javascript" type="text/javascript"></script><script src="skin/getMenu.js" language="javascript" type="text/javascript"></script><script src="skin/fontsize.js" language="javascript" type="text/javascript"></script>
<link rel="shortcut icon" href="images/favicon.ico">
</head>
<body onload="init()">
<script type="text/javascript">ndeSetTextSize();</script>
<div id="top">
<!--+
    |breadtrail
    +-->
<div class="breadtrail">
<a href="https://www.apache.org/">Apache</a> &gt; <a href="https://hadoop.apache.org/">Hadoop</a> &gt; <a href="https://hadoop.apache.org/core/">Core</a><script src="skin/breadcrumbs.js" language="JavaScript" type="text/javascript"></script>
</div>
<!--+
    |header
    +-->
<div class="header">
<!--+
    |start group logo
    +-->
<div class="grouplogo">
<a href="https://hadoop.apache.org/"><img class="logoImage" alt="Hadoop" src="images/hadoop-logo.jpg" title="Apache Hadoop"></a>
</div>
<!--+
    |end group logo
    +-->
<!--+
    |start Project Logo
    +-->
<div class="projectlogo">
<a href="https://hadoop.apache.org/core/"><img class="logoImage" alt="Hadoop" src="images/hadoop-logo-2.gif" title="Scalable Computing Platform"></a>
</div>
<!--+
    |end Project Logo
    +-->
<!--+
    |start Search
    +-->
<div class="searchbox">
<form action="http://www.google.com/search" method="get" class="roundtopsmall">
<input value="hadoop.apache.org" name="sitesearch" type="hidden"><input onFocus="getBlank (this, 'Search the site with google');" size="25" name="q" id="query" type="text" value="Search the site with google">&nbsp; 
                    <input name="Search" value="Search" type="submit">
</form>
</div>
<!--+
    |end search
    +-->
<!--+
    |start Tabs
    +-->
<ul id="tabs">
<li>
<a class="unselected" href="https://hadoop.apache.org/core/">Project</a>
</li>
<li>
<a class="unselected" href="https://wiki.apache.org/hadoop">Wiki</a>
</li>
<li class="current">
<a class="selected" href="index.html">Hadoop 1.2.1 Documentation</a>
</li>
</ul>
<!--+
    |end Tabs
    +-->
</div>
</div>
<div id="main">
<div id="publishedStrip">
<!--+
    |start Subtabs
    +-->
<div id="level2tabs"></div>
<!--+
    |end Endtabs
    +-->
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<!--+
    |breadtrail
    +-->
<div class="breadtrail">

             &nbsp;
           </div>
<!--+
    |start Menu, mainarea
    +-->
<!--+
    |start Menu
    +-->
<div id="menu">
<div onclick="SwitchMenu('menu_1.1', 'skin/')" id="menu_1.1Title" class="menutitle">Getting Started</div>
<div id="menu_1.1" class="menuitemgroup">
<div class="menuitem">
<a href="index.html">Overview</a>
</div>
<div class="menuitem">
<a href="single_node_setup.html">Single Node Setup</a>
</div>
<div class="menuitem">
<a href="cluster_setup.html">Cluster Setup</a>
</div>
<div class="menuitem">
<a href="cli_minicluster.html">CLI MiniCluster</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.2', 'skin/')" id="menu_1.2Title" class="menutitle">Guides</div>
<div id="menu_1.2" class="menuitemgroup">
<div class="menuitem">
<a href="HttpAuthentication.html">Authentication for Hadoop HTTP web-consoles</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.3', 'skin/')" id="menu_1.3Title" class="menutitle">MapReduce</div>
<div id="menu_1.3" class="menuitemgroup">
<div class="menuitem">
<a href="mapred_tutorial.html">MapReduce Tutorial</a>
</div>
<div class="menuitem">
<a href="streaming.html">Hadoop Streaming</a>
</div>
<div class="menuitem">
<a href="commands_manual.html">Hadoop Commands</a>
</div>
<div class="menuitem">
<a href="distcp.html">DistCp</a>
</div>
<div class="menuitem">
<a href="distcp2.html">DistCp Version 2</a>
</div>
<div class="menuitem">
<a href="vaidya.html">Vaidya</a>
</div>
<div class="menuitem">
<a href="hadoop_archives.html">Hadoop Archives</a>
</div>
<div class="menuitem">
<a href="gridmix.html">Gridmix</a>
</div>
<div class="menuitem">
<a href="rumen.html">Rumen</a>
</div>
<div class="menuitem">
<a href="capacity_scheduler.html">Capacity Scheduler</a>
</div>
<div class="menuitem">
<a href="fair_scheduler.html">Fair Scheduler</a>
</div>
<div class="menuitem">
<a href="hod_scheduler.html">Hod Scheduler</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.4', 'skin/')" id="menu_1.4Title" class="menutitle">HDFS</div>
<div id="menu_1.4" class="menuitemgroup">
<div class="menuitem">
<a href="hdfs_user_guide.html">HDFS Users </a>
</div>
<div class="menuitem">
<a href="hdfs_design.html">HDFS Architecture</a>
</div>
<div class="menuitem">
<a href="hdfs_permissions_guide.html">Permissions</a>
</div>
<div class="menuitem">
<a href="hdfs_quota_admin_guide.html">Quotas</a>
</div>
<div class="menuitem">
<a href="SLG_user_guide.html">Synthetic Load Generator</a>
</div>
<div class="menuitem">
<a href="hdfs_imageviewer.html">Offline Image Viewer</a>
</div>
<div class="menuitem">
<a href="hftp.html">HFTP</a>
</div>
<div class="menuitem">
<a href="webhdfs.html">WebHDFS REST API</a>
</div>
<div class="menuitem">
<a href="libhdfs.html">C API libhdfs</a>
</div>
</div>
<div onclick="SwitchMenu('menu_selected_1.5', 'skin/')" id="menu_selected_1.5Title" class="menutitle" style="background-image: url('skin/images/chapter_open.gif');">Common</div>
<div id="menu_selected_1.5" class="selectedmenuitemgroup" style="display: block;">
<div class="menuitem">
<a href="deployment_layout.html">Deployment Layout</a>
</div>
<div class="menupage">
<div class="menupagetitle">File System Shell</div>
</div>
<div class="menuitem">
<a href="service_level_auth.html">Service Level Authorization</a>
</div>
<div class="menuitem">
<a href="native_libraries.html">Native Libraries</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.6', 'skin/')" id="menu_1.6Title" class="menutitle">Miscellaneous</div>
<div id="menu_1.6" class="menuitemgroup">
<div class="menuitem">
<a href="Secure_Impersonation.html">Secure Impersonation</a>
</div>
<div class="menuitem">
<a href="api/index.html">API Docs</a>
</div>
<div class="menuitem">
<a href="jdiff/changes.html">API Changes</a>
</div>
<div class="menuitem">
<a href="https://wiki.apache.org/hadoop/">Wiki</a>
</div>
<div class="menuitem">
<a href="https://wiki.apache.org/hadoop/FAQ">FAQ</a>
</div>
<div class="menuitem">
<a href="releasenotes.html">Release Notes</a>
</div>
<div class="menuitem">
<a href="changes.html">Change Log</a>
</div>
</div>
<div id="credit"></div>
<div id="roundbottom">
<img style="display: none" class="corner" height="15" width="15" alt="" src="skin/images/rc-b-l-15-1body-2menu-3menu.png"></div>
<!--+
  |alternative credits
  +-->
<div id="credit2"></div>
</div>
<!--+
    |end Menu
    +-->
<!--+
    |start content
    +-->
<div id="content">
<div title="Portable Document Format" class="pdflink">
<a class="dida" href="file_system_shell.pdf"><img alt="PDF -icon" src="skin/images/pdfdoc.gif" class="skin"><br>
        PDF</a>
</div>
<h1>File System Shell Guide</h1>
<div id="front-matter">
<div id="minitoc-area">
<ul class="minitoc">
<li>
<a href="#Overview">Overview</a>
<ul class="minitoc">
<li>
<a href="#cat"> cat </a>
</li>
<li>
<a href="#chgrp"> chgrp </a>
</li>
<li>
<a href="#chmod"> chmod </a>
</li>
<li>
<a href="#chown"> chown </a>
</li>
<li>
<a href="#copyFromLocal">copyFromLocal</a>
</li>
<li>
<a href="#copyToLocal"> copyToLocal</a>
</li>
<li>
<a href="#count"> count </a>
</li>
<li>
<a href="#cp"> cp </a>
</li>
<li>
<a href="#du">du</a>
</li>
<li>
<a href="#dus"> dus </a>
</li>
<li>
<a href="#expunge"> expunge </a>
</li>
<li>
<a href="#get"> get </a>
</li>
<li>
<a href="#getmerge"> getmerge </a>
</li>
<li>
<a href="#ls">ls</a>
</li>
<li>
<a href="#lsr">lsr</a>
</li>
<li>
<a href="#mkdir"> mkdir </a>
</li>
<li>
<a href="#moveFromLocal"> moveFromLocal </a>
</li>
<li>
<a href="#moveToLocal"> moveToLocal</a>
</li>
<li>
<a href="#mv"> mv </a>
</li>
<li>
<a href="#put"> put </a>
</li>
<li>
<a href="#rm"> rm </a>
</li>
<li>
<a href="#rmr"> rmr </a>
</li>
<li>
<a href="#setrep"> setrep </a>
</li>
<li>
<a href="#stat"> stat </a>
</li>
<li>
<a href="#tail"> tail </a>
</li>
<li>
<a href="#test"> test </a>
</li>
<li>
<a href="#text"> text </a>
</li>
<li>
<a href="#touchz"> touchz </a>
</li>
</ul>
</li>
</ul>
</div>
</div>
		
<a name="Overview"></a>
<h2 class="h3">Overview</h2>
<div class="section">
<p>
      The File System (FS) shell includes various shell-like commands that directly
      interact with the Hadoop Distributed File System (HDFS) as well as other file systems that Hadoop supports,  
      such as Local FS, HFTP FS, S3 FS, and others. The FS shell is invoked by: </p>
<pre class="code">bin/hdfs dfs &lt;args&gt;</pre>
<p>
      All FS shell commands take path URIs as arguments. The URI
      format is <em>scheme://autority/path</em>. For HDFS the scheme
      is <em>hdfs</em>, and for the Local FS the scheme
      is <em>file</em>. The scheme and authority are optional. If not
      specified, the default scheme specified in the configuration is
      used. An HDFS file or directory such as <em>/parent/child</em>
      can be specified as <em>hdfs://namenodehost/parent/child</em> or
      simply as <em>/parent/child</em> (given that your configuration
      is set to point to <em>hdfs://namenodehost</em>). 
      </p>
<p>
      Most of the commands in FS shell behave like corresponding Unix
      commands. Differences are described with each of the
      commands. Error information is sent to <em>stderr</em> and the
      output is sent to <em>stdout</em>.
  </p>
<a name="cat"></a>
<h3 class="h4"> cat </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -cat URI [URI &hellip;]</span>
			
</p>
<p>
		   Copies source paths to <em>stdout</em>. 
		   </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 
		   </span>
				
</li>
				
<li>
					
<span class="codefrag">hdfs dfs -cat file:///file3 /user/hadoop/file4 </span>
				
</li>
			
</ul>
<p>Exit Code:<br>
		   
<span class="codefrag"> Returns 0 on success and -1 on error. </span>
</p>
<a name="chgrp"></a>
<h3 class="h4"> chgrp </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -chgrp [-R] GROUP URI [URI &hellip;]</span>
			
</p>
<p>
	    Change group association of files. With <span class="codefrag">-R</span>, make the change recursively through the directory structure. 
	    The user must be the owner of files, or else a super-user. 
	    Additional information is in the <a href="hdfs_permissions_guide.html">Permissions Guide</a>.
	    </p>
<a name="chmod"></a>
<h3 class="h4"> chmod </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI &hellip;]</span>
			
</p>
<p>
	    Change the permissions of files. With <span class="codefrag">-R</span>, make the change recursively through the directory structure. 
	    The user must be the owner of the file, or else a super-user. 
	    Additional information is in the <a href="hdfs_permissions_guide.html">Permissions Guide</a>.
	    </p>
<a name="chown"></a>
<h3 class="h4"> chown </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span>
			
</p>
<p>
	    Change the owner of files. With <span class="codefrag">-R</span>, make the change recursively through the directory structure. 
	    The user must be a super-user. 
	    Additional information is in the <a href="hdfs_permissions_guide.html">Permissions Guide</a>.
	    </p>
<a name="copyFromLocal"></a>
<h3 class="h4">copyFromLocal</h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -copyFromLocal &lt;localsrc&gt; URI</span>
			
</p>
<p>Similar to <a href="#put"><strong>put</strong></a> command, except that the source is restricted to a local file reference. </p>
<a name="copyToLocal"></a>
<h3 class="h4"> copyToLocal</h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</span>
			
</p>
<p> Similar to <a href="#get"><strong>get</strong></a> command, except that the destination is restricted to a local file reference.</p>
<a name="count"></a>
<h3 class="h4"> count </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -count [-q]  &lt;paths&gt;</span>
			
</p>
<p>
				Count the number of directories, files and bytes under the paths that match the specified file pattern. <br>
<br>
				The output columns with <span class="codefrag">-count </span> are:<br>
<br>
				
<span class="codefrag">DIR_COUNT, FILE_COUNT, CONTENT_SIZE FILE_NAME</span> 
<br>
<br>
				The output columns with <span class="codefrag">-count -q</span> are:<br>
<br>
				
<span class="codefrag">QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, 
				DIR_COUNT, FILE_COUNT, CONTENT_SIZE, FILE_NAME</span>
		   
</p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 
		   </span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -count -q hdfs://nn1.example.com/file1
		   </span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
			
</p>
<a name="cp"></a>
<h3 class="h4"> cp </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -cp URI [URI &hellip;] &lt;dest&gt;</span>
			
</p>
<p>
	    Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory.
	    <br>
	    Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -cp /user/hadoop/file1 /user/hadoop/file2</span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir </span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
			
</p>
<a name="du"></a>
<h3 class="h4">du</h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -du [-s] [-h] URI [URI &hellip;]</span>
			
</p>
<p>
	     Displays sizes of files and directories contained in the given directory or the length of a file in case its just a file.</p>
<p>Options:</p>
<ul>
             
<li>The <span class="codefrag">-s</span> option will result in an aggregate summary of file lengths being displayed, rather than the individual files.</li>
             
<li>The <span class="codefrag">-h</span> option will format file sizes in a "human-readable" fashion (e.g 64.0m instead of 67108864)</li>
             
</ul>
<p>
	     Example:<br>
<span class="codefrag">hdfs dfs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1</span>
<br>
	     Exit Code:<br>
<span class="codefrag"> Returns 0 on success and -1 on error. </span>
<br>
</p>
<a name="dus"></a>
<h3 class="h4"> dus </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -dus &lt;args&gt;</span>
			
</p>
<p>
	    Displays a summary of file lengths. This is an alternate form of <span class="codefrag">hdfs dfs -du -s</span>.
	   </p>
<a name="expunge"></a>
<h3 class="h4"> expunge </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -expunge</span>
			
</p>
<p>Empty the Trash. Refer to the <a href="hdfs_design.html">HDFS Architecture Guide</a>
			 for more information on the Trash feature.</p>
<a name="get"></a>
<h3 class="h4"> get </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -get [-ignorecrc] [-crc] &lt;src&gt; &lt;localdst&gt;</span>
				
<br>
			
</p>
<p>
	   Copy files to the local file system. Files that fail the CRC check may be copied with the  
	   <span class="codefrag">-ignorecrc</span> option. Files and CRCs may be copied using the 
	   <span class="codefrag">-crc</span> option.
	  </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -get /user/hadoop/file localfile </span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -get hdfs://nn.example.com/user/hadoop/file localfile</span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error. </span>
			
</p>
<a name="getmerge"></a>
<h3 class="h4"> getmerge </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -getmerge &lt;src&gt; &lt;localdst&gt; [addnl]</span>
			
</p>
<p>
	  Takes a source directory and a destination file as input and concatenates files in src into the destination local file. 
	  Optionally <span class="codefrag">addnl</span> can be set to enable adding a newline character at the end of each file.  
	  </p>
<a name="ls"></a>
<h3 class="h4">ls</h3>
<p>
               
<span class="codefrag">Usage: hdfs dfs -ls &lt;args&gt;</span>
           
</p>
<p>For a file returns stat on the file with the following format:</p>
<p>
               
<span class="codefrag">permissions number_of_replicas userid  groupid  filesize modification_date modification_time filename</span>
           
</p>
<p>For a directory it returns list of its direct children as in unix.A directory is listed as:</p>
<p>
               
<span class="codefrag">permissions userid groupid modification_date modification_time dirname</span>
           
</p>
<p>Example:</p>
<p>
               
<span class="codefrag">hdfs dfs -ls /user/hadoop/file1 </span>
           
</p>
<p>Exit Code:</p>
<p>
               
<span class="codefrag">Returns 0 on success and -1 on error.</span>
           
</p>
<a name="lsr"></a>
<h3 class="h4">lsr</h3>
<p>
<span class="codefrag">Usage: hdfs dfs -lsr &lt;args&gt;</span>
<br>
	      Recursive version of <span class="codefrag">ls</span>. Similar to Unix <span class="codefrag">ls -R</span>.
	      </p>
<a name="mkdir"></a>
<h3 class="h4"> mkdir </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -mkdir &lt;paths&gt;</span>
				
<br>
			
</p>
<p>
	   Takes path uri's as argument and creates directories. The behavior is much like unix mkdir -p creating parent directories along the path.
	  </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag">hdfs dfs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 </span>
				
</li>
				
<li>
					
<span class="codefrag">hdfs dfs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir
	  </span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag">Returns 0 on success and -1 on error.</span>
			
</p>
<a name="moveFromLocal"></a>
<h3 class="h4"> moveFromLocal </h3>
<p>
				
<span class="codefrag">Usage: dfs -moveFromLocal &lt;localsrc&gt; &lt;dst&gt;</span>
			
</p>
<p>Similar to <a href="#put"><strong>put</strong></a> command, except that the source <span class="codefrag">localsrc</span> is deleted after it's copied. </p>
<a name="moveToLocal"></a>
<h3 class="h4"> moveToLocal</h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -moveToLocal [-crc] &lt;src&gt; &lt;dst&gt;</span>
			
</p>
<p>Displays a "Not implemented yet" message.</p>
<a name="mv"></a>
<h3 class="h4"> mv </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -mv URI [URI &hellip;] &lt;dest&gt;</span>
			
</p>
<p>
	    Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. 
	    Moving files across file systems is not permitted.
	    <br>
	    Example:
	    </p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -mv /user/hadoop/file1 /user/hadoop/file2</span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1</span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
			
</p>
<a name="put"></a>
<h3 class="h4"> put </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -put &lt;localsrc&gt; ... &lt;dst&gt;</span>
			
</p>
<p>Copy single src, or multiple srcs from local file system to the destination file system. 
			Also reads input from stdin and writes to destination file system.<br>
	   
</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -put localfile /user/hadoop/hadoopfile</span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -put localfile1 localfile2 /user/hadoop/hadoopdir</span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -put localfile hdfs://nn.example.com/hadoop/hadoopfile</span>
				
</li>
				
<li>
<span class="codefrag">hdfs dfs -put - hdfs://nn.example.com/hadoop/hadoopfile</span>
<br>Reads the input from stdin.</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error. </span>
			
</p>
<a name="rm"></a>
<h3 class="h4"> rm </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -rm [-skipTrash] URI [URI &hellip;] </span>
			
</p>
<p>
	   Delete files specified as args. Only deletes files. If the <span class="codefrag">-skipTrash</span> option
	   is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately.  	This can be
		   useful when it is necessary to delete files from an over-quota directory.
	   Refer to rmr for recursive deletes.<br>
	   Example:
	   </p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -rm hdfs://nn.example.com/file </span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
			
</p>
<a name="rmr"></a>
<h3 class="h4"> rmr </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -rmr [-skipTrash] URI [URI &hellip;]</span>
			
</p>
<p>Recursive version of delete. The rmr command recursively deletes the directory and any content under it. If the <span class="codefrag">-skipTrash</span> option
		   is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be
		   useful when it is necessary to delete files from an over-quota directory.<br>
	   Example:
	   </p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -rmr /user/hadoop/dir </span>
				
</li>
				
<li>
					
<span class="codefrag"> hdfs dfs -rmr hdfs://nn.example.com/user/hadoop/dir </span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag"> Returns 0 on success and -1 on error. </span>
			
</p>
<a name="setrep"></a>
<h3 class="h4"> setrep </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -setrep [-R] &lt;path&gt;</span>
			
</p>
<p>
	   Changes the replication factor of a file. -R option is for recursively increasing the replication factor of files within a directory.
	  </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -setrep -w 3 -R /user/hadoop/dir1 </span>
				
</li>
			
</ul>
<p>Exit Code:</p>
<p>
				
<span class="codefrag">Returns 0 on success and -1 on error. </span>
			
</p>
<a name="stat"></a>
<h3 class="h4"> stat </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -stat URI [URI &hellip;]</span>
			
</p>
<p>
	   Returns the stat information on the path.
	   </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -stat path </span>
				
</li>
			
</ul>
<p>Exit Code:<br>
	   
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
</p>
<a name="tail"></a>
<h3 class="h4"> tail </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -tail [-f] URI </span>
			
</p>
<p>
	   Displays last kilobyte of the file to stdout. -f option can be used as in Unix.
	   </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -tail pathname </span>
				
</li>
			
</ul>
<p>Exit Code: <br>
	   
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
</p>
<a name="test"></a>
<h3 class="h4"> test </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -test -[ezd] URI</span>
			
</p>
<p>
	   Options: <br>
	   -e check to see if the file exists. Return 0 if true. <br>
	   -z check to see if the file is zero length. Return 0 if true. <br>
	   -d check to see if the path is directory. Return 0 if true. <br>
</p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hdfs dfs -test -e filename </span>
				
</li>
			
</ul>
<a name="text"></a>
<h3 class="h4"> text </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -text &lt;src&gt;</span>
				
<br>
			
</p>
<p>
	   Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.
	  </p>
<a name="touchz"></a>
<h3 class="h4"> touchz </h3>
<p>
				
<span class="codefrag">Usage: hdfs dfs -touchz URI [URI &hellip;]</span>
				
<br>
			
</p>
<p>
	   Create a file of zero length.
	   </p>
<p>Example:</p>
<ul>
				
<li>
					
<span class="codefrag"> hadoop -touchz pathname </span>
				
</li>
			
</ul>
<p>Exit Code:<br>
	   
<span class="codefrag"> Returns 0 on success and -1 on error.</span>
</p>
</div>
	
</div>
<!--+
    |end content
    +-->
<div class="clearboth">&nbsp;</div>
</div>
<div id="footer">
<!--+
    |start bottomstrip
    +-->
<div class="lastmodified">
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<div class="copyright">
        Copyright &copy;
         2008 <a href="http://www.apache.org/licenses/">The Apache Software Foundation.</a>
</div>
<!--+
    |end bottomstrip
    +-->
</div>
</body>
</html>
