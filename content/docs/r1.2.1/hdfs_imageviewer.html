<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta content="Apache Forrest" name="Generator">
<meta name="Forrest-version" content="0.9">
<meta name="Forrest-skin-name" content="pelt">
<title>Offline Image Viewer Guide</title>
<link type="text/css" href="skin/basic.css" rel="stylesheet">
<link media="screen" type="text/css" href="skin/screen.css" rel="stylesheet">
<link media="print" type="text/css" href="skin/print.css" rel="stylesheet">
<link type="text/css" href="skin/profile.css" rel="stylesheet">
<script src="skin/getBlank.js" language="javascript" type="text/javascript"></script><script src="skin/getMenu.js" language="javascript" type="text/javascript"></script><script src="skin/fontsize.js" language="javascript" type="text/javascript"></script>
<link rel="shortcut icon" href="images/favicon.ico">
</head>
<body onload="init()">
<script type="text/javascript">ndeSetTextSize();</script>
<div id="top">
<!--+
    |breadtrail
    +-->
<div class="breadtrail">
<a href="https://www.apache.org/">Apache</a> &gt; <a href="https://hadoop.apache.org/">Hadoop</a> &gt; <a href="https://hadoop.apache.org/core/">Core</a><script src="skin/breadcrumbs.js" language="JavaScript" type="text/javascript"></script>
</div>
<!--+
    |header
    +-->
<div class="header">
<!--+
    |start group logo
    +-->
<div class="grouplogo">
<a href="https://hadoop.apache.org/"><img class="logoImage" alt="Hadoop" src="images/hadoop-logo.jpg" title="Apache Hadoop"></a>
</div>
<!--+
    |end group logo
    +-->
<!--+
    |start Project Logo
    +-->
<div class="projectlogo">
<a href="https://hadoop.apache.org/core/"><img class="logoImage" alt="Hadoop" src="images/hadoop-logo-2.gif" title="Scalable Computing Platform"></a>
</div>
<!--+
    |end Project Logo
    +-->
<!--+
    |start Search
    +-->
<div class="searchbox">
<form action="http://www.google.com/search" method="get" class="roundtopsmall">
<input value="hadoop.apache.org" name="sitesearch" type="hidden"><input onFocus="getBlank (this, 'Search the site with google');" size="25" name="q" id="query" type="text" value="Search the site with google">&nbsp; 
                    <input name="Search" value="Search" type="submit">
</form>
</div>
<!--+
    |end search
    +-->
<!--+
    |start Tabs
    +-->
<ul id="tabs">
<li>
<a class="unselected" href="https://hadoop.apache.org/core/">Project</a>
</li>
<li>
<a class="unselected" href="https://wiki.apache.org/hadoop">Wiki</a>
</li>
<li class="current">
<a class="selected" href="index.html">Hadoop 1.2.1 Documentation</a>
</li>
</ul>
<!--+
    |end Tabs
    +-->
</div>
</div>
<div id="main">
<div id="publishedStrip">
<!--+
    |start Subtabs
    +-->
<div id="level2tabs"></div>
<!--+
    |end Endtabs
    +-->
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<!--+
    |breadtrail
    +-->
<div class="breadtrail">

             &nbsp;
           </div>
<!--+
    |start Menu, mainarea
    +-->
<!--+
    |start Menu
    +-->
<div id="menu">
<div onclick="SwitchMenu('menu_1.1', 'skin/')" id="menu_1.1Title" class="menutitle">Getting Started</div>
<div id="menu_1.1" class="menuitemgroup">
<div class="menuitem">
<a href="index.html">Overview</a>
</div>
<div class="menuitem">
<a href="single_node_setup.html">Single Node Setup</a>
</div>
<div class="menuitem">
<a href="cluster_setup.html">Cluster Setup</a>
</div>
<div class="menuitem">
<a href="cli_minicluster.html">CLI MiniCluster</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.2', 'skin/')" id="menu_1.2Title" class="menutitle">Guides</div>
<div id="menu_1.2" class="menuitemgroup">
<div class="menuitem">
<a href="HttpAuthentication.html">Authentication for Hadoop HTTP web-consoles</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.3', 'skin/')" id="menu_1.3Title" class="menutitle">MapReduce</div>
<div id="menu_1.3" class="menuitemgroup">
<div class="menuitem">
<a href="mapred_tutorial.html">MapReduce Tutorial</a>
</div>
<div class="menuitem">
<a href="streaming.html">Hadoop Streaming</a>
</div>
<div class="menuitem">
<a href="commands_manual.html">Hadoop Commands</a>
</div>
<div class="menuitem">
<a href="distcp.html">DistCp</a>
</div>
<div class="menuitem">
<a href="distcp2.html">DistCp Version 2</a>
</div>
<div class="menuitem">
<a href="vaidya.html">Vaidya</a>
</div>
<div class="menuitem">
<a href="hadoop_archives.html">Hadoop Archives</a>
</div>
<div class="menuitem">
<a href="gridmix.html">Gridmix</a>
</div>
<div class="menuitem">
<a href="rumen.html">Rumen</a>
</div>
<div class="menuitem">
<a href="capacity_scheduler.html">Capacity Scheduler</a>
</div>
<div class="menuitem">
<a href="fair_scheduler.html">Fair Scheduler</a>
</div>
<div class="menuitem">
<a href="hod_scheduler.html">Hod Scheduler</a>
</div>
</div>
<div onclick="SwitchMenu('menu_selected_1.4', 'skin/')" id="menu_selected_1.4Title" class="menutitle" style="background-image: url('skin/images/chapter_open.gif');">HDFS</div>
<div id="menu_selected_1.4" class="selectedmenuitemgroup" style="display: block;">
<div class="menuitem">
<a href="hdfs_user_guide.html">HDFS Users </a>
</div>
<div class="menuitem">
<a href="hdfs_design.html">HDFS Architecture</a>
</div>
<div class="menuitem">
<a href="hdfs_permissions_guide.html">Permissions</a>
</div>
<div class="menuitem">
<a href="hdfs_quota_admin_guide.html">Quotas</a>
</div>
<div class="menuitem">
<a href="SLG_user_guide.html">Synthetic Load Generator</a>
</div>
<div class="menupage">
<div class="menupagetitle">Offline Image Viewer</div>
</div>
<div class="menuitem">
<a href="hftp.html">HFTP</a>
</div>
<div class="menuitem">
<a href="webhdfs.html">WebHDFS REST API</a>
</div>
<div class="menuitem">
<a href="libhdfs.html">C API libhdfs</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.5', 'skin/')" id="menu_1.5Title" class="menutitle">Common</div>
<div id="menu_1.5" class="menuitemgroup">
<div class="menuitem">
<a href="deployment_layout.html">Deployment Layout</a>
</div>
<div class="menuitem">
<a href="file_system_shell.html">File System Shell</a>
</div>
<div class="menuitem">
<a href="service_level_auth.html">Service Level Authorization</a>
</div>
<div class="menuitem">
<a href="native_libraries.html">Native Libraries</a>
</div>
</div>
<div onclick="SwitchMenu('menu_1.6', 'skin/')" id="menu_1.6Title" class="menutitle">Miscellaneous</div>
<div id="menu_1.6" class="menuitemgroup">
<div class="menuitem">
<a href="Secure_Impersonation.html">Secure Impersonation</a>
</div>
<div class="menuitem">
<a href="api/index.html">API Docs</a>
</div>
<div class="menuitem">
<a href="jdiff/changes.html">API Changes</a>
</div>
<div class="menuitem">
<a href="https://wiki.apache.org/hadoop/">Wiki</a>
</div>
<div class="menuitem">
<a href="https://wiki.apache.org/hadoop/FAQ">FAQ</a>
</div>
<div class="menuitem">
<a href="releasenotes.html">Release Notes</a>
</div>
<div class="menuitem">
<a href="changes.html">Change Log</a>
</div>
</div>
<div id="credit"></div>
<div id="roundbottom">
<img style="display: none" class="corner" height="15" width="15" alt="" src="skin/images/rc-b-l-15-1body-2menu-3menu.png"></div>
<!--+
  |alternative credits
  +-->
<div id="credit2"></div>
</div>
<!--+
    |end Menu
    +-->
<!--+
    |start content
    +-->
<div id="content">
<div title="Portable Document Format" class="pdflink">
<a class="dida" href="hdfs_imageviewer.pdf"><img alt="PDF -icon" src="skin/images/pdfdoc.gif" class="skin"><br>
        PDF</a>
</div>
<h1>Offline Image Viewer Guide</h1>
<div id="front-matter">
<div id="minitoc-area">
<ul class="minitoc">
<li>
<a href="#Overview">Overview</a>
</li>
<li>
<a href="#Usage">Usage</a>
<ul class="minitoc">
<li>
<a href="#Basic">Basic</a>
</li>
<li>
<a href="#Example">Example</a>
</li>
</ul>
</li>
<li>
<a href="#options">Options</a>
<ul class="minitoc">
<li>
<a href="#Option+Index">Option Index</a>
</li>
</ul>
</li>
<li>
<a href="#analysis">Analyzing Results</a>
<ul class="minitoc">
<li>
<a href="#Total+Number+of+Files+for+Each+User">Total Number of Files for Each User</a>
</li>
<li>
<a href="#Files+That+Have+Never+Been+Accessed">Files That Have Never Been Accessed</a>
</li>
<li>
<a href="#Probable+Duplicated+Files+Based+on+File+Size">Probable Duplicated Files Based on File Size</a>
</li>
</ul>
</li>
</ul>
</div>
</div>

    
<a name="Overview"></a>
<h2 class="h3">Overview</h2>
<div class="section">
<p>The Offline Image Viewer is a tool to dump the contents of hdfs
      fsimage files to human-readable formats in order to allow offline analysis
      and examination of an Hadoop cluster's namespace. The tool is able to
      process very large image files relatively quickly, converting them to
      one of several output formats. The tool handles the layout formats that
      were included with Hadoop versions 16 and up. If the tool is not able to
      process an image file, it will exit cleanly. The Offline Image Viewer does not require
      an Hadoop cluster to be running; it is entirely offline in its operation.</p>
<p>The Offline Image Viewer provides several output processors:</p>
<ol>
        
<li>
<strong>Ls</strong> is the default output processor. It closely mimics the format of
          the <span class="codefrag">lsr </span> command. It includes the same fields, in the same order, as
          <span class="codefrag">lsr </span>: directory or file flag, permissions, replication, owner, group,
          file size, modification date, and full path. Unlike the <span class="codefrag">lsr </span> command,
          the root path is included. One important difference between the output
          of the <span class="codefrag">lsr </span> command this processor, is that this output is not sorted
          by directory name and contents. Rather, the files are listed in the
          order in which they are stored in the fsimage file. Therefore, it is
          not possible to directly compare the output of the <span class="codefrag">lsr </span> command this
          this tool. The Ls processor uses information contained within the Inode blocks to
          calculate file sizes and ignores the <span class="codefrag">-skipBlocks</span> option.</li>
        
<li>
<strong>Indented</strong> provides a more complete view of the fsimage's contents,
          including all of the information included in the image, such as image
          version, generation stamp and inode- and block-specific listings. This
          processor uses indentation to organize the output into a hierarchal manner.
          The <span class="codefrag">lsr </span> format is suitable for easy human comprehension.</li>
        
<li>
<strong>Delimited</strong> provides one file per line consisting of the path,
        replication, modification time, access time, block size, number of blocks, file size,
        namespace quota, diskspace quota, permissions, username and group name. If run against
        an fsimage that does not contain any of these fields, the field's column will be included,
        but no data recorded. The default record delimiter is a tab, but this may be changed
        via the <span class="codefrag">-delimiter</span> command line argument. This processor is designed to
        create output that is easily analyzed by other tools, such as <a href="https://hadoop.apache.org/pig/">Apache Pig</a>. 
        See the <a href="#analysis">Analyzing Results</a> section
        for further information on using this processor to analyze the contents of fsimage files.</li>
        
<li>
<strong>XML</strong> creates an XML document of the fsimage and includes all of the
          information within the fsimage, similar to the <span class="codefrag">lsr </span> processor. The output
          of this processor is amenable to automated processing and analysis with XML tools.
          Due to the verbosity of the XML syntax, this processor will also generate
          the largest amount of output.</li>
        
<li>
<strong>FileDistribution</strong> is the tool for analyzing file 
          sizes in the namespace image. In order to run the tool one should 
          define a range of integers <span class="codefrag">[0, maxSize]</span> by specifying
          <span class="codefrag">maxSize</span> and a <span class="codefrag">step</span>.
          The range of integers is divided into segments of size
          <span class="codefrag">step</span>:
          <span class="codefrag">[0, s</span><sub>1</sub><span class="codefrag">, ..., s</span><sub>n-1</sub><span class="codefrag">, maxSize]</span>, 
          and the processor calculates how many files in the system fall into 
          each segment <span class="codefrag">[s</span><sub>i-1</sub><span class="codefrag">, s</span><sub>i</sub><span class="codefrag">)</span>.
          Note that files larger than <span class="codefrag">maxSize</span> always fall into 
          the very last segment.
          The output file is formatted as a tab separated two column table:
          Size and NumFiles. Where Size represents the start of the segment,
          and numFiles is the number of files form the image which size falls
          in this segment.</li>
        
</ol>
</div> <!-- overview -->

    
<a name="Usage"></a>
<h2 class="h3">Usage</h2>
<div class="section">
<a name="Basic"></a>
<h3 class="h4">Basic</h3>
<p>The simplest usage of the Offline Image Viewer is to provide just an input and output
          file, via the <span class="codefrag">-i</span> and <span class="codefrag">-o</span> command-line switches:</p>
<p>
<span class="codefrag">bash$ bin/hadoop oiv -i fsimage -o fsimage.txt</span>
<br>
</p>
<p>This will create a file named fsimage.txt in the current directory using
        the Ls output processor.  For very large image files, this process may take
        several minutes.</p>
<p>One can specify which output processor via the command-line switch <span class="codefrag">-p</span>.
        For instance:</p>
<p>
<span class="codefrag">bash$ bin/hadoop oiv -i fsimage -o fsimage.xml -p XML</span>
<br>
</p>
<p>or</p>
<p>
<span class="codefrag">bash$ bin/hadoop oiv -i fsimage -o fsimage.txt -p Indented</span>
<br>
</p>
<p>This will run the tool using either the XML or Indented output processor,
        respectively.</p>
<p>One command-line option worth considering is <span class="codefrag">-skipBlocks</span>, which
        prevents the tool from explicitly enumerating all of the blocks that make up
        a file in the namespace. This is useful for file systems that have very large
        files. Enabling this option can significantly decrease the size of the resulting
        output, as individual blocks are not included. Note, however, that the Ls processor
        needs to enumerate the blocks and so overrides this option.</p>
<a name="Example"></a>
<h3 class="h4">Example</h3>
<p>Consider the following contrived namespace:</p>
<pre class="code">
drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:17 /anotherDir 

-rw-r--r--   3 theuser supergroup  286631664 2009-03-16 21:15 /anotherDir/biggerfile 

-rw-r--r--   3 theuser supergroup       8754 2009-03-16 21:17 /anotherDir/smallFile 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:11 /mapredsystem 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:11 /mapredsystem/theuser 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:11 /mapredsystem/theuser/mapredsystem 

drwx-wx-wx   - theuser supergroup          0 2009-03-16 21:11 /mapredsystem/theuser/mapredsystem/ip.redacted.com 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:12 /one 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:12 /one/two 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:16 /user 

drwxr-xr-x   - theuser supergroup          0 2009-03-16 21:19 /user/theuser 
</pre>
<p>Applying the Offline Image Processor against this file with default options would result in the following output:</p>
<pre class="code">
machine:hadoop-0.21.0-dev theuser$ bin/hadoop oiv -i fsimagedemo -o fsimage.txt 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:16 / 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:17 /anotherDir 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:11 /mapredsystem 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:12 /one 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:16 /user 

-rw-r--r--  3   theuser supergroup    286631664 2009-03-16 14:15 /anotherDir/biggerfile 

-rw-r--r--  3   theuser supergroup         8754 2009-03-16 14:17 /anotherDir/smallFile 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:11 /mapredsystem/theuser 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:11 /mapredsystem/theuser/mapredsystem 

drwx-wx-wx  -   theuser supergroup            0 2009-03-16 14:11 /mapredsystem/theuser/mapredsystem/ip.redacted.com 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:12 /one/two 

drwxr-xr-x  -   theuser supergroup            0 2009-03-16 14:19 /user/theuser 
</pre>
<p>Similarly, applying the Indented processor would generate output that begins with:</p>
<pre class="code">
machine:hadoop-0.21.0-dev theuser$ bin/hadoop oiv -i fsimagedemo -p Indented -o fsimage.txt 

FSImage 

  ImageVersion = -19 

  NamespaceID = 2109123098 

  GenerationStamp = 1003 

  INodes [NumInodes = 12] 

    Inode 

      INodePath =  

      Replication = 0 

      ModificationTime = 2009-03-16 14:16 

      AccessTime = 1969-12-31 16:00 

      BlockSize = 0 

      Blocks [NumBlocks = -1] 

      NSQuota = 2147483647 

      DSQuota = -1 

      Permissions 

        Username = theuser 

        GroupName = supergroup 

        PermString = rwxr-xr-x 

   remaining output omitted
</pre>
</div>

    
<a name="options"></a>
<h2 class="h3">Options</h2>
<div class="section">
<a name="Option+Index"></a>
<h3 class="h4">Option Index</h3>
<table class="ForrestTable" cellspacing="1" cellpadding="4">
          
<tr>
<th colspan="1" rowspan="1"> Flag </th><th colspan="1" rowspan="1"> Description </th>
</tr>
          
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">[-i|--inputFile] &lt;input file&gt;</span></td>
              <td colspan="1" rowspan="1">Specify the input fsimage file to process. Required.</td>
</tr>
          
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">[-o|--outputFile] &lt;output file&gt;</span></td>
              <td colspan="1" rowspan="1">Specify the output filename, if the specified output processor
              generates one. If the specified file already exists, it is silently overwritten. Required.
              </td>
</tr>
          
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">[-p|--processor] &lt;processor&gt;</span></td>
                  <td colspan="1" rowspan="1">Specify the image processor to apply against the image file. Currently
                    valid options are Ls (default), XML and Indented..
                  </td>
</tr>
          
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">-skipBlocks</span></td>
              <td colspan="1" rowspan="1">Do not enumerate individual blocks within files. This may save processing time
              and outfile file space on namespaces with very large files. The <span class="codefrag">Ls</span> processor reads
              the blocks to correctly determine file sizes and ignores this option.</td>
</tr>
          
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">-printToScreen</span></td>
              <td colspan="1" rowspan="1">Pipe output of processor to console as well as specified file. On extremely 
              large namespaces, this may increase processing time by an order of magnitude.</td>
</tr>
           
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">-delimiter &lt;arg&gt;</span></td>
                  <td colspan="1" rowspan="1">When used in conjunction with the Delimited processor, replaces the default
	                    tab delimiter with the string specified by <span class="codefrag">arg</span>.</td>
</tr>
          
<tr>
<td colspan="1" rowspan="1"><span class="codefrag">[-h|--help]</span></td>
              <td colspan="1" rowspan="1">Display the tool usage and help information and exit.</td>
</tr>
            
</table>
</div>
   
    
<a name="analysis"></a>
<h2 class="h3">Analyzing Results</h2>
<div class="section">
<p>The Offline Image Viewer makes it easy to gather large amounts of data about the hdfs namespace.
         This information can then be used to explore file system usage patterns or find
        specific files that match arbitrary criteria, along with other types of namespace analysis. The Delimited 
         image processor in particular creates
        output that is amenable to further processing by tools such as <a href="https://hadoop.apache.org/pig/">Apache Pig</a>. Pig provides a particularly
        good choice for analyzing these data as it is able to deal with the output generated from a small fsimage
        but also scales up to consume data from extremely large file systems.</p>
<p>The Delimited image processor generates lines of text separated, by default, by tabs and includes
        all of the fields that are common between constructed files and files that were still under constructed
        when the fsimage was generated. Examples scripts are provided demonstrating how to use this output to 
        accomplish three tasks: determine the number of files each user has created on the file system,
        find files were created but have not accessed, and find probable duplicates of large files by comparing
        the size of each file.</p>
<p>Each of the following scripts assumes you have generated an output file using the Delimited processor named
        <span class="codefrag">foo</span> and will be storing the results of the Pig analysis in a file named <span class="codefrag">results</span>.</p>
<a name="Total+Number+of+Files+for+Each+User"></a>
<h3 class="h4">Total Number of Files for Each User</h3>
<p>This script processes each path within the namespace, groups them by the file owner and determines the total
      number of files each user owns.</p>
<p>
<strong>numFilesOfEachUser.pig:</strong>
</p>
<pre class="code">
-- This script determines the total number of files each user has in
-- the namespace. Its output is of the form:
--   username, totalNumFiles

-- Load all of the fields from the file
A = LOAD '$inputFile' USING PigStorage('\t') AS (path:chararray,
                                                 replication:int,
                                                 modTime:chararray,
                                                 accessTime:chararray,
                                                 blockSize:long,
                                                 numBlocks:int,
                                                 fileSize:long,
                                                 NamespaceQuota:int,
                                                 DiskspaceQuota:int,
                                                 perms:chararray,
                                                 username:chararray,
                                                 groupname:chararray);


-- Grab just the path and username
B = FOREACH A GENERATE path, username;

-- Generate the sum of the number of paths for each user
C = FOREACH (GROUP B BY username) GENERATE group, COUNT(B.path);

-- Save results
STORE C INTO '$outputFile';
        </pre>
<p>This script can be run against pig with the following command:</p>
<p>
<span class="codefrag">bin/pig -x local -param inputFile=../foo -param outputFile=../results ../numFilesOfEachUser.pig</span>
<br>
</p>
<p>The output file's content will be similar to that below:</p>
<p>
        
<span class="codefrag">bart  1</span>
<br>
        
<span class="codefrag">lisa  16</span>
<br>
        
<span class="codefrag">homer 28</span>
<br>
        
<span class="codefrag">marge 2456</span>
<br>
      
</p>
<a name="Files+That+Have+Never+Been+Accessed"></a>
<h3 class="h4">Files That Have Never Been Accessed</h3>
<p>This script finds files that were created but whose access times were never changed, meaning they were never opened or viewed.</p>
<p>
<strong>neverAccessed.pig:</strong>
</p>
<pre class="code">
-- This script generates a list of files that were created but never
-- accessed, based on their AccessTime

-- Load all of the fields from the file
A = LOAD '$inputFile' USING PigStorage('\t') AS (path:chararray,
                                                 replication:int,
                                                 modTime:chararray,
                                                 accessTime:chararray,
                                                 blockSize:long,
                                                 numBlocks:int,
                                                 fileSize:long,
                                                 NamespaceQuota:int,
                                                 DiskspaceQuota:int,
                                                 perms:chararray,
                                                 username:chararray,
                                                 groupname:chararray);

-- Grab just the path and last time the file was accessed
B = FOREACH A GENERATE path, accessTime;

-- Drop all the paths that don't have the default assigned last-access time
C = FILTER B BY accessTime == '1969-12-31 16:00';

-- Drop the accessTimes, since they're all the same
D = FOREACH C GENERATE path;

-- Save results
STORE D INTO '$outputFile';
      </pre>
<p>This script can be run against pig with the following command and its output file's content will be a list of files that were created but never viewed afterwards.</p>
<p>
<span class="codefrag">bin/pig -x local -param inputFile=../foo -param outputFile=../results ../neverAccessed.pig</span>
<br>
</p>
<a name="Probable+Duplicated+Files+Based+on+File+Size"></a>
<h3 class="h4">Probable Duplicated Files Based on File Size</h3>
<p>This script groups files together based on their size, drops any that are of less than 100mb and returns a list of the file size, number of files found and a tuple of the file paths.  This can be used to find likely duplicates within the filesystem namespace.</p>
<p>
<strong>probableDuplicates.pig:</strong>
</p>
<pre class="code">
-- This script finds probable duplicate files greater than 100 MB by
-- grouping together files based on their byte size. Files of this size
-- with exactly the same number of bytes can be considered probable
-- duplicates, but should be checked further, either by comparing the
-- contents directly or by another proxy, such as a hash of the contents.
-- The scripts output is of the type:
--    fileSize numProbableDuplicates {(probableDup1), (probableDup2)}

-- Load all of the fields from the file
A = LOAD '$inputFile' USING PigStorage('\t') AS (path:chararray,
                                                 replication:int,
                                                 modTime:chararray,
                                                 accessTime:chararray,
                                                 blockSize:long,
                                                 numBlocks:int,
                                                 fileSize:long,
                                                 NamespaceQuota:int,
                                                 DiskspaceQuota:int,
                                                 perms:chararray,
                                                 username:chararray,
                                                 groupname:chararray);

-- Grab the pathname and filesize
B = FOREACH A generate path, fileSize;

-- Drop files smaller than 100 MB
C = FILTER B by fileSize &gt; 100L  * 1024L * 1024L;

-- Gather all the files of the same byte size
D = GROUP C by fileSize;

-- Generate path, num of duplicates, list of duplicates
E = FOREACH D generate group AS fileSize, COUNT(C) as numDupes, C.path AS files;

-- Drop all the files where there are only one of them
F = FILTER E by numDupes &gt; 1L;

-- Sort by the size of the files
G = ORDER F by fileSize;

-- Save results
STORE G INTO '$outputFile';
      </pre>
<p>This script can be run against pig with the following command:</p>
<p>
<span class="codefrag">bin/pig -x local -param inputFile=../foo -param outputFile=../results ../probableDuplicates.pig</span>
<br>
</p>
<p> The output file's content will be similar to that below:</p>
<pre class="code">
1077288632 2 {(/user/tennant/work1/part-00501),(/user/tennant/work1/part-00993)} 
1077288664 4 {(/user/tennant/work0/part-00567),(/user/tennant/work0/part-03980),(/user/tennant/work1/part-00725),(/user/eccelston/output/part-03395)} 
1077288668 3 {(/user/tennant/work0/part-03705),(/user/tennant/work0/part-04242),(/user/tennant/work1/part-03839)} 
1077288698 2 {(/user/tennant/work0/part-00435),(/user/eccelston/output/part-01382)} 
1077288702 2 {(/user/tennant/work0/part-03864),(/user/eccelston/output/part-03234)} 
</pre>
<p>Each line includes the file size in bytes that was found to be duplicated, the number of duplicates found, and a list of the duplicated paths. 
      Files less than 100MB are ignored, providing a reasonable likelihood that files of these exact sizes may be duplicates.</p>
</div>


  
</div>
<!--+
    |end content
    +-->
<div class="clearboth">&nbsp;</div>
</div>
<div id="footer">
<!--+
    |start bottomstrip
    +-->
<div class="lastmodified">
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<div class="copyright">
        Copyright &copy;
         2008 <a href="https://www.apache.org/licenses/">The Apache Software Foundation.</a>
</div>
<!--+
    |end bottomstrip
    +-->
</div>
</body>
</html>
