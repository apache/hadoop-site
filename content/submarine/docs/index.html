
<!DOCTYPE html>

<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Machine learning engine for Hadoop">
    <meta name="keywords" content="hadoop, submarine, machine learning, tensorflow, deep learning"/>
    <meta name="robots" content="index,follow"/>
    <meta name="language" content="en"/>

    <title>Apache Hadoop Submarine</title>

    <base href="/submarine/">

    <link rel="canonical" href="https://hadoop.apache.org/submarine">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"
          integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">

</head>

<body>


<div class="topnav">
    <div class="container">
        <ul class="breadcrumb col-md-6">
            <li>
                <img class="asf-logo" src="asf_feather.png"/>
                <a  href="https://www.apache.org">Apache Software Foundation</span></a>
            </li>
            <li><a href="https://hadoop.apache.org">Apache Hadoop&trade;</a></li>
            <li><a href="/submarine/">Submarine&trade;</a></li>
        </ul>
        <div class="col-md-6">
            <ul class="pull-right breadcrumb">
                <li><a href="https://www.apache.org/licenses/">License</a></li>
                <li><a href="https://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
                <li><a href="https://www.apache.org/foundation/thanks.html">Thanks</a></li>
                <li><a href="https://www.apache.org/security/">Security</a></li>
        </ul>
        </div>
    </div>

    <nav class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                        data-target="#ratis-menu" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>

            <div id="ratis-menu" class="collapse navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    
                    <li><a href="docs/Index">Documentation</a></li>
                    <li><a href="downloads">Download</a></li>
                    <li><a href="ecosystem">Ecosystem</a></li>
                    <li><a href="#qa">Q&A</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/HADOOP/Submarine+Contributor+Guide">Wiki</a></li>
                    <li><a href="activity">Activity</a></li>
                    <li><a href="team">Team</a></li>
                </ul>
            </div>

    </nav>

    <div style="max-height: 200px; overflow: hidden;">
      <img src="cloud.png" style="width: 100%; left:0px;"/>
    </div>

</div>

</div>

<div class="container">

<section id="main">
  <div>
    <h1 id="title">Docs Archive</h1>
        <ul id="list">
            
            <h1><a href="/submarine/docs/0.2.0/BuildFromCode/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h2 id="build-submarine-from-source-code">Build Submarine From Source Code</h2>

<ul>
<li><p>Run &lsquo;mvn install -DskipTests&rsquo; from Hadoop source top level once.</p></li>

<li><p>Navigate to hadoop-submarine folder and run &lsquo;mvn clean package&rsquo;.</p>

<ul>
<li><p>By default, hadoop-submarine is built based on hadoop 3.1.2 dependencies.
Both yarn service runtime and tony runtime are built.
You can also add a parameter of &ldquo;-Phadoop-3.2&rdquo; to specify the dependencies
to hadoop 3.2.0.</p></li>

<li><p>Hadoop-submarine can support hadoop 2.9.2 and hadoop 2.7.4 as well.
You can add &ldquo;-Phadoop-2.9&rdquo; to build submarine based on hadoop 2.9.2.
For example:</p>

<pre><code>mvn clean package -Phadoop-2.9
</code></pre>

<p>As yarn service is based on hadoop 3.*, so only tony runtime is built
in this case.</p></li>
</ul></li>
</ul>


            
            <h1><a href="/submarine/docs/0.2.0/Examples/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   https://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<h1 id="examples">Examples</h1>

<p>Here&rsquo;re some examples about Submarine usage.</p>

<p><a href="docs/0.2.0/RunningDistributedCifar10TFJobs">Running Distributed CIFAR 10 Tensorflow Job</a></p>

<p><a href="docs/0.2.0/RunningSingleNodeCifar10PTJobs">Running Standalone CIFAR 10 PyTorch Job</a></p>


            
            <h1><a href="/submarine/docs/0.2.0/HowToInstall/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="how-to-install-dependencies">How to Install Dependencies</h1>

<p>Submarine project may uses YARN Service (When Submarine YARN service runtime is being used, see <a href="docs/0.2.0/QuickStart">QuickStart</a>), Docker container, and GPU (when GPU hardware available and properly configured).</p>

<p>That means as an admin, you may have to properly setup YARN Service related dependencies, including:
- YARN Registry DNS</p>

<p>Docker related dependencies, including:
- Docker binary with expected versions.
- Docker network which allows Docker container can talk to each other across different nodes.</p>

<p>And when GPU plan to be used:
- GPU Driver.
- Nvidia-docker.</p>

<p>For your convenience, we provided installation documents to help you to setup your environment. You can always choose to have them installed in your own way.</p>

<p>Use Submarine installer to install dependencies: <a href="https://github.com/hadoopsubmarine/hadoop-submarine-ecosystem/tree/master/submarine-installer">EN</a> <a href="https://github.com/hadoopsubmarine/submarine-installer/blob/master/README-CN.md">CN</a></p>

<p>Alternatively, you can follow manual install dependencies: <a href="docs/0.2.0/InstallationGuide">EN</a> <a href="docs/0.2.0//InstallationGuideChineseVersion">CN</a></p>

<p>Once you have installed dependencies, please follow following guide to <a href="docs/0.2.0/TestAndTroubleshooting">TestAndTroubleshooting</a>.</p>


            
            <h1><a href="/submarine/docs/0.2.0/Index/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            <!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<p>Submarine is a project which allows infra engineer / data scientist to run
<em>unmodified</em> Tensorflow or PyTorch programs on YARN or Kubernetes.</p>

<p>Goals of Submarine:</p>

<ul>
<li><p>It allows jobs for easy access to data/models in HDFS and other storages.</p></li>

<li><p>Can launch services to serve Tensorflow/MXNet models.</p></li>

<li><p>Support run distributed Tensorflow jobs with simple configs.</p></li>

<li><p>Support run standalone PyTorch jobs with simple configs.</p></li>

<li><p>Support run user-specified Docker images.</p></li>

<li><p>Support specify GPU and other resources.</p></li>

<li><p>Support launch tensorboard for training jobs if user specified.</p></li>

<li><p>Support customized DNS name for roles (like tensorboard.$user.$domain:6006)</p></li>
</ul>

<p>Click below contents if you want to understand more.</p>

<ul>
<li><p><a href="docs/0.2.0/QuickStart">QuickStart Guide</a></p></li>

<li><p><a href="docs/0.2.0/Examples">Examples</a></p></li>

<li><p><a href="docs/0.2.0/WriteDockerfileTF">How to write Dockerfile for Submarine TensorFlow jobs</a></p></li>

<li><p><a href="docs/0.2.0/WriteDockerfilePT">How to write Dockerfile for Submarine PyTorch jobs</a></p></li>

<li><p><a href="docs/0.2.0/HowToInstall">Installation guides</a></p></li>
</ul>


            
            <h1><a href="/submarine/docs/0.2.0/InstallationGuide/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="submarine-installation-guide">Submarine Installation Guide</h1>

<h2 id="prerequisites">Prerequisites</h2>

<p>(Please note that all following prerequisites are just an example for you to install. You can always choose to install your own version of kernel, different users, different drivers, etc.).</p>

<h3 id="operating-system">Operating System</h3>

<p>The operating system and kernel versions we have tested are as shown in the following table, which is the recommneded minimum required versions.</p>

<table>
<thead>
<tr>
<th>Enviroment</th>
<th>Verion</th>
</tr>
</thead>

<tbody>
<tr>
<td>Operating System</td>
<td>centos-release-7-5.1804.el7.centos.x86_64</td>
</tr>

<tr>
<td>Kernal</td>
<td>3.10.0-862.el7.x86_64</td>
</tr>
</tbody>
</table>

<h3 id="user-group">User &amp; Group</h3>

<p>As there are some specific users and groups recommended to be created to install hadoop/docker. Please create them if they are missing.</p>

<pre><code>adduser hdfs
adduser mapred
adduser yarn
addgroup hadoop
usermod -aG hdfs,hadoop hdfs
usermod -aG mapred,hadoop mapred
usermod -aG yarn,hadoop yarn
usermod -aG hdfs,hadoop hadoop
groupadd docker
usermod -aG docker yarn
usermod -aG docker hadoop
</code></pre>

<h3 id="gcc-version">GCC Version</h3>

<p>Check the version of GCC tool (to compile kernel).</p>

<pre><code class="language-bash">gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
# install if needed
yum install gcc make g++
</code></pre>

<h3 id="kernel-header-kernel-devel">Kernel header &amp; Kernel devel</h3>

<pre><code class="language-bash"># Approach 1：
yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)
# Approach 2：
wget http://vault.centos.org/7.3.1611/os/x86_64/Packages/kernel-headers-3.10.0-862.el7.x86_64.rpm
rpm -ivh kernel-headers-3.10.0-862.el7.x86_64.rpm
</code></pre>

<h3 id="gpu-servers-only-for-nvidia-gpu-equipped-nodes">GPU Servers (Only for Nvidia GPU equipped nodes)</h3>

<pre><code>lspci | grep -i nvidia

# If the server has gpus, you can get info like this：
04:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
82:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
</code></pre>

<h3 id="nvidia-driver-installation-only-for-nvidia-gpu-equipped-nodes">Nvidia Driver Installation (Only for Nvidia GPU equipped nodes)</h3>

<p>To make a clean installation, if you have requirements to upgrade GPU drivers. If nvidia driver/cuda has been installed before, They should be uninstalled firstly.</p>

<pre><code># uninstall cuda：
sudo /usr/local/cuda-10.0/bin/uninstall_cuda_10.0.pl

# uninstall nvidia-driver：
sudo /usr/bin/nvidia-uninstall
</code></pre>

<p>To check GPU version, install nvidia-detect</p>

<pre><code>yum install nvidia-detect
# run 'nvidia-detect -v' to get reqired nvidia driver version：
nvidia-detect -v
Probing for supported NVIDIA devices...
[10de:13bb] NVIDIA Corporation GM107GL [Quadro K620]
This device requires the current xyz.nm NVIDIA driver kmod-nvidia
[8086:1912] Intel Corporation HD Graphics 530
An Intel display controller was also detected
</code></pre>

<p>Pay attention to <code>This device requires the current xyz.nm NVIDIA driver kmod-nvidia</code>.
Download the installer like <a href="https://www.nvidia.com/object/linux-amd64-display-archive.html">NVIDIA-Linux-x86_64-390.87.run</a>.</p>

<p>Some preparatory work for nvidia driver installation. (This is follow normal Nvidia GPU driver installation, just put here for your convenience)</p>

<pre><code># It may take a while to update
yum -y update
yum -y install kernel-devel

yum -y install epel-release
yum -y install dkms

# Disable nouveau
vim /etc/default/grub
# Add the following configuration in “GRUB_CMDLINE_LINUX” part
rd.driver.blacklist=nouveau nouveau.modeset=0

# Generate configuration
grub2-mkconfig -o /boot/grub2/grub.cfg

vim /etc/modprobe.d/blacklist.conf
# Add confiuration:
blacklist nouveau

mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img
dracut /boot/initramfs-$(uname -r).img $(uname -r)
reboot
</code></pre>

<p>Check whether nouveau is disabled</p>

<pre><code>lsmod | grep nouveau  # return null

# install nvidia driver
sh NVIDIA-Linux-x86_64-390.87.run
</code></pre>

<p>Some options during the installation</p>

<pre><code>Install NVIDIA's 32-bit compatibility libraries (Yes)
centos Install NVIDIA's 32-bit compatibility libraries (Yes)
Would you like to run the nvidia-xconfig utility to automatically update your X configuration file... (NO)
</code></pre>

<p>Check nvidia driver installation</p>

<pre><code>nvidia-smi
</code></pre>

<p>Reference：
<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></p>

<h3 id="docker-installation">Docker Installation</h3>

<p>The following steps show how to install docker 18.06.1.ce. You can choose other approaches to install Docker.</p>

<pre><code># Remove old version docker
sudo yum remove docker \
                docker-client \
                docker-client-latest \
                docker-common \
                docker-latest \
                docker-latest-logrotate \
                docker-logrotate \
                docker-engine

# Docker version
export DOCKER_VERSION=&quot;18.06.1.ce&quot;
# Setup the repository
sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

# Check docker version
yum list docker-ce --showduplicates | sort -r

# Install docker with specified DOCKER_VERSION
sudo yum install -y docker-ce-${DOCKER_VERSION} docker-ce-cli-${DOCKER_VERSION} containerd.io

# Start docker
systemctl start docker

chown hadoop:netease /var/run/docker.sock
chown hadoop:netease /usr/bin/docker
</code></pre>

<p>Reference：<a href="https://docs.docker.com/install/linux/docker-ce/centos/">https://docs.docker.com/install/linux/docker-ce/centos/</a></p>

<h3 id="docker-configuration">Docker Configuration</h3>

<p>Add a file, named daemon.json, under the path of /etc/docker/. Please replace the variables of image_registry_ip, etcd_host_ip, localhost_ip, yarn_dns_registry_host_ip, dns_host_ip with specific ips according to your environments.</p>

<pre><code>{
    &quot;insecure-registries&quot;: [&quot;${image_registry_ip}:5000&quot;],
    &quot;cluster-store&quot;:&quot;etcd://${etcd_host_ip1}:2379,${etcd_host_ip2}:2379,${etcd_host_ip3}:2379&quot;,
    &quot;cluster-advertise&quot;:&quot;{localhost_ip}:2375&quot;,
    &quot;dns&quot;: [&quot;${yarn_dns_registry_host_ip}&quot;, &quot;${dns_host_ip1}&quot;],
    &quot;hosts&quot;: [&quot;tcp://{localhost_ip}:2375&quot;, &quot;unix:///var/run/docker.sock&quot;]
}
</code></pre>

<p>Restart docker daemon：</p>

<pre><code>sudo systemctl restart docker
</code></pre>

<h3 id="check-docker-version">Check docker version</h3>

<pre><code class="language-bash">$ docker version

Client:
 Version:      18.06.1-ce
 API version:  1.38
 Go version:   go1.10.3
 Git commit:   e68fc7a
 Built:        Tue Aug 21 17:23:03 2018
 OS/Arch:      linux/amd64
 Experimental: false

Server:
 Version:      18.06.1-ce
 API version:  1.38 (minimum version 1.12)
 Go version:   go1.10.3
 Git commit:   e68fc7a
 Built:        Tue Aug 21 17:23:03 2018
 OS/Arch:      linux/amd64
 Experimental: false
</code></pre>

<h3 id="nvidia-docker-installation-only-for-nvidia-gpu-equipped-nodes">Nvidia-docker Installation (Only for Nvidia GPU equipped nodes)</h3>

<p>Submarine has already supported nvidia-docker V2</p>

<pre><code># Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo
sudo yum install -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
</code></pre>

<p>According to <code>nvidia-driver</code> version, add folders under the path of  <code>/var/lib/nvidia-docker/volumes/nvidia_driver/</code></p>

<pre><code>mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87
# 390.8 is nvidia driver version

mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64

cp /usr/bin/nvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
cp /usr/lib64/libcuda* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
cp /usr/lib64/libnvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64

# Test with nvidia-smi
nvidia-docker run --rm nvidia/cuda:10.0-devel nvidia-smi
</code></pre>

<p>Test docker, nvidia-docker, nvidia-driver installation</p>

<pre><code># Test 1
nvidia-docker run -rm nvidia/cuda nvidia-smi
</code></pre>

<pre><code># Test 2
nvidia-docker run -it tensorflow/tensorflow:1.9.0-gpu bash
# In docker container
python
import tensorflow as tf
tf.test.is_gpu_available()
</code></pre>

<p>The way to uninstall nvidia-docker V2</p>

<pre><code>sudo yum remove -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
</code></pre>

<p>Reference:
<a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a></p>

<h3 id="tensorflow-image">Tensorflow Image</h3>

<p>There is no need to install CUDNN and CUDA on the servers, because CUDNN and CUDA can be added in the docker images. We can get basic docker images by referring to <a href="docs/0.2.0/WriteDockerfileTF">Write Dockerfile</a>.</p>

<h3 id="test-tensorflow-in-a-docker-container">Test tensorflow in a docker container</h3>

<p>After docker image is built, we can check
Tensorflow environments before submitting a yarn job.</p>

<pre><code class="language-shell">$ docker run -it ${docker_image_name} /bin/bash
# &gt;&gt;&gt; In the docker container
$ python
$ python &gt;&gt; import tensorflow as tf
$ python &gt;&gt; tf.__version__
</code></pre>

<p>If there are some errors, we could check the following configuration.</p>

<ol>
<li>LD_LIBRARY_PATH environment variable</li>
</ol>

<pre><code>   echo $LD_LIBRARY_PATH
   /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
</code></pre>

<ol>
<li>The location of libcuda.so.1, libcuda.so</li>
</ol>

<pre><code>   ls -l /usr/local/nvidia/lib64 | grep libcuda.so
</code></pre>

<h3 id="etcd-installation">Etcd Installation</h3>

<p>etcd is a distributed reliable key-value store for the most critical data of a distributed system, Registration and discovery of services used in containers.
You can also choose alternatives like zookeeper, Consul.</p>

<p>To install Etcd on specified servers, we can run Submarine-installer/install.sh</p>

<pre><code class="language-shell">$ ./Submarine-installer/install.sh
# Etcd status
systemctl status Etcd.service
</code></pre>

<p>Check Etcd cluster health</p>

<pre><code class="language-shell">$ etcdctl cluster-health
member 3adf2673436aa824 is healthy: got healthy result from http://${etcd_host_ip1}:2379
member 85ffe9aafb7745cc is healthy: got healthy result from http://${etcd_host_ip2}:2379
member b3d05464c356441a is healthy: got healthy result from http://${etcd_host_ip3}:2379
cluster is healthy

$ etcdctl member list
3adf2673436aa824: name=etcdnode3 peerURLs=http://${etcd_host_ip1}:2380 clientURLs=http://${etcd_host_ip1}:2379 isLeader=false
85ffe9aafb7745cc: name=etcdnode2 peerURLs=http://${etcd_host_ip2}:2380 clientURLs=http://${etcd_host_ip2}:2379 isLeader=false
b3d05464c356441a: name=etcdnode1 peerURLs=http://${etcd_host_ip3}:2380 clientURLs=http://${etcd_host_ip3}:2379 isLeader=true
</code></pre>

<h3 id="calico-installation">Calico Installation</h3>

<p>Calico creates and manages a flat three-tier network, and each container is assigned a routable ip. We just add the steps here for your convenience.
You can also choose alternatives like Flannel, OVS.</p>

<p>To install Calico on specified servers, we can run Submarine-installer/install.sh</p>

<pre><code>systemctl start calico-node.service
systemctl status calico-node.service
</code></pre>

<h4 id="check-calico-network">Check Calico Network</h4>

<pre><code class="language-shell"># Run the following command to show the all host status in the cluster except localhost.
$ calicoctl node status
Calico process is running.

IPv4 BGP status
+---------------+-------------------+-------+------------+-------------+
| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |
+---------------+-------------------+-------+------------+-------------+
| ${host_ip1} | node-to-node mesh | up    | 2018-09-21 | Established |
| ${host_ip2} | node-to-node mesh | up    | 2018-09-21 | Established |
| ${host_ip3} | node-to-node mesh | up    | 2018-09-21 | Established |
+---------------+-------------------+-------+------------+-------------+

IPv6 BGP status
No IPv6 peers found.
</code></pre>

<p>Create containers to validate calico network</p>

<pre><code>docker network create --driver calico --ipam-driver calico-ipam calico-network
docker run --net calico-network --name workload-A -tid busybox
docker run --net calico-network --name workload-B -tid busybox
docker exec workload-A ping workload-B
</code></pre>

<h2 id="hadoop-installation">Hadoop Installation</h2>

<h3 id="get-hadoop-release">Get Hadoop Release</h3>

<p>You can either get Hadoop release binary or compile from source code. Please follow the <a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a> guides.</p>

<h3 id="start-yarn-service">Start YARN Service</h3>

<pre><code>YARN_LOGFILE=resourcemanager.log ./sbin/yarn-daemon.sh start resourcemanager
YARN_LOGFILE=nodemanager.log ./sbin/yarn-daemon.sh start nodemanager
YARN_LOGFILE=timeline.log ./sbin/yarn-daemon.sh start timelineserver
YARN_LOGFILE=mr-historyserver.log ./sbin/mr-jobhistory-daemon.sh start historyserver
</code></pre>

<h3 id="start-yarn-registery-dns-service-only-when-using-yarn-native-service-runtime">Start YARN Registery DNS Service (only when using YARN native service runtime)</h3>

<pre><code>sudo YARN_LOGFILE=registrydns.log ./yarn-daemon.sh start registrydns
</code></pre>

<h3 id="test-with-a-mr-wordcount-job">Test with a MR wordcount job</h3>

<pre><code>./bin/hadoop jar /home/hadoop/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0-SNAPSHOT.jar wordcount /tmp/wordcount.txt /tmp/wordcount-output4
</code></pre>

<h3 id="gpu-configurations-for-both-resourcemanager-and-nodemanager">GPU configurations for both resourcemanager and nodemanager</h3>

<p>Add the yarn resource configuration file, named resource-types.xml</p>

<pre><code>   &lt;configuration&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.resource-types&lt;/name&gt;
       &lt;value&gt;yarn.io/gpu&lt;/value&gt;
     &lt;/property&gt;
   &lt;/configuration&gt;
</code></pre>

<h3 id="gpu-configurations-for-resourcemanager">GPU configurations for resourcemanager</h3>

<p>The scheduler used by resourcemanager must be  capacity scheduler, and yarn.scheduler.capacity.resource-calculator in  capacity-scheduler.xml should be DominantResourceCalculator</p>

<pre><code>   &lt;configuration&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;
       &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;
     &lt;/property&gt;
   &lt;/configuration&gt;
</code></pre>

<h3 id="gpu-configurations-for-nodemanager">GPU configurations for nodemanager</h3>

<p>Add configurations in yarn-site.xml</p>

<pre><code>   &lt;configuration&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.nodemanager.resource-plugins&lt;/name&gt;
       &lt;value&gt;yarn.io/gpu&lt;/value&gt;
     &lt;/property&gt;
     &lt;!--Use nvidia docker v2--&gt;
     &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource-plugins.gpu.docker-plugin&lt;/name&gt;
        &lt;value&gt;nvidia-docker-v2&lt;/value&gt;
     &lt;/property&gt;
   &lt;/configuration&gt;
</code></pre>

<p>Add configurations in container-executor.cfg</p>

<pre><code>   [docker]
   ...
   # Add configurations in `[docker]` part：
   # /usr/bin/nvidia-docker is the path of nvidia-docker command
   # nvidia_driver_375.26 means that nvidia driver version is &lt;version&gt;. nvidia-smi command can be used to check the version
   docker.allowed.volume-drivers=/usr/bin/nvidia-docker
   docker.allowed.devices=/dev/nvidiactl,/dev/nvidia-uvm,/dev/nvidia-uvm-tools,/dev/nvidia1,/dev/nvidia0
   docker.allowed.ro-mounts=nvidia_driver_&lt;version&gt;
   # Use nvidia docker v2
   docker.allowed.runtimes=nvidia

   [gpu]
   module.enabled=true

   [cgroups]
   # /sys/fs/cgroup is the cgroup mount destination
   # /hadoop-yarn is the path yarn creates by default
   root=/sys/fs/cgroup
   yarn-hierarchy=/hadoop-yarn
</code></pre>

<h3 id="run-a-distributed-tensorflow-gpu-job">Run a distributed tensorflow gpu job</h3>

<pre><code class="language-bash"> ... job run \
 --env DOCKER_JAVA_HOME=/opt/java \
 --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf-gpu \
 --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
 --docker_image tf-1.13.1-gpu:0.0.1 \
 --input_path hdfs://default/tmp/cifar-10-data \
 --checkpoint_path hdfs://default/user/hadoop/tf-distributed-checkpoint \
 --num_ps 0 \
 --ps_resources memory=4G,vcores=2,gpu=0 \
 --ps_launch_cmd &quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0&quot; \
 --worker_resources memory=4G,vcores=2,gpu=1 --verbose \
 --num_workers 1 \
 --worker_launch_cmd &quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=1&quot;
</code></pre>


            
            <h1><a href="/submarine/docs/0.2.0/InstallationGuideChineseVersion/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="submarine-安装说明">Submarine 安装说明</h1>

<h2 id="prerequisites">Prerequisites</h2>

<h3 id="操作系统">操作系统</h3>

<p>我们使用的操作系统版本是 centos-release-7-5.1804.el7.centos.x86_64, 内核版本是 3.10.0-862.el7.x86_64。</p>

<table>
<thead>
<tr>
<th>Enviroment</th>
<th>Verion</th>
</tr>
</thead>

<tbody>
<tr>
<td>Operating System</td>
<td>centos-release-7-5.1804.el7.centos.x86_64</td>
</tr>

<tr>
<td>Kernal</td>
<td>3.10.0-862.el7.x86_64</td>
</tr>
</tbody>
</table>

<h3 id="user-group">User &amp; Group</h3>

<p>如果操作系统中没有这些用户组和用户，必须添加。一部分用户是 hadoop 运行需要，一部分用户是 docker 运行需要。</p>

<pre><code>adduser hdfs
adduser mapred
adduser yarn
addgroup hadoop
usermod -aG hdfs,hadoop hdfs
usermod -aG mapred,hadoop mapred
usermod -aG yarn,hadoop yarn
usermod -aG hdfs,hadoop hadoop
groupadd docker
usermod -aG docker yarn
usermod -aG docker hadoop
</code></pre>

<h3 id="gcc-版本">GCC 版本</h3>

<pre><code class="language-bash">gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
# 如果没有安装请执行以下命令进行安装
yum install gcc make g++
</code></pre>

<h3 id="kernel-header-devel">Kernel header &amp; devel</h3>

<pre><code class="language-bash"># 方法一：
yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r)
# 方法二：
wget http://vault.centos.org/7.3.1611/os/x86_64/Packages/kernel-headers-3.10.0-862.el7.x86_64.rpm
rpm -ivh kernel-headers-3.10.0-862.el7.x86_64.rpm
</code></pre>

<h3 id="检查-gpu-版本">检查 GPU 版本</h3>

<pre><code>lspci | grep -i nvidia

# 如果什么都没输出，就说明显卡不对，以下是我的输出：
# 04:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
# 82:00.0 3D controller: NVIDIA Corporation Device 1b38 (rev a1)
</code></pre>

<h3 id="安装-nvidia-驱动">安装 nvidia 驱动</h3>

<p>安装nvidia driver/cuda要确保已安装的nvidia driver/cuda已被清理</p>

<pre><code># 卸载cuda：
sudo /usr/local/cuda-10.0/bin/uninstall_cuda_10.0.pl

# 卸载nvidia-driver：
sudo /usr/bin/nvidia-uninstall
</code></pre>

<p>安装nvidia-detect，用于检查显卡版本</p>

<pre><code>yum install nvidia-detect
# 运行命令 nvidia-detect -v 返回结果：
nvidia-detect -v
Probing for supported NVIDIA devices...
[10de:13bb] NVIDIA Corporation GM107GL [Quadro K620]
This device requires the current 390.87 NVIDIA driver kmod-nvidia
[8086:1912] Intel Corporation HD Graphics 530
An Intel display controller was also detected
</code></pre>

<p>注意这里的信息 [Quadro K620] 和390.87。
下载 <a href="https://www.nvidia.com/object/linux-amd64-display-archive.html">NVIDIA-Linux-x86_64-390.87.run</a></p>

<p>安装前的一系列准备工作</p>

<pre><code># 若系统很久没更新，这句可能耗时较长
yum -y update
yum -y install kernel-devel

yum -y install epel-release
yum -y install dkms

# 禁用nouveau
vim /etc/default/grub  #在“GRUB_CMDLINE_LINUX”中添加内容 rd.driver.blacklist=nouveau nouveau.modeset=0
grub2-mkconfig -o /boot/grub2/grub.cfg # 生成配置
vim /etc/modprobe.d/blacklist.conf # 打开（新建）文件，添加内容blacklist nouveau

mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img
dracut /boot/initramfs-$(uname -r).img $(uname -r)   # 更新配置，并重启
reboot
</code></pre>

<p>开机后确认是否禁用</p>

<pre><code>lsmod | grep nouveau  # 应该返回空

# 开始安装
sh NVIDIA-Linux-x86_64-390.87.run
</code></pre>

<p>安装过程中，会遇到一些选项：</p>

<pre><code>Install NVIDIA's 32-bit compatibility libraries (Yes)
centos Install NVIDIA's 32-bit compatibility libraries (Yes)
Would you like to run the nvidia-xconfig utility to automatically update your X configuration file... (NO)
</code></pre>

<p>最后查看 nvidia gpu 状态</p>

<pre><code>nvidia-smi
</code></pre>

<p>reference：
<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></p>

<h3 id="安装-docker">安装 Docker</h3>

<pre><code># Remove old version docker
sudo yum remove docker \
                docker-client \
                docker-client-latest \
                docker-common \
                docker-latest \
                docker-latest-logrotate \
                docker-logrotate \
                docker-engine

# Docker version
export DOCKER_VERSION=&quot;18.06.1.ce&quot;
# Setup the repository
sudo yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

# Check docker version
yum list docker-ce --showduplicates | sort -r

# Install docker with specified DOCKER_VERSION
sudo yum install -y docker-ce-${DOCKER_VERSION} docker-ce-cli-${DOCKER_VERSION} containerd.io

# Start docker
systemctl start docker

chown hadoop:netease /var/run/docker.sock
chown hadoop:netease /usr/bin/docker
</code></pre>

<p>Reference：<a href="https://docs.docker.com/install/linux/docker-ce/centos/">https://docs.docker.com/install/linux/docker-ce/centos/</a></p>

<h3 id="配置-docker">配置 Docker</h3>

<p>在 <code>/etc/docker/</code> 目录下，创建<code>daemon.json</code>文件, 添加以下配置，变量如image_registry_ip, etcd_host_ip, localhost_ip, yarn_dns_registry_host_ip, dns_host_ip需要根据具体环境，进行修改</p>

<pre><code>{
    &quot;insecure-registries&quot;: [&quot;${image_registry_ip}:5000&quot;],
    &quot;cluster-store&quot;:&quot;etcd://${etcd_host_ip1}:2379,${etcd_host_ip2}:2379,${etcd_host_ip3}:2379&quot;,
    &quot;cluster-advertise&quot;:&quot;{localhost_ip}:2375&quot;,
    &quot;dns&quot;: [&quot;${yarn_dns_registry_host_ip}&quot;, &quot;${dns_host_ip1}&quot;],
    &quot;hosts&quot;: [&quot;tcp://{localhost_ip}:2375&quot;, &quot;unix:///var/run/docker.sock&quot;]
}
</code></pre>

<p>重启 docker daemon：</p>

<pre><code>sudo systemctl restart docker
</code></pre>

<h3 id="检查-docker-version">检查 Docker version</h3>

<pre><code class="language-bash">$ docker version

Client:
 Version:      18.06.1-ce
 API version:  1.38
 Go version:   go1.10.3
 Git commit:   e68fc7a
 Built:        Tue Aug 21 17:23:03 2018
 OS/Arch:      linux/amd64
 Experimental: false

Server:
 Version:      18.06.1-ce
 API version:  1.38 (minimum version 1.12)
 Go version:   go1.10.3
 Git commit:   e68fc7a
 Built:        Tue Aug 21 17:23:03 2018
 OS/Arch:      linux/amd64
 Experimental: false
</code></pre>

<h3 id="安装-nvidia-docker">安装 nvidia-docker</h3>

<p>Hadoop-3.2 的 submarine 已支持 V2 版本的 nvidia-docker</p>

<pre><code># Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo
sudo yum install -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
</code></pre>

<p>在 <code>/var/lib/nvidia-docker/volumes/nvidia_driver/</code> 路径下，根据 <code>nvidia-driver</code> 的版本创建文件夹：</p>

<pre><code>mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87
# 其中390.87是nvidia driver的版本号

mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
mkdir /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64

cp /usr/bin/nvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/bin
cp /usr/lib64/libcuda* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64
cp /usr/lib64/libnvidia* /var/lib/nvidia-docker/volumes/nvidia_driver/390.87/lib64

# Test nvidia-smi
nvidia-docker run --rm nvidia/cuda:10.0-devel nvidia-smi
</code></pre>

<p>测试 docker, nvidia-docker, nvidia-driver 安装</p>

<pre><code># 测试一
nvidia-docker run -rm nvidia/cuda nvidia-smi
</code></pre>

<pre><code># 测试二
nvidia-docker run -it tensorflow/tensorflow:1.9.0-gpu bash
# 在docker中执行
python
import tensorflow as tf
tf.test.is_gpu_available()
</code></pre>

<p>卸载 nvidia-docker V2 的方法：</p>

<pre><code>sudo yum remove -y nvidia-docker2-2.0.3-1.docker18.06.1.ce
</code></pre>

<p>reference:
<a href="https://github.com/NVIDIA/nvidia-docker">https://github.com/NVIDIA/nvidia-docker</a></p>

<h3 id="tensorflow-image">Tensorflow Image</h3>

<p>CUDNN 和 CUDA 其实不需要在物理机上安装，因为 Submarine 中提供了已经包含了CUDNN 和 CUDA 的镜像文件，基础的Dockfile可参见<a href="docs/0.2.0/WriteDockerfileTF">WriteDockerfile</a></p>

<h3 id="测试-tf-环境">测试 TF 环境</h3>

<p>创建好 docker 镜像后，需要先手动检查 TensorFlow 是否可以正常使用，避免通过 YARN 调度后出现问题，可以执行以下命令</p>

<pre><code class="language-shell">$ docker run -it ${docker_image_name} /bin/bash
# &gt;&gt;&gt; 进入容器
$ python
$ python &gt;&gt; import tensorflow as tf
$ python &gt;&gt; tf.__version__
</code></pre>

<p>如果出现问题，可以按照以下路径进行排查</p>

<ol>
<li>环境变量是否设置正确</li>
</ol>

<pre><code>   echo $LD_LIBRARY_PATH
   /usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
</code></pre>

<ol>
<li>libcuda.so.1,libcuda.so是否在LD_LIBRARY_PATH指定的路径中</li>
</ol>

<pre><code>   ls -l /usr/local/nvidia/lib64 | grep libcuda.so
</code></pre>

<h3 id="安装-etcd">安装 Etcd</h3>

<p>运行 Submarine/install.sh 脚本，就可以在指定服务器中安装 Etcd 组件和服务自启动脚本。</p>

<pre><code class="language-shell">$ ./Submarine/install.sh
# 通过如下命令查看 Etcd 服务状态
systemctl status Etcd.service
</code></pre>

<p>检查 Etcd 服务状态</p>

<pre><code class="language-shell">$ etcdctl cluster-health
member 3adf2673436aa824 is healthy: got healthy result from http://${etcd_host_ip1}:2379
member 85ffe9aafb7745cc is healthy: got healthy result from http://${etcd_host_ip2}:2379
member b3d05464c356441a is healthy: got healthy result from http://${etcd_host_ip3}:2379
cluster is healthy

$ etcdctl member list
3adf2673436aa824: name=etcdnode3 peerURLs=http://${etcd_host_ip1}:2380 clientURLs=http://${etcd_host_ip1}:2379 isLeader=false
85ffe9aafb7745cc: name=etcdnode2 peerURLs=http://${etcd_host_ip2}:2380 clientURLs=http://${etcd_host_ip2}:2379 isLeader=false
b3d05464c356441a: name=etcdnode1 peerURLs=http://${etcd_host_ip3}:2380 clientURLs=http://${etcd_host_ip3}:2379 isLeader=true
</code></pre>

<p>其中，${etcd_host_ip*} 是etcd服务器的ip</p>

<h3 id="安装-calico">安装 Calico</h3>

<p>运行 Submarine/install.sh 脚本，就可以在指定服务器中安装 Calico 组件和服务自启动脚本。</p>

<pre><code>systemctl start calico-node.service
systemctl status calico-node.service
</code></pre>

<h4 id="检查-calico-网络">检查 Calico 网络</h4>

<pre><code class="language-shell"># 执行如下命令，注意：不会显示本服务器的状态，只显示其他的服务器状态
$ calicoctl node status
Calico process is running.

IPv4 BGP status
+---------------+-------------------+-------+------------+-------------+
| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |
+---------------+-------------------+-------+------------+-------------+
| ${host_ip1} | node-to-node mesh | up    | 2018-09-21 | Established |
| ${host_ip2} | node-to-node mesh | up    | 2018-09-21 | Established |
| ${host_ip3} | node-to-node mesh | up    | 2018-09-21 | Established |
+---------------+-------------------+-------+------------+-------------+

IPv6 BGP status
No IPv6 peers found.
</code></pre>

<p>创建docker container，验证calico网络</p>

<pre><code>docker network create --driver calico --ipam-driver calico-ipam calico-network
docker run --net calico-network --name workload-A -tid busybox
docker run --net calico-network --name workload-B -tid busybox
docker exec workload-A ping workload-B
</code></pre>

<h2 id="安装-hadoop">安装 Hadoop</h2>

<h3 id="编译-hadoop">编译 Hadoop</h3>

<pre><code>mvn package -Pdist -DskipTests -Dtar
</code></pre>

<h3 id="启动-yarn服务">启动 YARN服务</h3>

<pre><code>YARN_LOGFILE=resourcemanager.log ./sbin/yarn-daemon.sh start resourcemanager
YARN_LOGFILE=nodemanager.log ./sbin/yarn-daemon.sh start nodemanager
YARN_LOGFILE=timeline.log ./sbin/yarn-daemon.sh start timelineserver
YARN_LOGFILE=mr-historyserver.log ./sbin/mr-jobhistory-daemon.sh start historyserver
</code></pre>

<h3 id="启动-registery-dns-服务-只有yarn-native-service-需要">启动 registery dns 服务 (只有YARN native service 需要)</h3>

<pre><code>sudo YARN_LOGFILE=registrydns.log ./yarn-daemon.sh start registrydns
</code></pre>

<h3 id="测试-wordcount">测试 wordcount</h3>

<p>通过测试最简单的 wordcount ，检查 YARN 是否正确安装</p>

<pre><code>./bin/hadoop jar /home/hadoop/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0-SNAPSHOT.jar wordcount /tmp/wordcount.txt /tmp/wordcount-output4
</code></pre>

<h2 id="在yarn上使用gpu">在YARN上使用GPU</h2>

<h3 id="resourcemanager-nodemanager-中添加gpu支持">Resourcemanager, Nodemanager 中添加GPU支持</h3>

<p>在 yarn 配置文件夹(conf或etc/hadoop)中创建 resource-types.xml，添加：</p>

<pre><code>   &lt;configuration&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.resource-types&lt;/name&gt;
       &lt;value&gt;yarn.io/gpu&lt;/value&gt;
     &lt;/property&gt;
   &lt;/configuration&gt;
</code></pre>

<h3 id="resourcemanager-的-gpu-配置">Resourcemanager 的 GPU 配置</h3>

<p>resourcemanager 使用的 scheduler 必须是 capacity scheduler，在 capacity-scheduler.xml 中修改属性：</p>

<pre><code>   &lt;configuration&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;
       &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;
     &lt;/property&gt;
   &lt;/configuration&gt;
</code></pre>

<h3 id="nodemanager-的-gpu-配置">Nodemanager 的 GPU 配置</h3>

<p>在 nodemanager 的 yarn-site.xml 中添加配置：</p>

<pre><code>   &lt;configuration&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.nodemanager.resource-plugins&lt;/name&gt;
       &lt;value&gt;yarn.io/gpu&lt;/value&gt;
     &lt;/property&gt;
     &lt;!--Use nvidia docker v2--&gt;
     &lt;property&gt;
       &lt;name&gt;yarn.nodemanager.resource-plugins.gpu.docker-plugin&lt;/name&gt;
       &lt;value&gt;nvidia-docker-v2&lt;/value&gt;
     &lt;/property&gt;
   &lt;/configuration&gt;
</code></pre>

<p>在 container-executor.cfg 中添加配置：</p>

<pre><code>   [docker]
   ...
   # 在[docker]已有配置中，添加以下内容：
   # /usr/bin/nvidia-docker是nvidia-docker路径
   # nvidia_driver_375.26的版本号375.26，可以使用nvidia-smi查看
   docker.allowed.volume-drivers=/usr/bin/nvidia-docker
   docker.allowed.devices=/dev/nvidiactl,/dev/nvidia-uvm,/dev/nvidia-uvm-tools,/dev/nvidia1,/dev/nvidia0
   docker.allowed.ro-mounts=nvidia_driver_375.26
   # Use nvidia docker v2
   docker.allowed.runtimes=nvidia

   [gpu]
   module.enabled=true

   [cgroups]
   # /sys/fs/cgroup是cgroup的mount路径
   # /hadoop-yarn是yarn在cgroup路径下默认创建的path
   root=/sys/fs/cgroup
   yarn-hierarchy=/hadoop-yarn
</code></pre>

<h3 id="提交验证">提交验证</h3>

<p>Distributed-shell + GPU + cgroup</p>

<pre><code class="language-bash"> ... job run \
 --env DOCKER_JAVA_HOME=/opt/java \
 --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf-gpu \
 --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
 --docker_image tf-1.13.1-gpu:0.0.1 \
 --input_path hdfs://default/tmp/cifar-10-data \
 --checkpoint_path hdfs://default/user/hadoop/tf-distributed-checkpoint \
 --num_ps 0 \
 --ps_resources memory=4G,vcores=2,gpu=0 \
 --ps_launch_cmd &quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0&quot; \
 --worker_resources memory=4G,vcores=2,gpu=1 --verbose \
 --num_workers 1 \
 --worker_launch_cmd &quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=%%input_path%% --job-dir=%checkpoint_path% --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=1&quot;
</code></pre>

<h2 id="问题">问题</h2>

<h3 id="问题一-操作系统重启导致-nodemanager-启动失败">问题一: 操作系统重启导致 nodemanager 启动失败</h3>

<pre><code>2018-09-20 18:54:39,785 ERROR org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Failed to bootstrap configured resource subsystems!
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Unexpected: Cannot create yarn cgroup Subsystem:cpu Mount points:/proc/mounts User:yarn Path:/sys/fs/cgroup/cpu,cpuacct/hadoop-yarn
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializePreMountedCGroupController(CGroupsHandlerImpl.java:425)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializeCGroupController(CGroupsHandlerImpl.java:377)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:98)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:87)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.bootstrap(ResourceHandlerChain.java:58)
  at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:320)
  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:389)
  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:929)
  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:997)
2018-09-20 18:54:39,789 INFO org.apache.hadoop.service.AbstractService: Service NodeManager failed in state INITED
</code></pre>

<p>解决方法：使用 <code>root</code> 账号给 <code>yarn</code> 用户修改 <code>/sys/fs/cgroup/cpu,cpuacct</code> 的权限</p>

<pre><code>chown :yarn -R /sys/fs/cgroup/cpu,cpuacct
chmod g+rwx -R /sys/fs/cgroup/cpu,cpuacct
</code></pre>

<p>在支持gpu时，还需cgroup devices路径权限</p>

<pre><code>chown :yarn -R /sys/fs/cgroup/devices
chmod g+rwx -R /sys/fs/cgroup/devices
</code></pre>

<h3 id="问题二-container-executor-权限问题">问题二：container-executor 权限问题</h3>

<pre><code>2018-09-21 09:36:26,102 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: IOException executing command:
java.io.IOException: Cannot run program &quot;/etc/yarn/sbin/Linux-amd64-64/container-executor&quot;: error=13, Permission denied
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:938)
        at org.apache.hadoop.util.Shell.run(Shell.java:901)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
</code></pre>

<p><code>/etc/yarn/sbin/Linux-amd64-64/container-executor</code> 该文件的权限应为6050</p>

<h3 id="问题三-查看系统服务启动日志">问题三：查看系统服务启动日志</h3>

<pre><code>journalctl -u docker
</code></pre>

<h3 id="问题四-docker-无法删除容器的问题-device-or-resource-busy">问题四：Docker 无法删除容器的问题 <code>device or resource busy</code></h3>

<pre><code class="language-bash">$ docker rm 0bfafa146431
Error response from daemon: Unable to remove filesystem for 0bfafa146431771f6024dcb9775ef47f170edb2f1852f71916ba44209ca6120a: remove /app/docker/containers/0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a/shm: device or resource busy
</code></pre>

<p>编写 <code>find-busy-mnt.sh</code> 脚本，检查 <code>device or resource busy</code> 状态的容器挂载文件</p>

<pre><code class="language-bash">#!/bin/bash

# A simple script to get information about mount points and pids and their
# mount namespaces.

if [ $# -ne 1 ];then
echo &quot;Usage: $0 &lt;devicemapper-device-id&gt;&quot;
exit 1
fi

ID=$1

MOUNTS=`find /proc/*/mounts | xargs grep $ID 2&gt;/dev/null`

[ -z &quot;$MOUNTS&quot; ] &amp;&amp;  echo &quot;No pids found&quot; &amp;&amp; exit 0

printf &quot;PID\tNAME\t\tMNTNS\n&quot;
echo &quot;$MOUNTS&quot; | while read LINE; do
PID=`echo $LINE | cut -d &quot;:&quot; -f1 | cut -d &quot;/&quot; -f3`
# Ignore self and thread-self
if [ &quot;$PID&quot; == &quot;self&quot; ] || [ &quot;$PID&quot; == &quot;thread-self&quot; ]; then
  continue
fi
NAME=`ps -q $PID -o comm=`
MNTNS=`readlink /proc/$PID/ns/mnt`
printf &quot;%s\t%s\t\t%s\n&quot; &quot;$PID&quot; &quot;$NAME&quot; &quot;$MNTNS&quot;
done
</code></pre>

<p>查找占用目录的进程</p>

<pre><code class="language-bash">$ chmod +x find-busy-mnt.sh
./find-busy-mnt.sh 0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a
# PID   NAME            MNTNS
# 5007  ntpd            mnt:[4026533598]
$ kill -9 5007
</code></pre>

<h3 id="问题五-命令sudo-nvidia-docker-run-报错">问题五：命令sudo nvidia-docker run 报错</h3>

<pre><code>docker: Error response from daemon: create nvidia_driver_361.42: VolumeDriver.Create: internal error, check logs for details.
See 'docker run --help'.
</code></pre>

<p>解决方法：</p>

<pre><code>#查看nvidia-docker状态，是不是启动有问题，可以使用
$ systemctl status nvidia-docker
$ journalctl -n -u nvidia-docker
#重启下nvidia-docker
systemctl stop nvidia-docker
systemctl start nvidia-docker
</code></pre>

<h3 id="问题六-yarn-启动容器失败">问题六：YARN 启动容器失败</h3>

<p>如果你创建的容器数（PS+Work&gt;GPU显卡总数），可能会出现容器创建失败，那是因为在一台服务器上同时创建了超过本机显卡总数的容器。</p>


            
            <h1><a href="/submarine/docs/0.2.0/QuickStart/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="submarine-quick-start-guide">Submarine Quick Start Guide</h1>

<h2 id="prerequisite">Prerequisite</h2>

<p>Must:</p>

<ul>
<li>Apache Hadoop version newer than 2.7.3</li>
</ul>

<p>Optional:</p>

<ul>
<li><a href="https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/yarn-service/Overview.html">Enable YARN Service</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html">Enable GPU on YARN</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-site/DockerContainers.html">Enable Docker on YARN</a></li>
<li><a href="docs/0.2.0/WriteDockerfileTF">Build Docker images</a></li>
</ul>

<p><br /></p>

<h2 id="submarine-configuration">Submarine Configuration</h2>

<p>After submarine 0.2.0, it supports two runtimes which are YARN service
 runtime and Linkedin&rsquo;s TonY runtime for YARN. Each runtime can support both
 Tensorflow and PyTorch framework. But we don&rsquo;t need to worry about the
  usage because the two runtime implements the same interface.</p>

<p>So before we start running a job, the runtime type should be picked. The
runtime choice may vary depending on different requirements. Check below
table to choose your runtime.</p>

<p>Note that if you want to quickly try Submarine on new or existing YARN cluster, use TonY runtime will help you get start easier(0.2.0+)</p>

<p><img src="./images/tony-vs-service-runtime.png" alt="alt text" title="Choose Runtime" /></p>

<p>For the environment setup, please check <a href="docs/0.2.0/InstallationGuide">InstallationGuide</a> or <a href="docs/0.2.0/InstallationGuideChineseVersion">InstallationGuideCN</a></p>

<p>Once the applicable runtime is chosen and environment is ready, a <code>submarine.xml</code> need to be created under
 <code>$HADOOP_CONF_DIR</code>. To use the TonY runtime, please set below value in the
 submarine configuration.</p>

<table>
<thead>
<tr>
<th align="left">Configuration Name</th>
<th align="left">Description</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>submarine.runtime.class</code></td>
<td align="left">&ldquo;org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory&rdquo; or &ldquo;org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory&rdquo;</td>
</tr>
</tbody>
</table>

<p><br /></p>

<p>A sample <code>submarine.xml</code> is here:</p>

<pre><code class="language-java">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;submarine.runtime.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.yarn.submarine.runtimes.tony.TonyRuntimeFactory&lt;/value&gt;
    &lt;!-- Alternatively, you can use:
    &lt;value&gt;org.apache.hadoop.yarn.submarine.runtimes.yarnservice.YarnServiceRuntimeFactory&lt;/value&gt;
    --&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>For more Submarine configuration:</p>

<table>
<thead>
<tr>
<th align="left">Configuration Name</th>
<th align="left">Description</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>submarine.localization.max-allowed-file-size-mb</code></td>
<td align="left">Optional. This sets a size limit to the file/directory to be localized in &ldquo;-localization&rdquo; CLI option. 2GB by default.</td>
</tr>
</tbody>
</table>

<p><br /></p>

<h2 id="launch-training-job">Launch Training Job</h2>

<p>This section will give us an idea of how the submarine CLI looks like.</p>

<p>Although the run job command looks simple, different job may have very different parameters.</p>

<p>For a quick try on Mnist example with TonY runtime, check <a href="docs/0.2.0/TonYRuntimeGuide">TonY Mnist Example</a></p>

<p>For a quick try on Cifar10 example with YARN native service runtime, check <a href="docs/0.2.0/RunningDistributedCifar10TFJobs">YARN Service Cifar10 Example</a></p>

<p><br /></p>

<h2 id="get-job-history-logs">Get job history / logs</h2>

<h3 id="get-job-status-from-cli">Get Job Status from CLI</h3>

<pre><code class="language-shell">CLASSPATH=path-to/hadoop-conf:path-to/hadoop-submarine-all-${SUBMARINE_VERSION}-hadoop-${HADOOP_VERSION}.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job show --name tf-job-001
</code></pre>

<p>Output looks like:</p>

<pre><code class="language-shell">Job Meta Info:
  Application Id: application_1532131617202_0005
  Input Path: hdfs://default/dataset/cifar-10-data
  Checkpoint Path: hdfs://default/tmp/cifar-10-jobdir
  Run Parameters: --name tf-job-001 --docker_image &lt;your-docker-image&gt;
                  (... all your commandline before run the job)
</code></pre>

<p>After that, you can run <code>tensorboard --logdir=&lt;checkpoint-path&gt;</code> to view Tensorboard of the job.</p>

<h3 id="get-component-logs-from-a-training-job">Get component logs from a training job</h3>

<p>We can use <code>yarn logs -applicationId &lt;applicationId&gt;</code> to get logs from CLI.
Or from YARN UI:</p>

<p><img src="./images/job-logs-ui.png" alt="alt text" title="Job logs UI" /></p>

<p><br /></p>

<h2 id="submarine-commandline-options">Submarine Commandline options</h2>

<pre><code class="language-$xslt">usage: ... job run

 -framework &lt;arg&gt;             Framework to use.
                              Valid values are: tensorflow, pytorch.
                              The default framework is Tensorflow.
 -checkpoint_path &lt;arg&gt;       Training output directory of the job, could
                              be local or other FS directory. This
                              typically includes checkpoint files and
                              exported model
 -docker_image &lt;arg&gt;          Docker image name/tag
 -env &lt;arg&gt;                   Common environment variable of worker/ps
 -input_path &lt;arg&gt;            Input of the job, could be local or other FS
                              directory
 -name &lt;arg&gt;                  Name of the job
 -num_ps &lt;arg&gt;                Number of PS tasks of the job, by default
                              it's 0
 -num_workers &lt;arg&gt;           Numnber of worker tasks of the job, by
                              default it's 1
 -ps_docker_image &lt;arg&gt;       Specify docker image for PS, when this is
                              not specified, PS uses --docker_image as
                              default.
 -ps_launch_cmd &lt;arg&gt;         Commandline of worker, arguments will be
                              directly used to launch the PS
 -ps_resources &lt;arg&gt;          Resource of each PS, for example
                              memory-mb=2048,vcores=2,yarn.io/gpu=2
 -queue &lt;arg&gt;                 Name of queue to run the job, by default it
                              uses default queue
 -saved_model_path &lt;arg&gt;      Model exported path (savedmodel) of the job,
                              which is needed when exported model is not
                              placed under ${checkpoint_path}could be
                              local or other FS directory. This will be
                              used to serve.
 -tensorboard &lt;arg&gt;           Should we run TensorBoard for this job? By
                              default it's true
 -verbose                     Print verbose log for troubleshooting
 -wait_job_finish             Specified when user want to wait the job
                              finish
 -worker_docker_image &lt;arg&gt;   Specify docker image for WORKER, when this
                              is not specified, WORKER uses --docker_image
                              as default.
 -worker_launch_cmd &lt;arg&gt;     Commandline of worker, arguments will be
                              directly used to launch the worker
 -worker_resources &lt;arg&gt;      Resource of each worker, for example
                              memory-mb=2048,vcores=2,yarn.io/gpu=2
 -localization &lt;arg&gt;          Specify localization to remote/local
                              file/directory available to all container(Docker).
                              Argument format is &quot;RemoteUri:LocalFilePath[:rw]&quot;
                              (ro permission is not supported yet).
                              The RemoteUri can be a file or directory in local
                              or HDFS or s3 or abfs or http .etc.
                              The LocalFilePath can be absolute or relative.
                              If relative, it'll be under container's implied
                              working directory.
                              This option can be set mutiple times.
                              Examples are
                              -localization &quot;hdfs:///user/yarn/mydir2:/opt/data&quot;
                              -localization &quot;s3a:///a/b/myfile1:./&quot;
                              -localization &quot;https:///a/b/myfile2:./myfile&quot;
                              -localization &quot;/user/yarn/mydir3:/opt/mydir3&quot;
                              -localization &quot;./mydir1:.&quot;
 -conf &lt;arg&gt;                  User specified configuration, as
                              key=val pairs.
</code></pre>

<h4 id="notes">Notes:</h4>

<p>When using <code>localization</code> option to make a collection of dependency Python
scripts available to entry python script in the container, you may also need to
set <code>PYTHONPATH</code> environment variable as below to avoid module import error
reported from <code>entry_script.py</code>.</p>

<pre><code class="language-shell">... job run
  # the entry point
  --localization entry_script.py:&lt;path&gt;/entry_script.py
  # the dependency Python scripts of the entry point
  --localization other_scripts_dir:&lt;path&gt;/other_scripts_dir
  # the PYTHONPATH env to make dependency available to entry script
  --env PYTHONPATH=&quot;&lt;path&gt;/other_scripts_dir&quot;
  --worker_launch_cmd &quot;python &lt;path&gt;/entry_script.py ...&quot;
</code></pre>

<p><br /></p>

<h2 id="build-from-source">Build From Source</h2>

<p>If you want to build the Submarine project by yourself, you can follow it <a href="docs/0.2.0/BuildFromCode">here</a></p>


            
            <h1><a href="/submarine/docs/0.2.0/RunningDistributedCifar10TFJobs/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="cifar10-tensorflow-estimator-example-with-yarn-service">Cifar10 Tensorflow Estimator Example With YARN Service</h1>

<h2 id="prepare-data-for-training">Prepare data for training</h2>

<p>CIFAR-10 is a common benchmark in machine learning for image recognition. Below example is based on CIFAR-10 dataset.</p>

<p>1) Checkout <a href="https://github.com/tensorflow/models/:">https://github.com/tensorflow/models/:</a></p>

<pre><code>git clone https://github.com/tensorflow/models/
</code></pre>

<p>2) Go to <code>models/tutorials/image/cifar10_estimator</code></p>

<p>3) Generate data by using following command: (required Tensorflow installed)</p>

<pre><code>python generate_cifar10_tfrecords.py --data-dir=cifar-10-data
</code></pre>

<p>4) Upload data to HDFS</p>

<pre><code>hadoop fs -put cifar-10-data/ /dataset/cifar-10-data
</code></pre>

<p><strong>Warning:</strong></p>

<p>Please note that YARN service doesn&rsquo;t allow multiple services with the same name, so please run following command</p>

<pre><code>yarn application -destroy &lt;service-name&gt;
</code></pre>

<p>to delete services if you want to reuse the same service name.</p>

<h2 id="prepare-docker-images">Prepare Docker images</h2>

<p>Refer to <a href="docs/0.2.0/WriteDockerfileTF">Write Dockerfile</a> to build a Docker image or use prebuilt one:</p>

<ul>
<li>hadoopsubmarine/tensorflow1.13.1-hadoop3.1.2-cpu:1.0.0</li>
<li>hadoopsubmarine/tensorflow1.13.1-hadoop3.1.2-gpu:1.0.0</li>
</ul>

<h2 id="run-tensorflow-jobs">Run Tensorflow jobs</h2>

<h3 id="run-standalone-training">Run standalone training</h3>

<pre><code>SUBMARINE_VERSION=0.2.0
CLASSPATH=`path-to/hadoop classpath --glob`:path-to/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:
path-to/hadoop-submarine-yarnservice-runtime-${SUBMARINE_VERSION}.jar:path-to/hadoop-submarine-tony-
runtime-${SUBMARINE_VERSION}.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run \
   --name tf-job-001 --verbose --docker_image &lt;image&gt; \
   --input_path hdfs://default/dataset/cifar-10-data \
   --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/
   --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current
   --num_workers 1 --worker_resources memory=8G,vcores=2,gpu=1 \
   --worker_launch_cmd &quot;cd /test/models/tutorials/image/cifar10_estimator &amp;&amp; python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --num-gpus=2 --sync&quot; \
   --tensorboard --tensorboard_docker_image tf-1.13.1-cpu:0.0.1
</code></pre>

<p>Explanations:</p>

<ul>
<li>When access of HDFS is required, the two environments are required to indicate: DOCKER_JAVA_HOME and DOCKER_HADOOP_HDFS_HOME to access libhdfs libraries <em>inside Docker image</em>. We will try to eliminate specifying this in the future.</li>
<li>Docker image for worker and tensorboard can be specified separately. For this case, Tensorboard doesn&rsquo;t need GPU, so we will use cpu Docker image for Tensorboard. (Same for parameter-server in the distributed example below).</li>
</ul>

<h3 id="run-distributed-training">Run distributed training</h3>

<pre><code>SUBMARINE_VERSION=0.2.0
CLASSPATH=`path-to/hadoop classpath --glob`:path-to/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:
path-to/hadoop-submarine-yarnservice-runtime-${SUBMARINE_VERSION}.jar:path-to/hadoop-submarine-tony-
runtime-${SUBMARINE_VERSION}.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run \
   --name tf-job-001 --verbose --docker_image tf-1.13.1-gpu:0.0.1 \
   --input_path hdfs://default/dataset/cifar-10-data \
   --env(s) (same as standalone)
   --num_workers 2 \
   --worker_resources memory=8G,vcores=2,gpu=1 \
   --worker_launch_cmd &quot;cd /test/models/tutorials/image/cifar10_estimator &amp;&amp; python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --num-gpus=2 --sync&quot;  \
   --ps_docker_image tf-1.13.1-cpu:0.0.1 \
   --num_ps 1 --ps_resources memory=4G,vcores=2,gpu=0  \
   --ps_launch_cmd &quot;cd /test/models/tutorials/image/cifar10_estimator &amp;&amp; python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --num-gpus=0&quot; \
   --tensorboard --tensorboard_docker_image tf-1.13.1-cpu:0.0.1
</code></pre>

<p>Explanations:</p>

<ul>
<li><code>&gt;1</code> num_workers indicates it is a distributed training.</li>
<li>Parameters / resources / Docker image of parameter server can be specified separately. For many cases, parameter server doesn&rsquo;t require GPU.</li>
</ul>

<p>For the meaning of the individual parameters, see the <a href="docs/0.2.0/QuickStart">QuickStart</a> page!</p>

<p><em>Outputs of distributed training</em></p>

<p>Sample output of master:</p>

<pre><code>...
allow_soft_placement: true
, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe77cb15050&gt;, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
...
2018-05-06 22:29:14.656022: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -&gt; {0 -&gt; localhost:8000}
2018-05-06 22:29:14.656097: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; ps-0.distributed-tf.root.tensorflow.site:8000}
2018-05-06 22:29:14.656112: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; worker-0.distributed-tf.root.tensorflow.site:8000}
2018-05-06 22:29:14.659359: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
...
INFO:tensorflow:Restoring parameters from hdfs://default/tmp/cifar-10-jobdir/model.ckpt-0
INFO:tensorflow:Evaluation [1/625]
INFO:tensorflow:Evaluation [2/625]
INFO:tensorflow:Evaluation [3/625]
INFO:tensorflow:Evaluation [4/625]
INFO:tensorflow:Evaluation [5/625]
INFO:tensorflow:Evaluation [6/625]
...
INFO:tensorflow:Validation (step 1): loss = 1220.6445, global_step = 1, accuracy = 0.1
INFO:tensorflow:loss = 6.3980675, step = 0
INFO:tensorflow:loss = 6.3980675, learning_rate = 0.1
INFO:tensorflow:global_step/sec: 2.34092
INFO:tensorflow:Average examples/sec: 1931.22 (1931.22), step = 100
INFO:tensorflow:Average examples/sec: 354.236 (38.6479), step = 110
INFO:tensorflow:Average examples/sec: 211.096 (38.7693), step = 120
INFO:tensorflow:Average examples/sec: 156.533 (38.1633), step = 130
INFO:tensorflow:Average examples/sec: 128.6 (38.7372), step = 140
INFO:tensorflow:Average examples/sec: 111.533 (39.0239), step = 150
</code></pre>

<p>Sample output of worker:</p>

<pre><code>, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc2a490b050&gt;, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
...
2018-05-06 22:28:45.807936: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -&gt; {0 -&gt; master-0.distributed-tf.root.tensorflow.site:8000}
2018-05-06 22:28:45.808040: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; ps-0.distributed-tf.root.tensorflow.site:8000}
2018-05-06 22:28:45.808064: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:8000}
2018-05-06 22:28:45.809919: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
...
INFO:tensorflow:loss = 5.319096, step = 0
INFO:tensorflow:loss = 5.319096, learning_rate = 0.1
INFO:tensorflow:Average examples/sec: 49.2338 (49.2338), step = 10
INFO:tensorflow:Average examples/sec: 52.117 (55.3589), step = 20
INFO:tensorflow:Average examples/sec: 53.2754 (55.7541), step = 30
INFO:tensorflow:Average examples/sec: 53.8388 (55.6028), step = 40
INFO:tensorflow:Average examples/sec: 54.1082 (55.2134), step = 50
INFO:tensorflow:Average examples/sec: 54.3141 (55.3676), step = 60
</code></pre>

<p>Sample output of ps:</p>

<pre><code>...
, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4be54dff90&gt;, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
...
2018-05-06 22:28:42.562316: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -&gt; {0 -&gt; master-0.distributed-tf.root.tensorflow.site:8000}
2018-05-06 22:28:42.562408: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:8000}
2018-05-06 22:28:42.562433: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; worker-0.distributed-tf.root.tensorflow.site:8000}
2018-05-06 22:28:42.564242: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:8000
</code></pre>

<h4 id="notes">Notes:</h4>

<p>When using YARN native service runtime, you can view multiple job training history like from the <code>Tensorboard</code> link:</p>

<p><img src="./images/multiple-tensorboard-jobs.png" alt="alt text" title="Tensorboard for multiple jobs" /></p>

<h2 id="run-tensorboard-to-monitor-your-jobs">Run tensorboard to monitor your jobs</h2>

<pre><code class="language-shell"># Cleanup previous tensorboard service if needed

SUBMARINE_VERSION=0.2.0
CLASSPATH=`path-to/hadoop classpath --glob`:path-to/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:path-to/hadoop-submarine-yarnservice-runtime-${SUBMARINE_VERSION}.jar:path-to/hadoop-submarine-tony-runtime-${SUBMARINE_VERSION}.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run \
  --name tensorboard-service \
  --verbose \
  --docker_image &lt;your-docker-image&gt; \
  --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre/ \
  --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current \
  --checkpoint_path hdfs://default/tmp/cifar-10-jobdir \
  --worker_resources memory=2G,vcores=2 \
  --worker_launch_cmd &quot;pwd&quot; \
  --tensorboard
</code></pre>


            
            <h1><a href="/submarine/docs/0.2.0/RunningSingleNodeCifar10PTJobs/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="tutorial-running-a-standalone-cifar10-pytorch-estimator-example">Tutorial: Running a standalone Cifar10 PyTorch Estimator Example.</h1>

<p>Currently, PyTorch integration with Submarine only supports PyTorch in standalone (non-distributed mode).
Please also note that HDFS as a data source is not yet supported by PyTorch.</p>

<h2 id="what-is-cifar-10">What is CIFAR-10?</h2>

<p>CIFAR-10 is a common benchmark in machine learning for image recognition. Below example is based on CIFAR-10 dataset.</p>

<p><strong>Warning:</strong></p>

<p>Please note that YARN service doesn&rsquo;t allow multiple services with the same name, so please run following command</p>

<pre><code>yarn application -destroy &lt;service-name&gt;
</code></pre>

<p>to delete services if you want to reuse the same service name.</p>

<h2 id="prepare-docker-images">Prepare Docker images</h2>

<p>Refer to <a href="docs/0.2.0/WriteDockerfilePT">Write Dockerfile</a> to build a Docker image or use prebuilt one.</p>

<h2 id="running-pytorch-jobs">Running PyTorch jobs</h2>

<h3 id="run-standalone-training">Run standalone training</h3>

<pre><code class="language-shell">export HADOOP_CLASSPATH=&quot;/home/systest/hadoop-submarine-score-yarnservice-runtime-0.2.0-SNAPSHOT.jar:/home/systest/hadoop-submarine-core-0.2.0-SNAPSHOT.jar&quot;
/opt/hadoop/bin/yarn jar /home/systest/hadoop-submarine-core-0.2.0-SNAPSHOT.jar job run \
--name pytorch-job-001 \
--verbose \
--framework pytorch \
--wait_job_finish \
--docker_image pytorch-latest-gpu:0.0.1 \
--input_path hdfs://unused \
--env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre \
--env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.2 \
--env YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL=true \
--num_workers 1 \
--worker_resources memory=5G,vcores=2 \
--worker_launch_cmd &quot;cd /test/ &amp;&amp; python cifar10_tutorial.py&quot;
</code></pre>

<p>For the meaning of the individual parameters, see the <a href="docs/0.2.0/QuickStart">QuickStart</a> page!</p>

<p><strong>Remarks:</strong>
Please note that the input path parameter is mandatory, but not yet used by the PyTorch docker container.</p>


            
            <h1><a href="/submarine/docs/0.2.0/TestAndTroubleshooting/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   https://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h4 id="test-with-a-tensorflow-job">Test with a tensorflow job</h4>

<p>Distributed-shell + GPU + cgroup</p>

<pre><code class="language-bash"> ... \
 job run \
 --env DOCKER_JAVA_HOME=/opt/java \
 --env DOCKER_HADOOP_HDFS_HOME=/hadoop-current --name distributed-tf-gpu \
 --env YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK=calico-network \
 --worker_docker_image tf-1.13.1-gpu:0.0.1 \
 --ps_docker_image tf-1.13.1-cpu:0.0.1 \
 --input_path hdfs://${dfs_name_service}/tmp/cifar-10-data \
 --checkpoint_path hdfs://${dfs_name_service}/user/hadoop/tf-distributed-checkpoint \
 --num_ps 0 \
 --ps_resources memory=4G,vcores=2,gpu=0 \
 --ps_launch_cmd &quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --num-gpus=0&quot; \
 --worker_resources memory=4G,vcores=2,gpu=1 --verbose \
 --num_workers 1 \
 --worker_launch_cmd &quot;python /test/cifar10_estimator/cifar10_main.py --data-dir=hdfs://${dfs_name_service}/tmp/cifar-10-data --job-dir=hdfs://${dfs_name_service}/tmp/cifar-10-jobdir --train-steps=500 --eval-batch-size=16 --train-batch-size=16 --sync --num-gpus=1&quot;
</code></pre>

<h2 id="issues">Issues:</h2>

<h3 id="issue-1-fail-to-start-nodemanager-after-system-reboot">Issue 1: Fail to start nodemanager after system reboot</h3>

<pre><code>2018-09-20 18:54:39,785 ERROR org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Failed to bootstrap configured resource subsystems!
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Unexpected: Cannot create yarn cgroup Subsystem:cpu Mount points:/proc/mounts User:yarn Path:/sys/fs/cgroup/cpu,cpuacct/hadoop-yarn
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializePreMountedCGroupController(CGroupsHandlerImpl.java:425)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.initializeCGroupController(CGroupsHandlerImpl.java:377)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:98)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.bootstrap(CGroupsCpuResourceHandlerImpl.java:87)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.bootstrap(ResourceHandlerChain.java:58)
  at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:320)
  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:389)
  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:929)
  at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:997)
2018-09-20 18:54:39,789 INFO org.apache.hadoop.service.AbstractService: Service NodeManager failed in state INITED
</code></pre>

<p>Solution: Grant user yarn the access to  <code>/sys/fs/cgroup/cpu,cpuacct</code>, which is the subfolder of cgroup mount destination.</p>

<pre><code>chown :yarn -R /sys/fs/cgroup/cpu,cpuacct
chmod g+rwx -R /sys/fs/cgroup/cpu,cpuacct
</code></pre>

<p>If GPUs are used，the access to cgroup devices folder is neede as well</p>

<pre><code>chown :yarn -R /sys/fs/cgroup/devices
chmod g+rwx -R /sys/fs/cgroup/devices
</code></pre>

<h3 id="issue-2-container-executor-permission-denied">Issue 2: container-executor permission denied</h3>

<pre><code>2018-09-21 09:36:26,102 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: IOException executing command:
java.io.IOException: Cannot run program &quot;/etc/yarn/sbin/Linux-amd64-64/container-executor&quot;: error=13, Permission denied
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:938)
        at org.apache.hadoop.util.Shell.run(Shell.java:901)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
</code></pre>

<p>Solution: The permission of <code>/etc/yarn/sbin/Linux-amd64-64/container-executor</code> should be 6050</p>

<h3 id="issue-3-how-to-get-docker-service-log">Issue 3：How to get docker service log</h3>

<p>Solution: we can get docker log with the following command</p>

<pre><code>journalctl -u docker
</code></pre>

<h3 id="issue-4-docker-can-t-remove-containers-with-errors-like-device-or-resource-busy">Issue 4：docker can&rsquo;t remove containers with errors like <code>device or resource busy</code></h3>

<pre><code class="language-bash">$ docker rm 0bfafa146431
Error response from daemon: Unable to remove filesystem for 0bfafa146431771f6024dcb9775ef47f170edb2f1852f71916ba44209ca6120a: remove /app/docker/containers/0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a/shm: device or resource busy
</code></pre>

<p>Solution: to find which process leads to a <code>device or resource busy</code>, we can add a shell script, named <code>find-busy-mnt.sh</code></p>

<pre><code class="language-bash">#!/bin/bash

# A simple script to get information about mount points and pids and their
# mount namespaces.

if [ $# -ne 1 ];then
echo &quot;Usage: $0 &lt;devicemapper-device-id&gt;&quot;
exit 1
fi

ID=$1

MOUNTS=`find /proc/*/mounts | xargs grep $ID 2&gt;/dev/null`

[ -z &quot;$MOUNTS&quot; ] &amp;&amp;  echo &quot;No pids found&quot; &amp;&amp; exit 0

printf &quot;PID\tNAME\t\tMNTNS\n&quot;
echo &quot;$MOUNTS&quot; | while read LINE; do
PID=`echo $LINE | cut -d &quot;:&quot; -f1 | cut -d &quot;/&quot; -f3`
# Ignore self and thread-self
if [ &quot;$PID&quot; == &quot;self&quot; ] || [ &quot;$PID&quot; == &quot;thread-self&quot; ]; then
  continue
fi
NAME=`ps -q $PID -o comm=`
MNTNS=`readlink /proc/$PID/ns/mnt`
printf &quot;%s\t%s\t\t%s\n&quot; &quot;$PID&quot; &quot;$NAME&quot; &quot;$MNTNS&quot;
done
</code></pre>

<p>Kill the process by pid, which is found by the script</p>

<pre><code class="language-bash">$ chmod +x find-busy-mnt.sh
./find-busy-mnt.sh 0bfafa146431771f6024dcb9775ef47f170edb2f152f71916ba44209ca6120a
# PID   NAME            MNTNS
# 5007  ntpd            mnt:[4026533598]
$ kill -9 5007
</code></pre>

<h3 id="issue-5-yarn-failed-to-start-containers">Issue 5：Yarn failed to start containers</h3>

<p>if the number of GPUs required by applications is larger than the number of GPUs in the cluster, there would be some containers can&rsquo;t be created.</p>


            
        </ul>
  </div>


<ul class="pagination">
    
    <li class="page-item">
        <a href="/submarine/docs/" class="page-link" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
    </li>
    
    <li class="page-item disabled">
    <a href="" class="page-link" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
    </li>
    
    
    
    
    
    
    
        
        
    
    
    <li class="page-item active"><a class="page-link" href="/submarine/docs/">1</a></li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item"><a class="page-link" href="/submarine/docs/page/2/">2</a></li>
    
    
    <li class="page-item">
    <a href="/submarine/docs/page/2/" class="page-link" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
    </li>
    
    <li class="page-item">
        <a href="/submarine/docs/page/2/" class="page-link" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
    </li>
    
</ul>

</section>
</div>

<footer>
    <div class="container">

        <div class="col-md-12 trademark">
            <p>&copy; 2019 <a href="http://apache.org">The Apache Software Foundation</a>,<br/>
                Apache, Apache Hadoop, the Apache feather logo, are trademarks of The Apache Software Foundation.
            <p>
        </div>
    </div>
</footer>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"
        integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS"
        crossorigin="anonymous"></script>


</body>
</html>

