
<!DOCTYPE html>

<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Machine learning engine for Hadoop">
    <meta name="keywords" content="hadoop, submarine, machine learning, tensorflow, deep learning"/>
    <meta name="robots" content="index,follow"/>
    <meta name="language" content="en"/>

    <title>Apache Hadoop Submarine</title>

    <base href="/submarine/">

    <link rel="canonical" href="https://hadoop.apache.org/submarine">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"
          integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">

</head>

<body>


<div class="topnav">
    <div class="container">
        <ul class="breadcrumb col-md-6">
            <li>
                <img class="asf-logo" src="asf_feather.png"/>
                <a  href="https://www.apache.org">Apache Software Foundation</span></a>
            </li>
            <li><a href="https://hadoop.apache.org">Apache Hadoop&trade;</a></li>
            <li><a href="/submarine/">Submarine&trade;</a></li>
        </ul>
        <div class="col-md-6">
            <ul class="pull-right breadcrumb">
                <li><a href="http://www.apache.org/licenses/">License</a></li>
                <li><a href="https://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
                <li><a href="https://www.apache.org/foundation/thanks.html">Thanks</a></li>
                <li><a href="https://www.apache.org/security/">Security</a></li>
        </ul>
        </div>
    </div>

    <nav class="navbar navbar-default navbar-static-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                        data-target="#ratis-menu" aria-expanded="false">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>

            <div id="ratis-menu" class="collapse navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    
                    <li><a href="docs/Index">Documentation</a></li>
                    <li><a href="downloads">Download</a></li>
                    <li><a href="ecosystem">Ecosystem</a></li>
                    <li><a href="#qa">Q&A</a></li>
                    <li><a href="https://cwiki.apache.org/confluence/display/HADOOP/Submarine+Contributor+Guide">Wiki</a></li>
                    <li><a href="activity">Activity</a></li>
                    <li><a href="team">Team</a></li>
                </ul>
            </div>

    </nav>

    <div style="max-height: 200px; overflow: hidden;">
      <img src="cloud.png" style="width: 100%; left:0px;"/>
    </div>

</div>

</div>

<div class="container">

<section id="main">
  <div>
    <h1 id="title">Docs Archive</h1>
        <ul id="list">
            
            <h1><a href="/submarine/docs/0.2.0/TonYRuntimeGuide/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="tony-runtime-quick-start-guide">TonY Runtime Quick Start Guide</h1>

<h2 id="prerequisite">Prerequisite</h2>

<p>Check out the <a href="docs/0.2.0/QuickStart">QuickStart</a></p>

<h2 id="launch-tensorflow-application">Launch TensorFlow Application:</h2>

<h3 id="without-docker">Without Docker</h3>

<p>You need:</p>

<ul>
<li>Build a Python virtual environment with TensorFlow 1.13.1 installed</li>
<li>A cluster with Hadoop 2.7 or above.</li>
<li>TonY library 0.3.2 or above. Download from <a href="https://github.com/linkedin/TonY/releases">here</a></li>
</ul>

<h3 id="building-a-python-virtual-environment-with-tensorflow">Building a Python virtual environment with TensorFlow</h3>

<p>TonY requires a Python virtual environment zip with TensorFlow and any needed Python libraries already installed.</p>

<pre><code>wget https://files.pythonhosted.org/packages/33/bc/fa0b5347139cd9564f0d44ebd2b147ac97c36b2403943dbee8a25fd74012/virtualenv-16.0.0.tar.gz
tar xf virtualenv-16.0.0.tar.gz

# Make sure to install using Python 3, as TensorFlow only provides Python 3 artifacts
python virtualenv-16.0.0/virtualenv.py venv
. venv/bin/activate
pip install tensorflow==1.13.1
zip -r myvenv.zip venv
deactivate
</code></pre>

<p>The above commands will produced a myvenv.zip and it will be used in below example. There&rsquo;s no need to copy it to other nodes. And it is not needed when using Docker to run the job.</p>

<p><strong>Note:</strong> If you require a version of TensorFlow and TensorBoard prior to <code>1.13.1</code>, take a look at <a href="https://github.com/linkedin/TonY/issues/42">this</a> issue.</p>

<h3 id="get-the-training-examples">Get the training examples</h3>

<p>Get mnist_distributed.py from <a href="https://github.com/linkedin/TonY/tree/master/tony-examples/mnist-tensorflow">https://github.com/linkedin/TonY/tree/master/tony-examples/mnist-tensorflow</a></p>

<pre><code>SUBMARINE_VERSION=0.2.0
SUBMARINE_HOME=path-to/hadoop-submarine-dist-0.2.0-hadoop-3.1
CLASSPATH=$(hadoop classpath --glob):${SUBMARINE_HOME}/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:${SUBMARINE_HOME}/hadoop-submarine-tony-runtime-${SUBMARINE_VERSION}.jar:path-to/tony-cli-0.3.13-all.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
 --framework tensorflow \
 --verbose \
 --input_path &quot;&quot; \
 --num_workers 2 \
 --worker_resources memory=1G,vcores=1 \
 --num_ps 1 \
 --ps_resources memory=1G,vcores=1 \
 --worker_launch_cmd &quot;myvenv.zip/venv/bin/python mnist_distributed.py --steps 2 --data_dir /tmp/data --working_dir /tmp/mode&quot; \
 --ps_launch_cmd &quot;myvenv.zip/venv/bin/python mnist_distributed.py --steps 2 --data_dir /tmp/data --working_dir /tmp/mode&quot; \
 --insecure \
 --conf tony.containers.resources=path-to/myvenv.zip#archive,path-to/mnist_distributed.py,path-to/tony-cli-0.3.13-all.jar

</code></pre>

<p>You should then be able to see links and status of the jobs from command line:</p>

<pre><code>2019-04-22 20:30:42,611 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: RUNNING
2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: RUNNING
2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: RUNNING
2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for ps 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi
2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi
2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 1 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi
2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: FINISHED
2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: FINISHED
2019-04-22 20:30:44,626 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: FINISHED

</code></pre>

<h3 id="with-docker">With Docker</h3>

<pre><code>SUBMARINE_VERSION=0.2.0
SUBMARINE_HOME=path-to/hadoop-submarine-dist-0.2.0-hadoop-3.1
CLASSPATH=$(hadoop classpath --glob):${SUBMARINE_HOME}/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:${SUBMARINE_HOME}/hadoop-submarine-tony-runtime-${SUBMARINE_VERSION}.jar:path-to/tony-cli-0.3.13-all.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
 --framework tensorflow \
 --docker_image hadoopsubmarine/tf-1.8.0-cpu:0.0.3 \
 --input_path hdfs://pi-aw:9000/dataset/cifar-10-data \
 --worker_resources memory=3G,vcores=2 \
 --worker_launch_cmd &quot;export CLASSPATH=\$(/hadoop-3.1.0/bin/hadoop classpath --glob) &amp;&amp; cd /test/models/tutorials/image/cifar10_estimator &amp;&amp; python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --variable-strategy=CPU --num-gpus=0 --sync&quot; \
 --env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
 --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 \
 --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
 --env HADOOP_HOME=/hadoop-3.1.0 \
 --env HADOOP_YARN_HOME=/hadoop-3.1.0 \
 --env HADOOP_COMMON_HOME=/hadoop-3.1.0 \
 --env HADOOP_HDFS_HOME=/hadoop-3.1.0 \
 --env HADOOP_CONF_DIR=/hadoop-3.1.0/etc/hadoop \
 --conf tony.containers.resources=/home/pi/hadoop/TonY/tony-cli/build/libs/tony-cli-0.3.2-all.jar
</code></pre>

<h4 id="notes">Notes:</h4>

<p>1) <code>DOCKER_JAVA_HOME</code> points to JAVA_HOME inside Docker image.</p>

<p>2) <code>DOCKER_HADOOP_HDFS_HOME</code> points to HADOOP_HDFS_HOME inside Docker image.</p>

<p>After Submarine v0.2.0, there is a uber jar <code>hadoop-submarine-all-${SUBMARINE_VERSION}-hadoop-${HADOOP_VERSION}.jar</code> released together with
the <code>hadoop-submarine-core-${SUBMARINE_VERSION}.jar</code>, <code>hadoop-submarine-yarnservice-runtime-${SUBMARINE_VERSION}.jar</code> and <code>hadoop-submarine-tony-runtime-${SUBMARINE_VERSION}.jar</code>.
<br />
To use the uber jar, we need to replace the three jars in previous command with one uber jar and put hadoop config directory in CLASSPATH.</p>

<h2 id="launch-pytoch-application">Launch PyToch Application:</h2>

<h3 id="without-docker-1">Without Docker</h3>

<p>You need:</p>

<ul>
<li>Build a Python virtual environment with PyTorch 0.4.0+ installed</li>
</ul>

<h3 id="building-a-python-virtual-environment-with-pytorch">Building a Python virtual environment with PyTorch</h3>

<p>TonY requires a Python virtual environment zip with PyTorch and any needed Python libraries already installed.</p>

<pre><code>wget https://files.pythonhosted.org/packages/33/bc/fa0b5347139cd9564f0d44ebd2b147ac97c36b2403943dbee8a25fd74012/virtualenv-16.0.0.tar.gz
tar xf virtualenv-16.0.0.tar.gz

python virtualenv-16.0.0/virtualenv.py venv
. venv/bin/activate
pip install pytorch==0.4.0
zip -r myvenv.zip venv
deactivate
</code></pre>

<h3 id="get-the-training-examples-1">Get the training examples</h3>

<p>Get mnist_distributed.py from <a href="https://github.com/linkedin/TonY/tree/master/tony-examples/mnist-pytorch">https://github.com/linkedin/TonY/tree/master/tony-examples/mnist-pytorch</a></p>

<pre><code>SUBMARINE_VERSION=0.2.0
SUBMARINE_HOME=path-to/hadoop-submarine-dist-0.2.0-hadoop-3.1
CLASSPATH=$(hadoop classpath --glob):${SUBMARINE_HOME}/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:${SUBMARINE_HOME}/hadoop-submarine-tony-runtime-${SUBMARINE_VERSION}.jar:path-to/tony-cli-0.3.13-all.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
 --num_workers 2 \
 --worker_resources memory=3G,vcores=2 \
 --num_ps 2 \
 --ps_resources memory=3G,vcores=2 \
 --worker_launch_cmd &quot;myvenv.zip/venv/bin/python mnist_distributed.py&quot; \
 --ps_launch_cmd &quot;myvenv.zip/venv/bin/python mnist_distributed.py&quot; \
 --insecure \
 --conf tony.containers.resources=PATH_TO_VENV_YOU_CREATED/myvenv.zip#archive,PATH_TO_MNIST_EXAMPLE/mnist_distributed.py, \
PATH_TO_TONY_CLI_JAR/tony-cli-0.3.2-all.jar \
--conf tony.application.framework=pytorch

</code></pre>

<p>You should then be able to see links and status of the jobs from command line:</p>

<pre><code>2019-04-22 20:30:42,611 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: RUNNING
2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: RUNNING
2019-04-22 20:30:42,612 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: RUNNING
2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for ps 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi
2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 0 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi
2019-04-22 20:30:42,612 INFO tony.TonyClient: Logs for worker 1 at: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi
2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: ps index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000002/pi status: FINISHED
2019-04-22 20:30:44,625 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 0 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000003/pi status: FINISHED
2019-04-22 20:30:44,626 INFO tony.TonyClient: Tasks Status Updated: [TaskInfo] name: worker index: 1 url: http://pi-aw:8042/node/containerlogs/container_1555916523933_0030_01_000004/pi status: FINISHED

</code></pre>

<h3 id="with-docker-1">With Docker</h3>

<pre><code>SUBMARINE_VERSION=0.2.0
SUBMARINE_HOME=path-to/hadoop-submarine-dist-0.2.0-hadoop-3.1
CLASSPATH=$(hadoop classpath --glob):${SUBMARINE_HOME}/hadoop-submarine-core-${SUBMARINE_VERSION}.jar:${SUBMARINE_HOME}/hadoop-submarine-tony-runtime-${SUBMARINE_VERSION}.jar:path-to/tony-cli-0.3.13-all.jar \
java org.apache.hadoop.yarn.submarine.client.cli.Cli job run --name tf-job-001 \
 --docker_image hadoopsubmarine/tf-1.8.0-cpu:0.0.3 \
 --input_path hdfs://pi-aw:9000/dataset/cifar-10-data \
 --worker_resources memory=3G,vcores=2 \
 --worker_launch_cmd &quot;export CLASSPATH=\$(/hadoop-3.1.0/bin/hadoop classpath --glob) &amp;&amp; cd /test/models/tutorials/image/cifar10_estimator &amp;&amp; python cifar10_main.py --data-dir=%input_path% --job-dir=%checkpoint_path% --train-steps=10000 --eval-batch-size=16 --train-batch-size=16 --variable-strategy=CPU --num-gpus=0 --sync&quot; \
 --env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
 --env DOCKER_HADOOP_HDFS_HOME=/hadoop-3.1.0 \
 --env DOCKER_JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \
 --env HADOOP_HOME=/hadoop-3.1.0 \
 --env HADOOP_YARN_HOME=/hadoop-3.1.0 \
 --env HADOOP_COMMON_HOME=/hadoop-3.1.0 \
 --env HADOOP_HDFS_HOME=/hadoop-3.1.0 \
 --env HADOOP_CONF_DIR=/hadoop-3.1.0/etc/hadoop \
 --conf tony.containers.resources=PATH_TO_TONY_CLI_JAR/tony-cli-0.3.2-all.jar \
 --conf tony.application.framework=pytorch
</code></pre>


            
            <h1><a href="/submarine/docs/0.2.0/WriteDockerfilePT/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="creating-docker-images-for-running-pytorch-on-yarn">Creating Docker Images for Running PyTorch on YARN</h1>

<h2 id="how-to-create-docker-images-to-run-pytorch-on-yarn">How to create docker images to run PyTorch on YARN</h2>

<p>Dockerfile to run PyTorch on YARN needs two parts:</p>

<p><strong>Base libraries which PyTorch depends on</strong></p>

<p>1) OS base image, for example <code>ubuntu:16.04</code></p>

<p>2) PyTorch dependent libraries and packages. For example <code>python</code>, <code>scipy</code>. For GPU support, you also need <code>cuda</code>, <code>cudnn</code>, etc.</p>

<p>3) PyTorch package.</p>

<p><strong>Libraries to access HDFS</strong></p>

<p>1) JDK</p>

<p>2) Hadoop</p>

<p>Here&rsquo;s an example of a base image (with GPU support) to install PyTorch:</p>

<pre><code class="language-shell">FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
ARG PYTHON_VERSION=3.6
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
         build-essential \
         cmake \
         git \
         curl \
         vim \
         ca-certificates \
         libjpeg-dev \
         libpng-dev \
         wget &amp;&amp;\
     rm -rf /var/lib/apt/lists/*


RUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  &amp;&amp; \
     chmod +x ~/miniconda.sh &amp;&amp; \
     ~/miniconda.sh -b -p /opt/conda &amp;&amp; \
     rm ~/miniconda.sh &amp;&amp; \
     /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include cython typing &amp;&amp; \
     /opt/conda/bin/conda install -y -c pytorch magma-cuda100 &amp;&amp; \
     /opt/conda/bin/conda clean -ya
ENV PATH /opt/conda/bin:$PATH
RUN pip install ninja
# This must be done before pip so that requirements.txt is available
WORKDIR /opt/pytorch
RUN git clone https://github.com/pytorch/pytorch.git
WORKDIR pytorch
RUN git submodule update --init
RUN TORCH_CUDA_ARCH_LIST=&quot;3.5 5.2 6.0 6.1 7.0+PTX&quot; TORCH_NVCC_FLAGS=&quot;-Xfatbin -compress-all&quot; \
    CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; \
    pip install -v .

WORKDIR /opt/pytorch
RUN git clone https://github.com/pytorch/vision.git &amp;&amp; cd vision &amp;&amp; pip install -v .

</code></pre>

<p>On top of above image, add files, install packages to access HDFS</p>

<pre><code class="language-shell">RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk wget
# Install hadoop
ENV HADOOP_VERSION=&quot;3.1.2&quot;
RUN wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
RUN rm hadoop-${HADOOP_VERSION}.tar.gz
</code></pre>

<p>Build and push to your own docker registry: Use <code>docker build ...</code> and <code>docker push ...</code> to finish this step.</p>

<h2 id="use-examples-to-build-your-own-pytorch-docker-images">Use examples to build your own PyTorch docker images</h2>

<p>We provided some example Dockerfiles for you to build your own PyTorch docker images.</p>

<p>For latest PyTorch</p>

<ul>
<li><em>docker/pytorch/base/ubuntu-16.04/Dockerfile.gpu.pytorch_latest</em>: Latest Pytorch that supports GPU, which is prebuilt to CUDA10.</li>
<li><em>docker/pytorch/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.pytorch_latest</em>: Latest Pytorch that GPU, which is prebuilt to CUDA10, with models.</li>
</ul>

<h2 id="build-docker-images">Build Docker images</h2>

<h3 id="manually-build-docker-image">Manually build Docker image:</h3>

<p>Under <code>docker/pytorch</code> directory, run <code>build-all.sh</code> to build all Docker images. This command will build the following Docker images:</p>

<ul>
<li><code>pytorch-latest-gpu-base:0.0.1</code> for base Docker image which includes Hadoop, PyTorch, GPU base libraries.</li>
<li><code>pytorch-latest-gpu:0.0.1</code> which includes cifar10 model as well</li>
</ul>

<h3 id="use-prebuilt-images">Use prebuilt images</h3>

<p>(No liability)
You can also use prebuilt images for convenience:</p>

<ul>
<li>hadoopsubmarine/pytorch-latest-gpu-base:0.0.1</li>
</ul>


            
            <h1><a href="/submarine/docs/0.2.0/WriteDockerfileTF/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h2 id="creating-docker-images-for-running-tensorflow-on-yarn">Creating Docker Images for Running Tensorflow on YARN</h2>

<h3 id="how-to-create-docker-images-to-run-tensorflow-on-yarn">How to create docker images to run Tensorflow on YARN</h3>

<p>Dockerfile to run Tensorflow on YARN need two part:</p>

<p><strong>Base libraries which Tensorflow depends on</strong></p>

<p>1) OS base image, for example <code>ubuntu:16.04</code></p>

<p>2) Tensorflow depended libraries and packages. For example <code>python</code>, <code>scipy</code>. For GPU support, need <code>cuda</code>, <code>cudnn</code>, etc.</p>

<p>3) Tensorflow package.</p>

<p><strong>Libraries to access HDFS</strong></p>

<p>1) JDK</p>

<p>2) Hadoop</p>

<p>Here&rsquo;s an example of a base image (w/o GPU support) to install Tensorflow:</p>

<pre><code class="language-shell">FROM ubuntu:16.04

# Pick up some TF dependencies
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        libfreetype6-dev \
        libpng12-dev \
        libzmq3-dev \
        pkg-config \
        python \
        python-dev \
        rsync \
        software-properties-common \
        unzip \
        &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

RUN export DEBIAN_FRONTEND=noninteractive &amp;&amp; apt-get update &amp;&amp; apt-get install -yq krb5-user libpam-krb5 &amp;&amp; apt-get clean

RUN curl -O https://bootstrap.pypa.io/get-pip.py &amp;&amp; \
    python get-pip.py &amp;&amp; \
    rm get-pip.py

RUN pip --no-cache-dir install \
        Pillow \
        h5py \
        ipykernel \
        jupyter \
        matplotlib \
        numpy \
        pandas \
        scipy \
        sklearn \
        &amp;&amp; \
    python -m ipykernel.kernelspec

RUN pip --no-cache-dir install \
    http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.13.1-cp27-none-linux_x86_64.whl
</code></pre>

<p>On top of above image, add files, install packages to access HDFS</p>

<pre><code class="language-shell">RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk wget
# Install hadoop
ENV HADOOP_VERSION=&quot;3.1.2&quot;
RUN wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
RUN tar zxf hadoop-${HADOOP_VERSION}.tar.gz
RUN ln -s hadoop-${HADOOP_VERSION} hadoop-current
RUN rm hadoop-${HADOOP_VERSION}.tar.gz
</code></pre>

<p>Build and push to your own docker registry: Use <code>docker build ...</code> and <code>docker push ...</code> to finish this step.</p>

<h3 id="use-examples-to-build-your-own-tensorflow-docker-images">Use examples to build your own Tensorflow docker images</h3>

<p>We provided following examples for you to build tensorflow docker images.</p>

<p>For Tensorflow 1.13.1 (Precompiled to CUDA 10.x)</p>

<ul>
<li><em>docker/tensorflow/base/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1</em>: Tensorflow 1.13.1 supports CPU only.</li>
<li><em>docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.cpu.tf_1.13.1</em>: Tensorflow 1.13.1 supports CPU only, and included models</li>
<li><em>docker/tensorflow/base/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1</em>: Tensorflow 1.13.1 supports GPU, which is prebuilt to CUDA10.</li>
<li><em>docker/tensorflow/with-cifar10-models/ubuntu-16.04/Dockerfile.gpu.tf_1.13.1</em>: Tensorflow 1.13.1 supports GPU, which is prebuilt to CUDA10, with models.</li>
</ul>

<h3 id="build-docker-images">Build Docker images</h3>

<h4 id="manually-build-docker-image">Manually build Docker image:</h4>

<p>Under <code>docker/</code> directory, run <code>build-all.sh</code> to build Docker images. It will build following images:</p>

<ul>
<li><code>tf-1.13.1-gpu-base:0.0.1</code> for base Docker image which includes Hadoop, Tensorflow, GPU base libraries.</li>
<li><code>tf-1.13.1-gpu-base:0.0.1</code> for base Docker image which includes Hadoop. Tensorflow.</li>
<li><code>tf-1.13.1-gpu:0.0.1</code> which includes cifar10 model</li>
<li><code>tf-1.13.1-cpu:0.0.1</code> which inclues cifar10 model (cpu only).</li>
</ul>

<h4 id="use-prebuilt-images">Use prebuilt images</h4>

<p>(No liability)
You can also use prebuilt images for convenience:</p>

<ul>
<li>hadoopsubmarine/tf-1.13.1-gpu:0.0.1</li>
<li>hadoopsubmarine/tf-1.13.1-cpu:0.0.1</li>
</ul>


            
            <h1><a href="/submarine/docs/Index/">Apache Hadoop Submarine Documentation</a></h1>
            <p><small>0001 Jan 1 </small></p>

            

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<h1 id="documentation">Documentation</h1>

<p>Choose a version:</p>

<ul>
<li><p><a href="docs/0.2.0/Index">0.2.0</a></p></li>

<li><p><a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-submarine/Index.html">0.1.0 (Apache Hadoop 3.2.0)</a></p></li>
</ul>


            
        </ul>
  </div>


<ul class="pagination">
    
    <li class="page-item">
        <a href="/submarine/docs/" class="page-link" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
    </li>
    
    <li class="page-item">
    <a href="/submarine/docs/" class="page-link" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
    </li>
    
    
    
    
    
    
    
        
        
    
    
    <li class="page-item"><a class="page-link" href="/submarine/docs/">1</a></li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item active"><a class="page-link" href="/submarine/docs/page/2/">2</a></li>
    
    
    <li class="page-item disabled">
    <a href="" class="page-link" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
    </li>
    
    <li class="page-item">
        <a href="/submarine/docs/page/2/" class="page-link" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
    </li>
    
</ul>

</section>
</div>

<footer>
    <div class="container">

        <div class="col-md-12 trademark">
            <p>&copy; 2019 <a href="http://apache.org">The Apache Software Foundation</a>,<br/>
                Apache, Apache Hadoop, the Apache feather logo, are trademarks of The Apache Software Foundation.
            <p>
        </div>
    </div>
</footer>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"
        integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS"
        crossorigin="anonymous"></script>


</body>
</html>

